<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-14</h1>
<div><h3><a href='https://arxiv.org/abs/2510.10484'>CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor</a></h3><h3><a href='https://arxiv.org/pdf/2510.10484' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/45</p><p><b>作者：</b>Buqing Xu, Jianfeng Zhu, Yichi Zhang, Qinyi Cai, Guanhua Li, Shaojun Wei, Leibo Liu</p><p>CPU simulators are vital for computer architecture research, primarily for estimating performance under different programs. This poses challenges for fast and accurate simulation of modern CPUs, especially in multi-core systems. Modern CPU peformance simulators such as GEM5 adopt the cycle-accurate and event-driven approach, which is timeconsuming to simulate the extensive microarchitectural behavior of a real benchmark running on out-of-order CPUs. Recently, machine leaning based approach has been proposed to improve simulation speed, but they are currently limited to estimating the cycles of basic blocks rather than the complete benchmark program. This paper introduces a novel ML-based CPU simulator named CAPSim, which uses an attention-based neural network performance predictor and instruction trace sampling method annotated with context. The attention mechanism effectively captures long-range influence within the instruction trace, emphasizing critical context information. This allows the model to improve performance prediction accuracy by focusing on important code instruction. CAPSim can predict the execution time of unseen benchmarks at a significantly fast speed compared with an accurate O3 simulator built with gem5. Our evaluation on a commercial Intel Xeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to using gem5 built simulator, which is superior to the cutting-edge deep learning approach</p><p><h4>cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09842'>Towards Automated and Predictive Network-Level Energy Profiling in Reconfigurable IoT Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.09842' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/45</p><p><b>作者：</b>Mohammud J. Bocus, Senhui Qiu, Robert J. Piechocki, Kerstin Eder</p><p>Energy efficiency has emerged as a defining constraint in the evolution of sustainable Internet of Things (IoT) networks. This work moves beyond simulation-based or device-centric studies to deliver measurement-driven, network-level smart energy analysis. The proposed system enables end-to-end visibility of energy flows across distributed IoT infrastructures, uniting Bluetooth Low Energy (BLE) and Visible Light Communication (VLC) modes with environmental sensing and E-ink display subsystems under a unified profiling and prediction platform. Through automated, time-synchronized instrumentation, the framework captures fine-grained energy dynamics across both node and gateway layers. We developed a suite of tools that generate energy datasets for IoT ecosystems, addressing the scarcity of such data and enabling AI-based predictive and adaptive energy optimization. Validated within a network-level IoT testbed, the approach demonstrates robust performance under real operating conditions.</p><p><h4>cs.NI, cs.AR, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09847'>THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling</a></h3><h3><a href='https://arxiv.org/pdf/2510.09847' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/45</p><p><b>作者：</b>Said Muhammad, Lahlou Laaziz, Nadjia Kara, Phat Tan Nguyen, Timothy Murphy</p><p>The dynamic adaptation of resource levels enables the system to enhance energy efficiency while maintaining the necessary computational resources, particularly in scenarios where workloads fluctuate significantly over time. The proposed approach can play a crucial role in heterogeneous systems where workload characteristics are not uniformly distributed, such as non-pinning tasks. The deployed THEAS algorithm in this research work ensures a balance between performance and power consumption, making it suitable for a wide range of real-time applications. A comparative analysis of the proposed THEAS algorithm with well-known scheduling techniques such as Completely Fair Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling (HeteroSched), and Utility-Based Scheduling is presented in Table III. Each scheme is compared based on adaptability, core selection criteria, performance scaling, cache awareness, overhead, and real-time suitability.</p><p><h4>cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10209'>LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2510.10209' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/45</p><p><b>作者：</b>Massinissa Merouani, Afif Boudaoud, Riyadh Baghdadi</p><p>The advancement of machine learning for compiler optimization, particularly within the polyhedral model, is constrained by the scarcity of large-scale, public performance datasets. This data bottleneck forces researchers to undertake costly data generation campaigns, slowing down innovation and hindering reproducible research learned code optimization. To address this gap, we introduce LOOPerSet, a new public dataset containing 28 million labeled data points derived from 220,000 unique, synthetically generated polyhedral programs. Each data point maps a program and a complex sequence of semantics-preserving transformations (such as fusion, skewing, tiling, and parallelism)to a ground truth performance measurement (execution time). The scale and diversity of LOOPerSet make it a valuable resource for training and evaluating learned cost models, benchmarking new model architectures, and exploring the frontiers of automated polyhedral scheduling. The dataset is released under a permissive license to foster reproducible research and lower the barrier to entry for data-driven compiler optimization.</p><p><h4>cs.PL, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10747'>CPU-Limits kill Performance: Time to rethink Resource Control</a></h3><h3><a href='https://arxiv.org/pdf/2510.10747' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/45</p><p><b>作者：</b>Chirag Shetty, Sarthak Chakraborty, Hubertus Franke, Larisa Shwartz, Chandra Narayanaswami, Indranil Gupta, Saurabh Jha</p><p>Research in compute resource management for cloud-native applications is dominated by the problem of setting optimal CPU limits -- a fundamental OS mechanism that strictly restricts a container&#x27;s CPU usage to its specified CPU-limits . Rightsizing and autoscaling works have innovated on allocation/scaling policies assuming the ubiquity and necessity of CPU-limits . We question this. Practical experiences of cloud users indicate that CPU-limits harms application performance and costs more than it helps. These observations are in contradiction to the conventional wisdom presented in both academic research and industry best practices. We argue that this indiscriminate adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is essential for operational and safety purposes. We provide empirical evidence making a case for eschewing CPU-limits completely from latency-sensitive applications. This prompts a fundamental rethinking of auto-scaling and billing paradigms and opens new research avenues. Finally, we highlight specific scenarios where CPU-limits can be beneficial if used in a well-reasoned way (e.g. background jobs).</p><p><h4>cs.DC, cs.OS, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11015'>A new $1/(1-\rho)$-scaling bound for multiserver queues via a leave-one-out technique</a></h3><h3><a href='https://arxiv.org/pdf/2510.11015' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/45</p><p><b>作者：</b>Yige Hong</p><p>Bounding the queue length in a multiserver queue is a central challenge in queueing theory. Even for the classic $GI/GI/n$ queue with homogeneous servers, it is highly non-trivial to derive a simple and accurate bound for the steady-state queue length that holds across all scaling regimes. A recent breakthrough by Li and Goldberg (2025) establishes bounds that scale as $1/(1-\rho)$ for any load $\rho &lt; 1$ and number of servers $n$, which is the correct scaling in many well-known scaling regimes, including classic heavy-traffic, Halfin-Whitt and Nondegenerate-Slowdown. However, their bounds entail large constant factors and a highly intricate proof, suggesting room for further improvement.  In this paper, we present a new $1/(1-\rho)$-scaling bound for the $GI/GI/n$ queue. Our bound, while restricted to the light-tailed case and the first moment of the queue length, has a more interpretable and often tighter leading constant. Our proof is relatively simple, utilizing a modified $GI/GI/n$ queue, the stationarity of a quadratic test function, and a novel leave-one-out coupling technique.  Finally, we also extend our method to $GI/GI/n$ queues with fully heterogeneous service-time distributions.</p><p><h4>math.PR, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11310'>Detection of Performance Changes in MooBench Results Using Nyrki\&quot;o on GitHub Actions</a></h3><h3><a href='https://arxiv.org/pdf/2510.11310' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/45</p><p><b>作者：</b>Shinhyung Yang, David Georg Reichelt, Henrik Ingo, Wilhelm Hasselbring</p><p>In GitHub with its 518 million hosted projects, performance changes within these projects are highly relevant to the project&#x27;s users. Although performance measurement is supported by GitHub CI/CD, performance change detection is a challenging topic.  In this paper, we demonstrate how we incorporated Nyrki\&quot;o to MooBench. Prior to this work, Moobench continuously ran on GitHub virtual machines, measuring overhead of tracing agents, but without change detection. By adding the upload of the measurements to the Nyrki\&quot;o change detection service, we made it possible to detect performance changes. We identified one major performance regression and examined the performance change in depth. We report that (1) it is reproducible with GitHub actions, and (2) the performance regression is caused by a Linux Kernel version change.</p><p><h4>cs.SE, cs.OS, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11361'>A protocol to reduce worst-case latency in deflection-based on-chip networks</a></h3><h3><a href='https://arxiv.org/pdf/2510.11361' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/45</p><p><b>作者：</b>Leandro Soares Indrusiak</p><p>We present a novel protocol that reduces worst-case packet latency in deflection-based on-chip interconnect networks. It enforces the deflection of the header of a packet but not its payload, resulting in a reduction in overall network traffic and, more importantly, worst-case packet latency due to decreased pre-injection latency.</p><p><h4>cs.NI, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2410.19274'>Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation Linking</a></h3><h3><a href='https://arxiv.org/pdf/2410.19274' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/45</p><p><b>作者：</b>Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren</p><p>Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.  In this paper, we propose Neuralink, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Neuralink leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize I/O efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Neuralink achieves on average $1.49\times$ improvements in end-to-end latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Neuralink explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design for LLM inference.</p><p><h4>cs.LG, cs.AI, cs.OS, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.12741'>Surrogate Modeling for Scalable Evaluation of Distributed Computing Systems for HEP Applications</a></h3><h3><a href='https://arxiv.org/pdf/2502.12741' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/45</p><p><b>作者：</b>Larissa Schmid, Maximilian Horzela, Valerii Zhyla, Manuel Giffels, G\&quot;unter Quast, Anne Koziolek</p><p>The Worldwide LHC Computing Grid (WLCG) provides the robust computing infrastructure essential for the LHC experiments by integrating global computing resources into a cohesive entity. Simulations of different compute models present a feasible approach for evaluating future adaptations that are able to cope with future increased demands. However, running these simulations incurs a trade-off between accuracy and scalability. For example, while the simulator DCSim can provide accurate results, it falls short on scaling with the size of the simulated platform. Using Generative Machine Learning as a surrogate presents a candidate for overcoming this challenge.  In this work, we evaluate the usage of three different Machine Learning models for the simulation of distributed computing systems and assess their ability to generalize to unseen situations. We show that those models can predict central observables derived from execution traces of compute jobs with approximate accuracy but with orders of magnitude faster execution times. Furthermore, we identify potentials for improving the predictions towards better accuracy and generalizability.</p><p><h4>cs.DC, cs.PF, hep-ex</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.01249'>SysLLMatic: Large Language Models are Software System Optimizers</a></h3><h3><a href='https://arxiv.org/pdf/2506.01249' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/45</p><p><b>作者：</b>Huiyun Peng, Arjun Gupte, Ryan Hasler, Nicholas John Eliopoulos, Chien-Chou Ho, Rishi Mantri, Leo Deng, Konstantin L\&quot;aufer, George K. Thiruvathukal, James C. Davis</p><p>Automatic software system optimization can improve software speed and save energy. Traditional approaches to optimization rely on manual tuning and compiler heuristics, limiting their ability to generalize across diverse codebases. Recent methods using LLMs introduce automation, but they do not scale effectively to the complexity and size of real-world software systems, leaving a gap in practical applicability. We present SysLLMatic, a system that integrates LLMs with performance diagnostics feedback and a curated catalog of 43 optimization patterns to automatically optimize software code. Our approach builds on recent advances in LLM-based code optimization and specifically targets the limitations of existing systems in handling real-world software applications. We evaluate it on three benchmark suites: HumanEval_CPP (competitive programming in C++), SciMark2 (scientific kernels in Java), and DaCapoBench (large-scale software systems in Java). Results show that SysLLMatic can improve software system performance, including latency, throughput, energy efficiency, memory usage, and CPU utilization. It consistently outperforms state-of-the-art LLM baselines on microbenchmarks. On large-scale application codes, to which prior LLM approaches have not scaled, it surpasses compiler optimizations, achieving average relative improvements of 1.5x in latency (vs. 1.01x for the compiler) and 1.76x in throughput (vs. 1.02x for the compiler). Our findings demonstrate that LLMs, guided by principled system thinking through the optimization pattern catalog and appropriate performance diagnostics, can serve as viable software system optimizers. We further identify limitations of our approach and the challenges involved in handling complex applications. This work provides a foundation for generating optimized code across various languages, benchmarks, and program sizes in a principled manner.</p><p><h4>cs.SE, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10225'>ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism</a></h3><h3><a href='https://arxiv.org/pdf/2510.10225' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/45</p><p><b>作者：</b>Jialin Sun, Yuchen Hu, Dean You, Yushu Du, Hui Wang, Xinwei Fang, Weiwei Shan, Nan Guan, Zhe Jiang</p><p>Functional verification is a critical bottleneck in integrated circuit development, with CPU verification being especially time-intensive and labour-consuming. Industrial practice relies on differential testing for CPU verification, yet faces bottlenecks at nearly each stage of the framework pipeline: front-end stimulus generation lacks micro-architectural awareness, yielding low-quality and redundant tests that impede coverage closure and miss corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA acceleration, often stalls on long-running tests and offers limited visibility, delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a full-stack, Large Language Model (LLM)-aided CPU verification framework with FPGA parallelism, from bug categorisation and stimulus generation to simulation infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC&#x27;s front-end, infused with micro-architectural knowledge and historical bug patterns, generating highly targeted tests that rapidly achieve coverage goals and capture elusive corner cases. In ISAAC&#x27;s back-end, we introduce a lightweight forward-snapshot mechanism and a decoupled co-simulation architecture between the Instruction Set Simulator (ISS) and the Design Under Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the simulation throughput is significantly improved. As a demonstration, we used ISAAC to verify a mature CPU that has undergone multiple successful tape-outs. Results show up to 17,536x speed-up over software RTL simulation, while detecting several previously unknown bugs, two of which are reported in this paper.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10623'>ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration</a></h3><h3><a href='https://arxiv.org/pdf/2510.10623' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/45</p><p><b>作者：</b>Ahmed J. Abdelmaksoud, Cristian Sestito, Shiwei Wang, Themis Prodromakis</p><p>Transformers are at the core of modern AI nowadays. They rely heavily on matrix multiplication and require efficient acceleration due to their substantial memory and computational requirements. Quantization plays a vital role in reducing memory usage, and can be exploited for computations by designing reconfigurable architectures that enhance matrix multiplication by dynamically adjusting the precision. This paper proposes ADiP, a novel adaptive-precision systolic array architecture designed for efficient matrix multiplication acceleration.The proposed architecture consists of NxN adaptive-precision processing elements (PEs) and shared accumulators. ADiP supports multiple computation modes, including symmetric single-matrix multiplication as well as asymmetric multi-matrix multiplication with a shared input matrix, thereby improving data-reuse and PE utilization. In addition, ADiP maximizes the computational density by adapting to different precisions, such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed for ADiP architecture, including latency and throughput for versatile architecture configurations. A comprehensive hardware design space exploration is demonstrated using 22nm commercial technology, achieving up to a 4x higher computational throughput. Furthermore, ADiP is evaluated on different transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models, delivering latency improvement up to 53.6%, and energy improvement up to 24.4% for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit, 8bitx4bit, and 8bitx2bit operations, respectively.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10872'>FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash</a></h3><h3><a href='https://arxiv.org/pdf/2510.10872' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/45</p><p><b>作者：</b>Sumukh Pinge, Ashkan Moradifirouzabadi, Keming Fan, Prasanna Venkatesan Ravindran, Tanvir H. Pantha, Po-Kai Hsu, Zheyu Li, Weihong Xu, Zihan Xia, Flavio Ponzina, Winston Chern, Taeyoung Song, Priyankka Ravikumar, Mengkun Tian, Lance Fernandes, Huy Tran, Hari Jayasankar, Hang Chen, Chinsung Park, Amrit Garlapati, Kijoon Kim, Jongho Woo, Suhwan Lim, Kwangsoo Kim, Wanki Kim, Daewon Ha, Duygu Kuzum, Shimeng Yu, Sourav Dutta, Asif Khan, Tajana Rosing, Mingu Kang</p><p>The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of terabytes, poses significant challenges for efficient, large-scale library search - a critical component for drug discovery. Traditional processors struggle to handle this data volume efficiently, making in-storage computing (ISP) a promising alternative. This work introduces an ISP architecture leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly higher density, faster speeds, and lower voltage requirements compared to traditional NAND flash. Despite its superior density, the NAND structure has not been widely utilized in ISP applications due to limited throughput associated with row-by-row reads from serially connected cells. To overcome these limitations, we integrate hyperdimensional computing (HDC), a brain-inspired paradigm that enables highly parallel processing with simple operations and strong error tolerance. By combining HDC with the proposed dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND structure, we parallelize vector computations to enable efficient MS spectral library search, achieving 43x speedup and 21x higher energy efficiency over state-of-the-art 3D NAND methods, while maintaining comparable accuracy.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.05354'>relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication</a></h3><h3><a href='https://arxiv.org/pdf/2508.05354' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/45</p><p><b>作者：</b>Michael Rogenmoser, Angelo Garofalo, Luca Benini</p><p>On-chip communication is a critical element of modern systems-on-chip (SoCs), allowing processor cores to interact with memory and peripherals. Interconnects require special care in radiation-heavy environments, as any soft error within the SoC interconnect is likely to cause a functional failure of the whole SoC. This work proposes relOBI, an extension to the Open Bus Interface (OBI) combining triple modular redundancy (TMR) for critical handshake signals with error correction codes (ECC) protection on other signals. Implementing and testing the reliable crossbar shows improved reliability to injected single faults from a vulnerability of 34.85 % to zero compared to the irredundant baseline, with an area increase of 2.6 $\times$. The area overhead is 1.8 $\times$ lower than that reported in the literature for fine-grained triplication and voting.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.25626'>LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels</a></h3><h3><a href='https://arxiv.org/pdf/2509.25626' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/45</p><p><b>作者：</b>Yi Hu (North Carolina State University), Huiyang Zhou (North Carolina State University)</p><p>3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10676'>Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation</a></h3><h3><a href='https://arxiv.org/pdf/2510.10676' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/45</p><p><b>作者：</b>Mukul Lokhande, Tanushree Dewangan, Mohd Sharik Mansoori, Tejas Chaudhari, Akarsh J., Damayanti Lokhande, Adam Teman, Santosh Kumar Vishvakarma</p><p>This paper introduces Bhasha-Rupantarika, a light and efficient multilingual translation system tailored through algorithm-hardware codesign for resource-limited settings. The method investigates model deployment at sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in inference speed, which correlates with an increased throughput of 66 tokens/s (improvement by 4.8x). This underscores the importance of ultra-low precision quantization for real-time deployment in IoT devices using FPGA accelerators, achieving performance on par with expectations. Our evaluation covers bidirectional translation between Indian and international languages, showcasing its adaptability in low-resource linguistic contexts. The FPGA deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs, resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x enhancement compared to HPTA. Overall, the evaluation provides a viable solution based on quantisation-aware translation along with hardware efficiency suitable for deployable multilingual AI systems. The entire codes [https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for reproducibility are publicly available, facilitating rapid integration and further development by researchers.</p><p><h4>cs.AR, cs.CL, cs.RO, eess.AS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11192'>Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs</a></h3><h3><a href='https://arxiv.org/pdf/2510.11192' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/45</p><p><b>作者：</b>Jo\~ao Paulo Cardoso de Lima, Marc Dietrich, Jeronimo Castrillon, Asif Ali Khan</p><p>Structured sparsity enables deploying large language models (LLMs) on resource-constrained systems. Approaches like dense-to-sparse fine-tuning are particularly compelling, achieving remarkable structured sparsity by reducing the model size by over 6.7x, while still maintaining acceptable accuracy. Despite this reduction, LLM inference, especially the decode stage being inherently memory-bound, is extremely expensive on conventional Von-Neumann architectures. Compute-in-memory (CIM) architectures mitigate this by performing computations directly in memory, and when paired with sparse LLMs, enable storing and computing the entire model in memory, eliminating the data movement on the off-chip bus and improving efficiency. Nonetheless, naively mapping sparse matrices onto CIM arrays leads to poor array utilization and diminished computational efficiency. In this paper, we present an automated framework with novel mapping and scheduling strategies to accelerate sparse LLM inference on CIM accelerators. By exploiting block-diagonal sparsity, our approach improves CIM array utilization by over 50%, achieving more than 4x reduction in both memory footprint and the number of required floating-point operations.</p><p><h4>cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09932'>ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions</a></h3><h3><a href='https://arxiv.org/pdf/2510.09932' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/45</p><p><b>作者：</b>Devansh Jain, Akash Pardeshi, Marco Frigo, Krut Patel, Kaustubh Khulbe, Jai Arora, Charith Mendis</p><p>Tensor compilers play a key role in enabling high-performance implementations of deep learning workloads. These compilers rely on existing CPU and GPU code generation backends to generate device-specific code. Recently, many tensor accelerators (neural processing units) have been proposed to further accelerate these workloads. Compared to commodity hardware, however, most of the proposed tensor accelerators do not have compiler backends with code generation support. Moreover, the accelerator designs are subject to fast iteration cycles, making it difficult to manually develop compiler backends similar to commodity hardware platforms. Therefore, to increase adoption and enable faster software development cycles for novel tensor accelerator designs, we need to make the compiler backend construction process more agile.  To address this gap, we introduce ACT, a compiler backend generator that automatically generates compiler backends for tensor accelerators, given just the instruction set architecture (ISA) descriptions. We first formally specify the compiler backend generation problem that introduces a novel specification for describing tensor accelerator ISAs. Next, we design ACT such that it supports user-programmable memories and complex parameterized instructions that are prevalent in tensor accelerators. ACT uses a novel parameterized equality saturation-based instruction selection phase and a constraint programming-based memory allocation phase. We prove that compiler backends generated by ACT are sound and complete. Finally, we generate compiler backends for three accelerator platforms from industry and academia, and show that they match or outperform code written using hand-optimized kernel libraries while maintaining low compilation overheads.</p><p><h4>cs.PL, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10718'>HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing</a></h3><h3><a href='https://arxiv.org/pdf/2510.10718' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/45</p><p><b>作者：</b>Rajat Bhattacharjya, Woohyeok Park, Arnab Sarkar, Hyunwoo Oh, Mohsen Imani, Nikil Dutt</p><p>Direction of Arrival (DoA) estimation techniques face a critical trade-off, as classical methods often lack accuracy in challenging, low signal-to-noise ratio (SNR) conditions, while modern deep learning approaches are too energy-intensive and opaque for resource-constrained, safety-critical systems. We introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing (HDC). The framework introduces two distinct feature extraction strategies -- Mean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline, and then reframes DoA estimation as a pattern recognition problem. This approach leverages HDC&#x27;s inherent robustness to noise and its transparent algebraic operations to bypass the expensive matrix decompositions and ``black-box&#x27;&#x27; nature of classical and deep learning methods, respectively. Our evaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than state-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it also consumes ~93% less energy than competing neural baselines on an embedded NVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and efficiency establishes HYPERDOA as a robust and viable solution for mission-critical applications on edge devices.</p><p><h4>eess.SP, cs.AI, cs.AR, cs.SC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10862'>A Joint Learning Approach to Hardware Caching and Prefetching</a></h3><h3><a href='https://arxiv.org/pdf/2510.10862' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/45</p><p><b>作者：</b>Samuel Yuan, Divyanshu Saxena, Jiayi Chen, Nihal Sharma, Aditya Akella</p><p>Several learned policies have been proposed to replace heuristics for scheduling, caching, and other system components in modern systems. By leveraging diverse features, learning from historical trends, and predicting future behaviors, such models promise to keep pace with ever-increasing workload dynamism and continuous hardware evolution. However, policies trained in isolation may still achieve suboptimal performance when placed together. In this paper, we inspect one such instance in the domain of hardware caching -- for the policies of cache replacement and prefetching. We argue that these two policies are bidirectionally interdependent and make the case for training the two jointly. We propose a joint learning approach based on developing shared representations for the features used by the two policies. We present two approaches to develop these shared representations, one based on a joint encoder and another based on contrastive learning of the embeddings, and demonstrate promising preliminary results for both of these. Finally, we lay down an agenda for future research in this direction.</p><p><h4>cs.LG, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11484'>Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware</a></h3><h3><a href='https://arxiv.org/pdf/2510.11484' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/45</p><p><b>作者：</b>Lion Mueller, Alberto Garcia-Ortiz, Ardalan Najafi, Adam Fuks, Lennart Bamberg</p><p>Integer AI inference significantly reduces computational complexity in embedded systems. Quantization-aware training (QAT) helps mitigate accuracy degradation associated with post-training quantization but still overlooks the impact of integer rescaling during inference, which is a hardware costly operation in integer-only AI inference. This work shows that rescaling cost can be dramatically reduced post-training, by applying a stronger quantization to the rescale multiplicands at no model-quality loss. Furthermore, we introduce Rescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling multiplicands. Experiments show that even with 8x reduced rescaler widths, the full accuracy is preserved through minimal incremental retraining. This enables more energy-efficient and cost-efficient AI inference for resource-constrained embedded systems.</p><p><h4>cs.LG, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11119'>Improving AI Efficiency in Data Centres by Power Dynamic Response</a></h3><h3><a href='https://arxiv.org/pdf/2510.11119' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/45</p><p><b>作者：</b>Andrea Marinoni, Sai Shivareddy, Pietro Lio&#x27;, Weisi Lin, Erik Cambria, Clare Grey</p><p>The steady growth of artificial intelligence (AI) has accelerated in the recent years, facilitated by the development of sophisticated models such as large language models and foundation models. Ensuring robust and reliable power infrastructures is fundamental to take advantage of the full potential of AI. However, AI data centres are extremely hungry for power, putting the problem of their power management in the spotlight, especially with respect to their impact on environment and sustainable development. In this work, we investigate the capacity and limits of solutions based on an innovative approach for the power management of AI data centres, i.e., making part of the input power as dynamic as the power used for data-computing functions. The performance of passive and active devices are quantified and compared in terms of computational gain, energy efficiency, reduction of capital expenditure, and management costs by analysing power trends from multiple data platforms worldwide. This strategy, which identifies a paradigm shift in the AI data centre power management, has the potential to strongly improve the sustainability of AI hyperscalers, enhancing their footprint on environmental, financial, and societal fields.</p><p><h4>cs.AI, cs.AR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.07690'>HYLU: Hybrid Parallel Sparse LU Factorization</a></h3><h3><a href='https://arxiv.org/pdf/2509.07690' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/45</p><p><b>作者：</b>Xiaoming Chen</p><p>This article introduces HYLU, a hybrid parallel LU factorization-based general-purpose solver designed for efficiently solving sparse linear systems (Ax=b) on multi-core shared-memory architectures. The key technical feature of HYLU is the integration of hybrid numerical kernels so that it can adapt to various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL PARDISO in the numerical factorization phase by geometric means of 1.71X (for one-time solving) and 2.21X (for repeated solving). HYLU can be downloaded from https://github.com/chenxm1986/hylu.</p><p><h4>cs.AR, cs.DC, cs.MS, cs.NA, math.NA</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2202.12029'>Systematic Prevention of On-Core Timing Channels by Full Temporal Partitioning</a></h3><h3><a href='https://arxiv.org/pdf/2202.12029' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/45</p><p><b>作者：</b>Nils Wistoff, Moritz Schneider, Frank K. G\&quot;urkaynak, Gernot Heiser, Luca Benini</p><p>Microarchitectural timing channels enable unwanted information flow across security boundaries, violating fundamental security assumptions. They leverage timing variations of several state-holding microarchitectural components and have been demonstrated across instruction set architectures and hardware implementations. Analogously to memory protection, Ge et al. have proposed time protection for preventing information leakage via timing channels. They also showed that time protection calls for hardware support. This work leverages the open and extensible RISC-V instruction set architecture (ISA) to introduce the temporal fence instruction fence.t, which provides the required mechanisms by clearing vulnerable microarchitectural state and guaranteeing a history-independent context-switch latency. We propose and discuss three different implementations of fence.t and implement them on an experimental version of the seL4 microkernel and CVA6, an open-source, in-order, application class, 64-bit RISC-V core. We find that a complete, systematic, ISA-supported erasure of all non-architectural core components is the most effective implementation while featuring a low implementation effort, a minimal performance overhead of less than 1%, and negligible hardware costs.</p><p><h4>cs.CR, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2305.14019'>ChipGPT: How far are we from natural language hardware design</a></h3><h3><a href='https://arxiv.org/pdf/2305.14019' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/45</p><p><b>作者：</b>Kaiyan Chang, Ying Wang, Haimeng Ren, Mengdi Wang, Shengwen Liang, Yinhe Han, Huawei Li, Xiaowei Li</p><p>As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sheds some light on whether LLMs can generate correct and complete hardware logic designs described by natural language for some specifications. It is shown that ChipGPT improves programmability, and controllability, and shows broader design optimization space compared to prior work and native LLMs alone.</p><p><h4>cs.AI, cs.AR, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.24183'>QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation</a></h3><h3><a href='https://arxiv.org/pdf/2505.24183' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/45</p><p><b>作者：</b>Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</p><p>Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage &quot;distill-then-RL&quot; training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.</p><p><h4>cs.LG, cs.AR, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09851'>QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters</a></h3><h3><a href='https://arxiv.org/pdf/2510.09851' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/45</p><p><b>作者：</b>Haci Ismail Aslan, Syed Muhammad Mahmudul Haque, Joel Witzke, Odej Kao</p><p>Modern applications increasingly span across cloud, fog, and edge environments, demanding orchestration systems that can adapt to diverse deployment contexts while meeting Quality-of-Service (QoS) requirements. Standard Kubernetes schedulers do not account for user-defined objectives such as energy efficiency, cost optimization, and global performance, often leaving operators to make manual, cluster-by-cluster placement decisions. To address this need, we present QONNECT, a vendor-agnostic orchestration framework that enables declarative, QoS-driven application deployment across heterogeneous Kubernetes and K3s clusters. QONNECT introduces a distributed architecture composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster. Through a minimal YAML-based interface, users specify high-level QoS goals, which the system translates into concrete placement and migration actions. Our implementation is evaluated on a federated testbed of up to nine cloud-fog-edge clusters using the Istio Bookinfo microservice application. The system demonstrates dynamic, policy-driven microservice placement, automated failover, QoS-compliant rescheduling, and leader re-election after node failure, all without manual intervention. By bridging the gap between declarative deployment models and operational QoS goals, QONNECT transforms the cloud-edge continuum into a unified, self-optimizing platform.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10126'>FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments</a></h3><h3><a href='https://arxiv.org/pdf/2510.10126' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/45</p><p><b>作者：</b>Sehar Zehra, Hassan Jamil Syed, Ummay Faseeha</p><p>Kubernetes multi-cluster deployments demand scalable and privacy-preserving anomaly detection. Existing eBPF-based monitors provide low-overhead system and network visibility but are limited to single clusters, while centralized approaches incur bandwidth, privacy, and heterogeneity challenges. We propose FedMon, a federated eBPF framework that unifies kernel-level telemetry with federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF agents capture syscalls and network events, extract local statistical and sequence features, and share only model updates with a global server. A hybrid detection engine combining Variational Autoencoders (VAEs) with Isolation Forests enables both temporal pattern modeling and outlier detection. Deployed across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall, and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to centralized baselines. Results demonstrate that FedMon enhances accuracy, scalability, and privacy, providing an effective defense for large-scale, multi-tenant cloud-native environments.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10166'>Proactive and Reactive Autoscaling Techniques for Edge Computing</a></h3><h3><a href='https://arxiv.org/pdf/2510.10166' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/45</p><p><b>作者：</b>Suhrid Gupta, Muhammed Tawfiqul Islam, Rajkumar Buyya</p><p>Edge computing allows for the decentralization of computing resources. This decentralization is achieved through implementing microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. Several auto-scaling algorithms have been proposed to try to achieve these compliance challenges, but they suffer from performance issues and configuration complexity. This chapter provides a brief overview of edge computing architecture, its uses, benefits, and challenges for resource scaling. We then introduce Service Level Agreements, and existing research on devising algorithms used in edge computing environments to meet these agreements, along with their benefits and drawbacks.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10302'>SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference</a></h3><h3><a href='https://arxiv.org/pdf/2510.10302' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>31/45</p><p><b>作者：</b>Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu</p><p>The Mixture-of-Experts (MoE) architecture has been widely adopted in large language models (LLMs) to reduce computation cost through model sparsity. Employing speculative decoding (SD) can further accelerate MoE inference by drafting multiple tokens per step and verifying them in parallel. However, combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification. Existing MoE offloading systems are SD-agnostic and do not address this bottleneck. We present SP-MoE, the first SD-aware expert-offloading and compute-communication pipelining framework. SP-MoE introduces: (1) speculative expert prefetching that exploits structural correspondence between the draft and target models to prefetch likely experts ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch depth based on empirical profiles and an analytical latency model, guaranteeing just-in-time availability without overfetch; and (3) a pipelined runtime with asynchronous prefetch threads and batched I/O to hide loading latency. Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10818'>Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes</a></h3><h3><a href='https://arxiv.org/pdf/2510.10818' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>32/45</p><p><b>作者：</b>Kevin Chalmers, Jan B{\ae}kgaard Pedersen</p><p>We present the first spin-free, kernel-lock-free mutex that cooperates with user-mode schedulers and is formally proven FIFO-fair and linearizable using CSP/FDR. Our fairness oracle and stability-based proof method are reusable across coroutine runtime designs. We designed the claim/release protocol for a process-oriented language -- ProcessJ -- to manage the race for claiming shared inter-process communication channels. Internally, we use a lock-free queue to park waiting processes for gaining access to a shared object, such as exclusive access to a shared channel to read from or write to. The queue ensures control and fairness for processes wishing to access a shared resource, as the protocol handles claim requests in the order they are inserted into the queue. We produce CSP models of our protocol and a mutex specification, demonstrating with FDR that our protocol behaves as a locking mutex.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10833'>FIDRS: A Novel Framework for Integrated Distributed Reliable Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.10833' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>33/45</p><p><b>作者：</b>Mehdi Zekriyapanah Gashti</p><p>In this paper we represent a new framework for integrated distributed and reliable systems. In the proposed framework we have used three parts to increase Satisfaction and Performance of this framework. At first we analyze previous frameworks related to integrated systems, then represent new proposed framework in order to improving previous framework, and we discuss its different phases. Finally we compare the results of simulation of the new framework with previous ones. In FIDRS framework, the technique of heterogeneous distributed data base is used to improve Performance and speed in responding to users and in this way we can improve dependability and reliability of framework simultaneously. In extraction phase of the new framework we have used RMSD algorithm that decreases responding time in big database. Finally by using FDIRS framework we succeeded to increase Efficiency, Performance and reliability of integrated systems and remove some of previous frameworks problems.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11189'>A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.11189' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>34/45</p><p><b>作者：</b>Yangyang Wen, Paul Townend, Per-Olov \&quot;Ostberg, Abel Souza, Cl\&#x27;ement Courageux-Sudan</p><p>As microservice-based systems scale across the cloud-edge continuum, traditional centralized scheduling mechanisms increasingly struggle with latency, coordination overhead, and fault tolerance. This paper presents a new architectural direction: leveraging service mesh sidecar proxies as decentralized, in-situ schedulers to enable scalable, low-latency coordination in large-scale, cloud-native environments. We propose embedding lightweight, autonomous scheduling logic into each sidecar, allowing scheduling decisions to be made locally without centralized control. This approach leverages the growing maturity of service mesh infrastructures, which support programmable distributed traffic management. We describe the design of such an architecture and present initial results demonstrating its scalability potential in terms of response time and latency under varying request rates. Rather than delivering a finalized scheduling algorithm, this paper presents a system-level architectural direction and preliminary evidence to support its scalability potential.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11211'>An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models</a></h3><h3><a href='https://arxiv.org/pdf/2510.11211' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>35/45</p><p><b>作者：</b>Sheikh Azizul Hakim, Saem Hasan</p><p>Large language models (LLM) are advanced AI systems trained on extensive textual data, leveraging deep learning techniques to understand and generate human-like language. Today&#x27;s LLMs with billions of parameters are so huge that hardly any single computing node can train, fine-tune, or infer from them. Therefore, several distributed computing techniques are being introduced in the literature to properly utilize LLMs. We have explored the application of distributed computing techniques in LLMs from two angles.  \begin{itemize}  \item We study the techniques that democratize the LLM, that is, how large models can be run on consumer-grade computers. Here, we also implement a novel metaheuristics-based modification to an existing system.  \item We perform a comparative study on three state-of-the-art LLM serving techniques. \end{itemize}</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11513'>An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.11513' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>36/45</p><p><b>作者：</b>Alex Elwood, Tom Deakin, Justin Lovegrove, Chris Nelson</p><p>Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a challenge to scale due to complex data dependencies, memory access patterns and a high-dimensional domain. In this paper, we review the performance bottlenecks within the shared memory parallelization scheme of an existing transport solver on modern many-core architectures with high core counts. With this analysis, we then survey the performance of this solver across a variety of compute hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for shared memory parallelism, present results showing an increase in computational performance over the existing method, and evaluate why performance is improved.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10380'>FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes</a></h3><h3><a href='https://arxiv.org/pdf/2510.10380' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>37/45</p><p><b>作者：</b>Shouxu Lin, Zimeng Pan, Yuhang Yao, Haeyoung Noh, Pei Zhang, Carlee Joe-Wong</p><p>Multi-Model Federated Learning (MMFL) is an emerging direction in Federated Learning (FL) where multiple models are trained in parallel, generally on various datasets. Optimizing the models&#x27; accuracies and training times in the MMFL setting requires adapting to data and system heterogeneity across clients as in single-model FL; these challenges are amplified in the MMFL setting due to additional heterogeneity across models. Neither existing solutions nor na\&quot;ive extensions of single-model FL frameworks efficiently address these challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL training framework. FLAMMABLE optimizes model training by intelligently adapting client batch sizes while engaging them to train multiple carefully chosen models, depending on their system capabilities, in each training round. To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL setting, which may enable future reproducible MMFL research. Extensive evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final model accuracy by 1.3$\sim$5.4\% compared to several known baselines.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10620'>DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism</a></h3><h3><a href='https://arxiv.org/pdf/2510.10620' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>38/45</p><p><b>作者：</b>Chenyu Jiang, Zhenkun Cai, Ye Tian, Zhen Jia, Yida Wang, Chuan Wu</p><p>Context parallelism has emerged as a key technique to support long-context training, a growing trend in generative AI for modern large models. However, existing context parallel methods rely on static parallelization configurations that overlook the dynamic nature of training data, specifically, the variability in sequence lengths and token relationships (i.e., attention patterns) across samples. As a result, these methods often suffer from unnecessary communication overhead and imbalanced computation. In this paper, we present DCP, a dynamic context parallel training framework that introduces fine-grained blockwise partitioning of both data and computation. By enabling flexible mapping of data and computation blocks to devices, DCP can adapt to varying sequence characteristics, effectively reducing communication and improving memory and computation balance. Micro-benchmarks demonstrate that DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns. Additionally, we observe up to 0.94x~1.16x end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse masks.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.11697'>A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem</a></h3><h3><a href='https://arxiv.org/pdf/2510.11697' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>39/45</p><p><b>作者：</b>Matteo Mordacchini, Emanuele Carlini, Patrizio Dazzi</p><p>We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in a decentralized network. MWVC, a classical NP-hard problem, is foundational in applications such as network monitoring and resource placement. We propose a fully decentralized protocol where each node makes decisions using only local knowledge and communicates with its neighbors. The method is adaptive, communication-efficient, and avoids centralized coordination. We evaluate the protocol on real-world and synthetic graphs, comparing it to both centralized and decentralized baselines. Our results demonstrate competitive solution quality with reduced communication overhead, highlighting the feasibility of MWVC computation in decentralized environments.</p><p><h4>cs.DC, cs.DS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09690'>A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022</a></h3><h3><a href='https://arxiv.org/pdf/2510.09690' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>40/45</p><p><b>作者：</b>Morteza Sargolzaei Javan</p><p>Cloud computing has become the foundation of modern digital infrastructure, yet the absence of a unified architectural and compliance framework impedes interoperability, auditability, and robust security. This paper introduces a formal, machine-readable semantic model for Cloud Engines, integrating the architectural taxonomy of ISO/IEC 22123 (Cloud Reference Architecture) with the security and compliance controls of ISO/IEC 27001:2022 and ISO/IEC TR 3445:2022. The model decomposes cloud systems into four canonical interfaces--Control, Business, Audit, and Data--and extends them with a security ontology that maps mechanisms such as authentication, authorization, and encryption to specific compliance controls. Expressed in RDF/Turtle, the model enables semantic reasoning, automated compliance validation, and vendor-neutral architecture design. We demonstrate its practical utility through OpenStack and AWS case studies, and provide reproducible validation workflows using SPARQL and SHACL. This work advances the state of cloud security modeling by bridging architectural and compliance standards in a unified framework, with a particular emphasis on auditability.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09799'>Distributed clustering in partially overlapping feature spaces</a></h3><h3><a href='https://arxiv.org/pdf/2510.09799' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>41/45</p><p><b>作者：</b>Alessio Maritan, Luca Schenato</p><p>We introduce and address a novel distributed clustering problem where each participant has a private dataset containing only a subset of all available features, and some features are included in multiple datasets. This scenario occurs in many real-world applications, such as in healthcare, where different institutions have complementary data on similar patients. We propose two different algorithms suitable for solving distributed clustering problems that exhibit this type of feature space heterogeneity. The first is a federated algorithm in which participants collaboratively update a set of global centroids. The second is a one-shot algorithm in which participants share a statistical parametrization of their local clusters with the central server, who generates and merges synthetic proxy datasets. In both cases, participants perform local clustering using algorithms of their choice, which provides flexibility and personalized computational costs. Pretending that local datasets result from splitting and masking an initial centralized dataset, we identify some conditions under which the proposed algorithms are expected to converge to the optimal centralized solution. Finally, we test the practical performance of the algorithms on three public datasets.</p><p><h4>cs.DS, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10028'>Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2510.10028' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>42/45</p><p><b>作者：</b>Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</p><p>The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.</p><p><h4>cs.LG, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10531'>A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)</a></h3><h3><a href='https://arxiv.org/pdf/2510.10531' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>43/45</p><p><b>作者：</b>Guillaume Ambal, George Hodgkins, Mark Madler, Gregory Chockler, Brijesh Dongol, Joseph Izraelevitz, Azalea Raad, Viktor Vafeiadis</p><p>Remote Direct Memory Access (RDMA) is a memory technology that allows remote devices to directly write to and read from each other&#x27;s memory, bypassing components such as the CPU and operating system. This enables low-latency high-throughput networking, as required for many modern data centres, HPC applications and AI/ML workloads. However, baseline RDMA comprises a highly permissive weak memory model that is difficult to use in practice and has only recently been formalised. In this paper, we introduce the Library of Composable Objects (LOCO), a formally verified library for building multi-node objects on RDMA, filling the gap between shared memory and distributed system programming. LOCO objects are well-encapsulated and take advantage of the strong locality and the weak consistency characteristics of RDMA. They have performance comparable to custom RDMA systems (e.g. distributed maps), but with a far simpler programming model amenable to formal proofs of correctness. To support verification, we develop a novel modular declarative verification framework, called Mowgli, that is flexible enough to model multinode objects and is independent of a memory consistency model. We instantiate Mowgli with the RDMA memory model, and use it to verify correctness of LOCO libraries.</p><p><h4>cs.PL, cs.DC, cs.LO, cs.SY, eess.SY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.10570'>Multitask Learning with Learned Task Relationships</a></h3><h3><a href='https://arxiv.org/pdf/2510.10570' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>44/45</p><p><b>作者：</b>Zirui Wan, Stefan Vlaski</p><p>Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.</p><p><h4>cs.LG, cs.DC, cs.MA</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08479'>Rethinking Provenance Completeness with a Learning-Based Linux Scheduler</a></h3><h3><a href='https://arxiv.org/pdf/2510.08479' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>45/45</p><p><b>作者：</b>Jinsong Mao, Benjamin E. Ujcich, Shiqing Ma</p><p>Provenance plays a critical role in maintaining traceability of a system&#x27;s actions for root cause analysis of security threats and impacts. Provenance collection is often incorporated into the reference monitor of systems to ensure that an audit trail exists of all events, that events are completely captured, and that logging of such events cannot be bypassed. However, recent research has questioned whether existing state-of-the-art provenance collection systems fail to ensure the security guarantees of a true reference monitor due to the &#x27;super producer threat&#x27; in which provenance generation can overload a system to force the system to drop security-relevant events and allow an attacker to hide their actions. One approach towards solving this threat is to enforce resource isolation, but that does not fully solve the problems resulting from hardware dependencies and performance limitations.  In this paper, we show how an operating system&#x27;s kernel scheduler can mitigate this threat, and we introduce Aegis, a learned scheduler for Linux specifically designed for provenance. Unlike conventional schedulers that ignore provenance completeness requirements, Aegis leverages reinforcement learning to learn provenance task behavior and to dynamically optimize resource allocation. We evaluate Aegis&#x27;s efficacy and show that Aegis significantly improves both the completeness and efficiency of provenance collection systems compared to traditional scheduling, while maintaining reasonable overheads and even improving overall runtime in certain cases compared to the default Linux scheduler.</p><p><h4>cs.CR, cs.OS</h4></p></div><hr>
</body>
</html>
