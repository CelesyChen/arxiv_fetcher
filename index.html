<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-10</h1>
<div><h3><a href='https://arxiv.org/abs/2510.08230'>pyGinkgo: A Sparse Linear Algebra Operator Framework for Python</a></h3><h3><a href='https://arxiv.org/pdf/2510.08230' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/26</p><p><b>作者：</b>Keshvi Tuteja, Gregor Olenik, Roman Mishchuk, Yu-Hsiang Tsai, Markus G\&quot;otz, Achim Streit, Hartwig Anzt, Charlotte Debus</p><p>Sparse linear algebra is a cornerstone of many scientific computing and machine learning applications. Python has become a popular choice for these applications due to its simplicity and ease of use. Yet high performance sparse kernels in Python remain limited in functionality, especially on modern CPU and GPU architectures. We present pyGinkgo, a lightweight and Pythonic interface to the Ginkgo library, offering high-performance sparse linear algebra support with platform portability across CUDA, HIP, and OpenMP backends. pyGinkgo bridges the gap between high-performance C++ backends and Python usability by exposing Ginkgo&#x27;s capabilities via Pybind11 and a NumPy and PyTorch compatible interface. We benchmark pyGinkgo&#x27;s performance against state-of-the-art Python libraries including SciPy, CuPy, PyTorch, and TensorFlow. Results across hardware from different vendors demonstrate that pyGinkgo consistently outperforms existing Python tools in both sparse matrix vector (SpMV) product and iterative solver performance, while maintaining performance parity with native Ginkgo C++ code. Our work positions pyGinkgo as a compelling backend for sparse machine learning models and scientific workflows.</p><p><h4>cs.MS, cs.DC, cs.PF, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2410.08618'>SwitchFS: Asynchronous Metadata Updates for Distributed Filesystems with In-Network Coordination</a></h3><h3><a href='https://arxiv.org/pdf/2410.08618' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/26</p><p><b>作者：</b>Jingwei Xu, Mingkai Dong, Qiulin Tian, Ziyi Tian, Tong Xin, Haibo Chen</p><p>Distributed filesystem metadata updates are typically synchronous. This creates inherent challenges for access efficiency, load balancing, and directory contention, especially under dynamic and skewed workloads. This paper argues that synchronous updates are overly conservative. We propose SwitchFS with asynchronous metadata updates that allow operations to return early and defer directory updates until reads, both hiding latency and amortizing overhead. The key challenge lies in efficiently maintaining the synchronous POSIX semantics of metadata updates. To address this, SwitchFS is co-designed with a programmable switch, leveraging the limited on-switch resources to track directory states with negligible overhead. This allows SwitchFS to aggregate and apply delayed updates efficiently, using batching and consolidation before directory reads. Evaluation shows that SwitchFS achieves up to 13.34$\times$ and 3.85$\times$ higher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art distributed filesystems, Emulated-InfiniFS and Emulated-CFS, respectively, under skewed workloads. For real-world workloads, SwitchFS improves end-to-end throughput by 21.1$\times$, 1.1$\times$, and 0.3$\times$ over CephFS, Emulated-InfiniFS, and Emulated-CFS, respectively.</p><p><h4>cs.DC, cs.OS, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.01616'>Phantora: Maximizing Code Reuse in Simulation-based Machine Learning System Performance Estimation</a></h3><h3><a href='https://arxiv.org/pdf/2505.01616' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/26</p><p><b>作者：</b>Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Tianjun Yuan, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo</p><p>Modern machine learning (ML) training workloads place substantial demands on both computational and communication resources. Consequently, accurate performance estimation has become increasingly critical for guiding system design decisions, such as the selection of parallelization strategies, cluster configurations, and hardware provisioning. Existing simulation-based performance estimation requires reimplementing the ML framework in a simulator, which demands significant manual effort and is hard to maintain as ML frameworks evolve rapidly.  This paper introduces Phantora, a hybrid GPU cluster simulator designed for performance estimation of ML training workloads. Phantora executes unmodified ML frameworks as is within a distributed, containerized environment. Each container emulates the behavior of a GPU server in a large-scale cluster, while Phantora intercepts and simulates GPU- and communication-related operations to provide high-fidelity performance estimation. We call this approach hybrid simulation of ML systems, in contrast to traditional methods that simulate static workloads. The primary advantage of hybrid simulation is that it allows direct reuse of ML framework source code in simulation, avoiding the need for reimplementation. Our evaluation shows that Phantora provides accuracy comparable to static workload simulation while supporting three state-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora operates on a single GPU, eliminating the need for the resource-intensive trace collection and workload extraction steps required by traditional trace-based simulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.</p><p><h4>cs.DC, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07449'>How long can you sleep? Idle Time System Inefficiencies and Opportunities</a></h3><h3><a href='https://arxiv.org/pdf/2510.07449' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/26</p><p><b>作者：</b>Georgia Antoniou (University of Cyprus), Haris Volos (University of Cyprus), Jawad Haj Yahya (Rivos Inc), Yiannakis Sazeides (University of Cyprus)</p><p>This work introduces a model-based framework that reveals the idle opportunity of modern servers running latency-critical applications. Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to estimate the theoretical idle time distribution at the CPU core and system (package) level. A comparison of the actual idleness of a real server and that from the theoretical models reveals significant missed opportunities to enter deep idle states. This inefficiency is attributed to the idle-governor inaccuracy and the high latency to transition to/from legacy deep-idle states. The proposed methodology offers the means for an early-stage design exploration and insights into idle time behavior and opportunities for varying server system configurations and load.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07719'>DL-PIM: Improving Data Locality in Processing-in-Memory Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.07719' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/26</p><p><b>作者：</b>Parker Hao Tian, Zahra Yousefijamarani, Alaa Alameldeen</p><p>PIM architectures aim to reduce data transfer costs between processors and memory by integrating processing units within memory layers. Prior PIM architectures have shown potential to improve energy efficiency and performance. However, such advantages rely on data proximity to the processing units performing computations. Data movement overheads can degrade PIM&#x27;s performance and energy efficiency due to the need to move data between a processing unit and a distant memory location. %they face challenges due to the overhead of transferring data from remote memory locations to processing units inside memory for computation. In this paper, we demonstrate that a large fraction of PIM&#x27;s latency per memory request is attributed to data transfers and queuing delays from remote memory accesses. To improve PIM&#x27;s data locality, we propose DL-PIM, a novel architecture that dynamically detects the overhead of data movement, and proactively moves data to a reserved area in the local memory of the requesting processing unit. DL-PIM uses a distributed address-indirection hardware lookup table to redirect traffic to the current data location. We propose DL-PIM implementations on two 3D stacked memories: HMC and HBM. While some workloads benefit from DL-PIM, others are negatively impacted by the additional latency due to indirection accesses. Therefore, we propose an adaptive mechanism that assesses the cost and benefit of indirection and dynamically enables or disables it to prevent degrading workloads that suffer from indirection. Overall, DL-PIM reduces the average memory latency per request by 54% in HMC and 50% in HBM which resulted in performance improvement of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup in HBM, showing that DL-PIM enhances data locality and overall system performance.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08137'>A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations</a></h3><h3><a href='https://arxiv.org/pdf/2510.08137' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/26</p><p><b>作者：</b>Anastasios Petropoulos, Theodore Antonakopoulos</p><p>Deep neural network (DNN) inference relies increasingly on specialized hardware for high computational efficiency. This work introduces a field-programmable gate array (FPGA)-based dynamically configurable accelerator featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two processing unit (PU) configurations with different computing capabilities using the same interfaces and peripheral blocks. By instantiating multiple PUs and employing a heuristic weight transfer schedule, the architecture achieves notable throughput efficiency over prior works. Moreover, we outline how the architecture can be extended to emulate analog in-memory computing (AIMC) devices to aid next-generation heterogeneous AIMC chip designs and investigate device-level noise behavior. Overall, this brief presents a versatile DNN inference acceleration architecture adaptable to various models and future FPGA designs.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08351'>FMCache: File-System Metadata Caching in Programmable Switches</a></h3><h3><a href='https://arxiv.org/pdf/2510.08351' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/26</p><p><b>作者：</b>Qingxiu Liu (The Chinese University of Hong Kong), Jiazhen Cai (The Chinese University of Hong Kong), Siyuan Sheng (The Chinese University of Hong Kong), Yuhui Chen (Xiamen University), Lu Tang (Xiamen University), Zhirong Shen (Xiamen University), Patrick P. C. Lee (The Chinese University of Hong Kong)</p><p>Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We propose FMCache, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, FMCache addresses file-system-specific path dependencies under stringent switch resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. FMCache achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08544'>SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference</a></h3><h3><a href='https://arxiv.org/pdf/2510.08544' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/26</p><p><b>作者：</b>Hengrui Zhang, Pratyush Patel, August Ning, David Wentzlaff</p><p>Large Language Models (LLMs) have gained popularity in recent years, driving up the demand for inference. LLM inference is composed of two phases with distinct characteristics: a compute-bound prefill phase followed by a memory-bound decode phase. To efficiently serve LLMs, prior work proposes prefill-decode disaggregation to run each phase on separate hardware. However, existing hardware poorly matches the different requirements of each phase. Current datacenter GPUs and TPUs follow a more-is-better design philosophy that maximizes compute and memory resources, causing memory bandwidth underutilization in the prefill phase and compute underutilization in the decode phase. Such underutilization directly translates into increased serving costs.  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting a less-is-more methodology to design specialized chips tailored to the distinct characteristics of prefill and decode phases. The proposed Prefill Chips have larger systolic arrays and use cost-effective GDDR memory, whereas the proposed Decode Chips retain high memory bandwidth but reduce compute capacity. Compared to modeled H100s, simulations show that the proposed Prefill Chips deliver 8% higher prefill performance on average at 52% lower hardware cost, while the proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.  End-to-end simulations on production traces show that SPAD reduces hardware cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while offering the same performance. Even when models and workloads change, SPAD can reallocate either type of chip to run either phase and still achieve 11%-43% lower hardware costs, demonstrating the longevity of the SPAD design.</p><p><h4>cs.AR, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.14053'>MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging</a></h3><h3><a href='https://arxiv.org/pdf/2508.14053' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/26</p><p><b>作者：</b>Jinwei Tang (Katie), Jiayin Qin (Katie), Nuo Xu (Katie), Pragnya Sudershan Nalla (Katie), Yu Cao (Katie),  Yang (Katie),  Zhao, Caiwen Ding</p><p>As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.</p><p><h4>cs.AR, cs.AI, cs.MA</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2412.05393'>HiVeGen -- Hierarchical LLM-based Verilog Generation for Scalable Chip Design</a></h3><h3><a href='https://arxiv.org/pdf/2412.05393' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/26</p><p><b>作者：</b>Jinwei Tang (Katie), Jiayin Qin (Katie), Kiran Thorat (Katie), Chen Zhu-Tian (Katie), Yu Cao (Katie),  Yang (Katie),  Zhao, Caiwen Ding</p><p>With Large Language Models (LLMs) recently demonstrating impressive proficiency in code generation, it is promising to extend their abilities to Hardware Description Language (HDL). However, LLMs tend to generate single HDL code blocks rather than hierarchical structures for hardware designs, leading to hallucinations, particularly in complex designs like Domain-Specific Accelerators (DSAs). To address this, we propose HiVeGen, a hierarchical LLM-based Verilog generation framework that decomposes generation tasks into LLM-manageable hierarchical submodules. HiVeGen further harnesses the advantages of such hierarchical structures by integrating automatic Design Space Exploration (DSE) into hierarchy-aware prompt generation, introducing weight-based retrieval to enhance code reuse, and enabling real-time human-computer interaction to lower error-correction cost, significantly improving the quality of generated designs.</p><p><h4>cs.LG, cs.AI, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07811'>Adaptive Execution Scheduler for DataDios SmartDiff</a></h3><h3><a href='https://arxiv.org/pdf/2510.07811' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/26</p><p><b>作者：</b>Aryan Poduri</p><p>We present an adaptive scheduler for a single differencing engine (SmartDiff) with two execution modes: (i) in-memory threads and (ii) Dask based parallelism. The scheduler continuously tunes batch size and worker/thread count within fixed CPU and memory budgets to minimize p95 latency. A lightweight preflight profiler estimates bytes/row and I/O rate; an online cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors lower latency with backpressure and straggler mitigation. Backend selection is gated by a conservative working-set estimate so that in-memory execution is chosen when safe, otherwise Dask is used. Across synthetic and public tabular benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines), while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed) with zero OOMs and comparable throughput.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08164'>A Multi-Simulation Bridge for IoT Digital Twins</a></h3><h3><a href='https://arxiv.org/pdf/2510.08164' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/26</p><p><b>作者：</b>Marco Picone, Samuele Burattini, Marco Melloni, Prasad Talasila, Davide Ziglioli, Matteo Martinelli, Nicola Bicocchi, Alessandro Ricci, Peter Gorm Larsen</p><p>The increasing capabilities of Digital Twins (DTs) in the context of the Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless integration with simulation platforms to support system design, validation, and real-time operation. This paper introduces the concept, design, and experimental evaluation of the DT Simulation Bridge - a software framework that enables diverse interaction patterns between active DTs and simulation environments. The framework supports both the DT development lifecycle and the incorporation of simulations during active operation. Through bidirectional data exchange, simulations can update DT models dynamically, while DTs provide real-time feedback to adapt simulation parameters. We describe the architectural design and core software components that ensure flexible interoperability and scalable deployment. Experimental results show that the DT Simulation Bridge enhances design agility, facilitates virtual commissioning, and supports live behavioral analysis under realistic conditions, demonstrating its effectiveness across a range of industrial scenarios.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08180'>Towards Energy-Efficient Serverless Computing with Hardware Isolation</a></h3><h3><a href='https://arxiv.org/pdf/2510.08180' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/26</p><p><b>作者：</b>Natalie Carl, Tobias Pfandzelter, David Bermbach</p><p>Serverless computing provides just-in-time infrastructure provisioning with rapid elasticity and a finely-grained pricing model. As full control of resource allocation is in the hands of the cloud provider and applications only consume resources when they actually perform work, we believe that serverless computing is uniquely positioned to maximize energy efficiency.  However, the focus of current serverless platforms is to run hundreds or thousands of serverless functions from different tenants on traditional server hardware, requiring expensive software isolation mechanisms and a high degree of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared caches, high clock frequencies, and many-core architectures, servers today are optimized for large, singular workloads but not to run thousands of isolated functions.  We propose rethinking the serverless hardware architecture to align it with the requirements of serverless software. Specifically, we propose using hardware isolation with individual processors per function instead of software isolation resulting in a serverless hardware stack that consumes energy only when an application actually performs work. In preliminary evaluation with real hardware and a typical serverless workload we find that this could reduce energy consumption overheads by 90.63% or an average 70.8MW.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08228'>Distributed Resource Selection for Self-Organising Cloud-Edge Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.08228' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/26</p><p><b>作者：</b>Quentin Renau, Amjad Ullah, Emma Hart</p><p>This paper presents a distributed resource selection mechanism for diverse cloud-edge environments, enabling dynamic and context-aware allocation of resources to meet the demands of complex distributed applications. By distributing the decision-making process, our approach ensures efficiency, scalability, and resilience in highly dynamic cloud-edge environments where centralised coordination becomes a bottleneck. The proposed mechanism aims to function as a core component of a broader, distributed, and self-organising orchestration system that facilitates the intelligent placement and adaptation of applications in real-time. This work leverages a consensus-based mechanism utilising local knowledge and inter-agent collaboration to achieve efficient results without relying on a central controller, thus paving the way for distributed orchestration. Our results indicate that computation time is the key factor influencing allocation decisions. Our approach consistently delivers rapid allocations without compromising optimality or incurring additional cost, achieving timely results at scale where exhaustive search is infeasible and centralised heuristics run up to 30 times slower.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08244'>Energy-Efficient Maximal Independent Sets in Radio Networks</a></h3><h3><a href='https://arxiv.org/pdf/2510.08244' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/26</p><p><b>作者：</b>Dominick Banasik, Varsha Dani, Fabien Dufoulon, Aayush Gupta, Thomas P. Hayes, Gopal Pandurangan</p><p>The maximal independent set (MIS) is one of the most fundamental problems in distributed computing, and it has been studied intensively for over four decades. This paper focuses on the MIS problem in the Radio Network model, a standard model widely used to model wireless networks, particularly ad hoc wireless and sensor networks. Energy is a premium resource in these networks, which are typically battery-powered. Hence, designing distributed algorithms that use as little energy as possible is crucial. We use the well-established energy model where a node can be sleeping or awake in a round, and only the awake rounds (when it can send or listen) determine the energy complexity of the algorithm, which we want to minimize.  We present new, more energy-efficient MIS algorithms in radio networks with arbitrary and unknown graph topology. We present algorithms for two popular variants of the radio model -- with collision detection (CD) and without collision detection (no-CD). Specifically, we obtain the following results:  1. CD model: We present a randomized distributed MIS algorithm with energy complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability $1 / poly(n)$, where $n$ is the network size. We show that our energy complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.  2. no-CD model: In the more challenging no-CD model, we present a randomized distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$, round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 / poly(n)$. The energy complexity of our algorithm is significantly lower than the round (and energy) complexity of $O(\log^3 n)$ of the best known distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.</p><p><h4>cs.DC, cs.DS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08536'>Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver</a></h3><h3><a href='https://arxiv.org/pdf/2510.08536' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/26</p><p><b>作者：</b>Gregor Olenik, Marcel Koch, Hartwig Anzt</p><p>Modern high-performance computing (HPC) increasingly relies on GPUs, but integrating GPU acceleration into complex scientific frameworks like OpenFOAM remains a challenge. Existing approaches either fully refactor the codebase or use plugin-based GPU solvers, each facing trade-offs between performance and development effort. In this work, we address the limitations of plugin-based GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better balances CPU matrix assembly and GPU-based linear solves. We present a detailed computational model, describe a novel matrix repartitioning and update procedure, and evaluate its performance on large-scale CFD simulations. Our results show that the proposed method significantly mitigates oversubscription issues, improving solver performance and resource utilization in heterogeneous CPU-GPU environments.</p><p><h4>cs.DC, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07664'>FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning</a></h3><h3><a href='https://arxiv.org/pdf/2510.07664' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/26</p><p><b>作者：</b>Yunbo Li, Jiaping Gui, Zhihang Deng, Fanchao Meng, Yue Wu</p><p>Federated learning (FL) enables collaborative model training across multiple parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as a balanced approach between synchronous and asynchronous FL. However, SAFL faces significant challenges in optimizing both gradient-based (e.g., FedSGD) and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct trade-offs in accuracy, convergence speed, and stability. While gradient aggregation achieves faster convergence and higher accuracy, it suffers from pronounced fluctuations, whereas model aggregation offers greater stability but slower convergence and suboptimal accuracy. This paper presents FedQS, the first framework to theoretically analyze and address these disparities in SAFL. FedQS introduces a divide-and-conquer strategy to handle client heterogeneity by classifying clients into four distinct types and adaptively optimizing their local training based on data distribution characteristics and available computational resources. Extensive experiments on computer vision, natural language processing, and real-world tasks demonstrate that FedQS achieves the highest accuracy, attains the lowest loss, and ranks among the fastest in convergence speed, outperforming state-of-the-art baselines. Our work bridges the gap between aggregation strategies in SAFL, offering a unified solution for stable, accurate, and efficient federated learning. The code and datasets are available at https://anonymous.4open.science/r/FedQS-EDD6.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07922'>SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening</a></h3><h3><a href='https://arxiv.org/pdf/2510.07922' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/26</p><p><b>作者：</b>Murtaza Rangwala, Farag Azzedin, Richard O. Sinnott, Rajkumar Buyya</p><p>Decentralized Federated Learning (DFL) enables privacy-preserving collaborative training without centralized servers, but remains vulnerable to Byzantine attacks where malicious clients submit corrupted model updates. Existing Byzantine-robust DFL defenses rely on similarity-based neighbor screening that requires every client to exchange and compare complete high-dimensional model vectors with all neighbors in each training round, creating prohibitive communication and computational costs that prevent deployment at web scale. We propose SketchGuard, a general framework that decouples Byzantine filtering from model aggregation through sketch-based neighbor screening. SketchGuard compresses $d$-dimensional models to $k$-dimensional sketches ($k \ll d$) using Count Sketch for similarity comparisons, then selectively fetches full models only from accepted neighbors, reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| + d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \le |N_i|$ is the accepted neighbor count. We establish rigorous convergence guarantees in both strongly convex and non-convex settings, proving that Count Sketch compression preserves Byzantine resilience with controlled degradation bounds where approximation errors introduce only a $(1+O(\epsilon))$ factor in the effective threshold parameter. Comprehensive experiments across multiple datasets, network topologies, and attack scenarios demonstrate that SketchGuard maintains identical robustness to state-of-the-art methods while reducing computation time by up to 82% and communication overhead by 50-70% depending on filtering effectiveness, with benefits scaling multiplicatively with model dimensionality and network connectivity. These results establish the viability of sketch-based compression as a fundamental enabler of robust DFL at web scale.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08055'>From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill</a></h3><h3><a href='https://arxiv.org/pdf/2510.08055' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/26</p><p><b>作者：</b>Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn</p><p>Large Language Model (LLM) inference in production must meet stringent service-level objectives for both time-to-first-token (TTFT) and time-between-token (TBT) while maximizing throughput under fixed compute, memory, and interconnect budgets. Modern serving systems adopt stall-free scheduling techniques such as chunked prefill, which splits long prompt processing along the token dimension and interleaves prefill with ongoing decode iterations. While effective at stabilizing TBT, chunked prefill incurs substantial overhead in Mixture-of-Experts (MoE) models: redundant expert weight loads increase memory traffic by up to 39% and inflate energy consumption. We propose layered prefill, a new scheduling paradigm that treats transformer layer groups as the primary scheduling unit. By vertically partitioning the model into contiguous layer groups and interleaving prefill and decode across the groups, layered prefill sustains stall-free decoding while eliminating chunk-induced MoE weight reloads. It reduces off-chip bandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and per-token energy by up to 22%. Evaluations show that layered prefill consistently improves the TTFT--TBT Pareto frontier over chunked prefill, reducing expert-load traffic and energy cost while maintaining stall-free decoding. Overall, shifting the scheduling axis from tokens to layers unlocks a new operating regime for high-efficiency, energy-aware LLM serving in co-located environments.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08522'>DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.08522' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/26</p><p><b>作者：</b>Yuanjun Dai, Keqiang He, An Wang</p><p>Existing batch size selection approaches in dis- tributed machine learning rely on static allocation or simplistic heuristics that fail to adapt to heterogeneous, dynamic computing environments. We present DYNAMIX, a reinforcement learning framework that formulates batch size optimization as a sequen- tial decision-making problem using Proximal Policy Optimiza- tion (PPO). Our approach employs a multi-dimensional state representation encompassing network-level metrics, system-level resource utilization, and training statistical efficiency indicators to enable informed decision-making across diverse computational resources. Our approach eliminates the need for explicit system modeling while integrating seamlessly with existing distributed training frameworks. Through evaluations across diverse work- loads, hardware configurations, and network conditions, DY- NAMIX achieves up to 6.3% improvement in the final model accuracy and 46% reduction in the total training time. Our scalability experiments demonstrate that DYNAMIX maintains the best performance as cluster size increases to 32 nodes, while policy transfer experiments show that learned policies generalize effectively across related model architectures.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07901'>Decentralised Blockchain Management Through Digital Twins</a></h3><h3><a href='https://arxiv.org/pdf/2510.07901' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/26</p><p><b>作者：</b>Georgios Diamantopoulos, Nikos Tziritas, Rami Bahsoon, Georgios Theodoropoulos</p><p>The necessity of blockchain systems to remain decentralised limits current solutions to blockchain governance and dynamic management, forcing a trade-off between control and decentralisation. In light of the above, this work proposes a dynamic and decentralised blockchain management mechanism based on digital twins. To ensure decentralisation, the proposed mechanism utilises multiple digital twins that the system&#x27;s stakeholders control. To facilitate decentralised decision-making, the twins are organised in a secondary blockchain system that orchestrates agreement on, and propagation of decisions to the managed blockchain. This enables the management of blockchain systems without centralised control. A preliminary evaluation of the performance and impact of the overheads introduced by the proposed mechanism is conducted through simulation. The results demonstrate the proposed mechanism&#x27;s ability to reach consensus on decisions quickly and reconfigure the primary blockchain with minimal overhead.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08072'>When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains</a></h3><h3><a href='https://arxiv.org/pdf/2510.08072' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/26</p><p><b>作者：</b>Vamsi Addanki</p><p>As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, collective communication has emerged as a critical bottleneck in scale-up systems. Programmable photonic interconnects offer a promising path forward: by dynamically reconfiguring the fabric, they can establish direct, high-bandwidth optical paths between communicating endpoints -- \emph{synchronously and guided by the structure of collective operations} (e.g., AllReduce). However, realizing this vision -- \emph{when light bends to the collective will} -- requires navigating a fundamental trade-off between reconfiguration delay and the performance gains of adaptive topologies.  In this paper, we present a simple theoretical framework for adaptive photonic scale-up domains that makes this trade-off explicit and clarifies when reconfiguration is worthwhile. Along the way, we highlight a connection -- not surprising but still powerful -- between the Birkhoff--von Neumann (BvN) decomposition, maximum concurrent flow (a classic measure of network throughput), and the well-known $\alpha$--$\beta$ cost model for collectives. Finally, we outline a research agenda in algorithm design and systems integration that can build on this foundation.</p><p><h4>cs.NI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08139'>BlockSDN: Towards a High-Performance Blockchain via Software-Defined Cross Networking optimization</a></h3><h3><a href='https://arxiv.org/pdf/2510.08139' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/26</p><p><b>作者：</b>Wenyang Jia, Jingjing Wang, Ziwei Yan, Xiangli Peng, Guohui Yuan</p><p>The scalability of blockchain systems is constrained by inefficient P2P broadcasting, as most existing optimizations focus only on the logical layer without considering physical network conditions. To address this, we propose BlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN employs a distributed control plane for a global network view, a graph engine for hierarchical clustering, and a hybrid macro-micro neighbor selection with hierarchical broadcasting. A dedicated simulation platform shows that BlockSDN reduces global block synchronization time by 65% and 55% compared to Gossip and Mercury, respectively.These results highlight the potential of SDN-enabled cross-layer coordination to significantly enhance blockchain scalability and performance.</p><p><h4>cs.NI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2504.19519'>Efficient and Adaptable Overlapping for Computation and Communication via Signaling and Reordering</a></h3><h3><a href='https://arxiv.org/pdf/2504.19519' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/26</p><p><b>作者：</b>Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang</p><p>Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency becomes an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, which utilizes a novel signaling mechanism: when part of the output finishes, the computation kernel sends a signal to trigger the communication of that part, while continuing the computation of the remaining part (interference-free computation). Consequently, the communication of the finished part and the computation of the remaining part can be overlapped. On top of the signaling mechanism, FlashOverlap comprises two key components: (1) the determination of the signaling timing to boost the overlap efficiency (tile-wise overlapping), and (2) a pre-communication reordering to create the contiguous address for finished data, enabling communication by simply calling NCCL APIs (communication agnosticism), and a post-communication reordering to correct the data order. Experiments show that FlashOverlap achieves up to 1.65x speedup through overlap, outperforming existing works in most cases. Code is available at https://github.com/infinigence/FlashOverlap.</p><p><h4>cs.DC, cs.CL, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.05258'>Scaling Performance of Large Language Model Pretraining</a></h3><h3><a href='https://arxiv.org/pdf/2509.05258' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/26</p><p><b>作者：</b>Alexander Interrante-Grant, Carla Varela-Rosa, Suhaas Narayan, Chris Connelly, Albert Reuther</p><p>Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, very little information about the scaling performance and training considerations of these large training pipelines is released publicly. Working with very large datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08479'>Rethinking Provenance Completeness with a Learning-Based Linux Scheduler</a></h3><h3><a href='https://arxiv.org/pdf/2510.08479' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/26</p><p><b>作者：</b>Jinsong Mao, Benjamin E. Ujcich, Shiqing Ma</p><p>Provenance plays a critical role in maintaining traceability of a system&#x27;s actions for root cause analysis of security threats and impacts. Provenance collection is often incorporated into the reference monitor of systems to ensure that an audit trail exists of all events, that events are completely captured, and that logging of such events cannot be bypassed. However, recent research has questioned whether existing state-of-the-art provenance collection systems fail to ensure the security guarantees of a true reference monitor due to the &#x27;super producer threat&#x27; in which provenance generation can overload a system to force the system to drop security-relevant events and allow an attacker to hide their actions. One approach towards solving this threat is to enforce resource isolation, but that does not fully solve the problems resulting from hardware dependencies and performance limitations.  In this paper, we show how an operating system&#x27;s kernel scheduler can mitigate this threat, and we introduce Venus, a learned scheduler for Linux specifically designed for provenance. Unlike conventional schedulers that ignore provenance completeness requirements, Venus leverages reinforcement learning to learn provenance task behavior and to dynamically optimize resource allocation. We evaluate Venus&#x27;s efficacy and show that Venus significantly improves both the completeness and efficiency of provenance collection systems compared to traditional scheduling, while maintaining reasonable overheads and even improving overall runtime in certain cases compared to the default Linux scheduler.</p><p><h4>cs.CR, cs.OS</h4></p></div><hr>
</body>
</html>
