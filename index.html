<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-11-05</h1>
<div><h3><a href='https://arxiv.org/abs/2511.00342'>MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research</a></h3><h3><a href='https://arxiv.org/pdf/2511.00342' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/36</p><p><b>作者：</b>Hendrio Braganca, Diego Kreutz, Vanderson Rocha, Joner Assolin, and Eduardo Feitosa</p><p>We present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset&#x27;s invaluable role in understanding the evolving landscape of malware.</p><p><h4>cs.CR, cs.AI, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00592'>Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2511.00592' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/36</p><p><b>作者：</b>Massinissa Merouani, Islem Kara Bernou, Riyadh Baghdadi</p><p>Automatic code optimization remains a difficult challenge, particularly for complex loop nests on modern hardware. This paper investigates a novel approach to code optimization where Large Language Models (LLMs) guide the process through a closed-loop interaction with a compiler. We present ComPilot, an experimental framework that leverages off-the-shelf LLMs, without any task-specific fine-tuning, as interactive optimization agents. ComPilot establishes a feedback loop where an LLM proposes transformations for a given loop nest to a compiler. The compiler attempts the transformations, reporting back legality status and measured speedup or slowdown. The LLM utilizes this concrete feedback to iteratively refine its optimization strategy. Our extensive evaluation across the PolyBench benchmark suite demonstrates the effectiveness of this zero-shot approach. ComPilot achieves geometric mean speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original code. Furthermore, ComPilot demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases. This experimental study demonstrates that general-purpose LLMs can effectively guide the code optimization process when grounded by compiler feedback, opening promising research directions for agentic AI in code optimization.</p><p><h4>cs.PL, cs.DC, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01001'>Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver</a></h3><h3><a href='https://arxiv.org/pdf/2511.01001' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/36</p><p><b>作者：</b>Johansell Villalobos, Daniel Caviedes-Voulli\`eme, Silvio Rizzi, Esteban Meneses</p><p>Current climate change has posed a grand challenge in the field of numerical modeling due to its complex, multiscale dynamics. In hydrological modeling, the increasing demand for high-resolution, real-time simulations has led to the adoption of GPU-accelerated platforms and performance portable programming frameworks such as Kokkos. In this work, we present a comprehensive performance study of the SERGHEI-SWE solver, a shallow water equations code, across four state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs, demonstrating consistent scalability with a speedup of 32 and an efficiency upwards of 90\% for most almost all the test range. Roofline analysis reveals that memory bandwidth is the dominant performance bottleneck, with key solver kernels residing in the memory-bound region. To evaluate performance portability, we apply both harmonic and arithmetic mean-based metrics while varying problem size. Results indicate that while SERGHEI-SWE achieves portability across devices with tuned problem sizes (&lt;70\%), there is room for kernel optimization within the solver with more granular control of the architecture specifically by using Kokkos teams and architecture specific tunable parameters. These findings position SERGHEI-SWE as a robust, scalable, and portable simulation tool for large-scale geophysical applications under evolving HPC architectures with potential to enhance its performance.</p><p><h4>cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01244'>Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim</a></h3><h3><a href='https://arxiv.org/pdf/2511.01244' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/36</p><p><b>作者：</b>Wajid Ali, Ayaz Akram, Deepak Shankar</p><p>This paper focuses on the simulation of multi-die System-on-Chip (SoC) architectures using VisualSim, emphasiz- ing chiplet-based system modeling and performance analysis. Chiplet technology presents a promising alternative to traditional monolithic chips, which face increasing challenges in manufactur- ing costs, power efficiency, and performance scaling. By integrat- ing multiple small modular silicon units into a single package, chiplet-based architectures offer greater flexibility and scalability at a lower overall cost. In this study, we developed a detailed sim- ulation model of a chiplet-based system, incorporating multicore ARM processor clusters interconnected through a ARM CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The simulation framework in VisualSim enables the evaluation of critical system metrics, including inter-chiplet communication latency, memory access efficiency, workload distribution, and the power-performance tradeoff under various workloads. Through simulation-driven insights, this research highlights key factors influencing chiplet system performance and provides a foundation for optimizing future chiplet-based semiconductor designs.</p><p><h4>cs.AR, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2504.07042'>Towards a Higher Roofline for Matrix-Vector Multiplication in Matrix-Free HOSFEM</a></h3><h3><a href='https://arxiv.org/pdf/2504.07042' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/36</p><p><b>作者：</b>Zijian Cao, Qiao Sun, Tiangong Zhang, Huiyuan Li</p><p>Modern GPGPUs provide massive arithmetic throughput, yet many scientific kernels remain limited by memory bandwidth. In particular, repeatedly loading precomputed auxiliary data wastes abundant compute resources while stressing the memory hierarchy. A promising strategy is to replace memory traffic with inexpensive recomputation, thereby alleviating bandwidth pressure and enabling applications to better exploit heterogeneous compute units. Guided by this strategy, we optimize the high-order/spectral finite element method (HOSFEM), a widely used approach for solving PDEs. Its performance is largely determined by AxLocal, a matrix-free kernel for element-local matrix-vector multiplications. In AxLocal, geometric factors dominate memory accesses while contributing minimally to computation, creating a bandwidth bottleneck that caps the performance roofline. To address this challenge, we propose the first practical, low-overhead on-the-fly recomputation of geometric factors for trilinear and parallelepiped elements. This reformulation reduces data movement and raises the achievable roofline, revealing untapped optimization potential for tensor contractions. With hardware-aware techniques including loop unrolling, Tensor Core acceleration, and constant memory utilization, the optimized kernels reach 85%-100% of the roofline efficiency. Compared with state-of-the-art implementations in the Nek series, they deliver speedups of 1.74x-4.10x on NVIDIA A100 and 1.99x-3.78x on Hygon K100, leading to a 1.12x-1.40x improvement in the full HOSFEM benchmark. These results demonstrate that combining algorithmic reformulation with hardware-specific tuning can remove long-standing bottlenecks and fully exploit the performance potential of large-scale high-order simulations.</p><p><h4>cs.PF, cs.MS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2308.15179'>On the Role of Search Budgets in Model-Based Software Refactoring Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2308.15179' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/36</p><p><b>作者：</b>J. Andres Diaz-Pace, Daniele Di Pompeo, Michele Tucci</p><p>Software model optimization is a process that automatically generates design alternatives aimed at improving quantifiable non-functional properties of software systems, such as performance and reliability. Multi-objective evolutionary algorithms effectively help designers identify trade-offs among the desired non-functional properties. To reduce the use of computational resources, this work examines the impact of implementing a search budget to limit the search for design alternatives. In particular, we analyze how time budgets affect the quality of Pareto fronts by utilizing quality indicators and exploring the structural features of the generated design alternatives. This study identifies distinct behavioral differences among evolutionary algorithms when a search budget is implemented. It further reveals that design alternatives generated under a budget are structurally different from those produced without one. Additionally, we offer recommendations for designers on selecting algorithms in relation to time constraints, thereby facilitating the effective application of automated refactoring to improve non-functional properties.</p><p><h4>cs.SE, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.19073'>CARMA: Collocation-Aware Resource Manager</a></h3><h3><a href='https://arxiv.org/pdf/2508.19073' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/36</p><p><b>作者：</b>Ehsan Yousefzadeh-Asl-Miandoab, Reza Karimzadeh, Bulat Ibragimov, Florina M. Ciorba, P{\i}nar T\&quot;oz\&quot;un</p><p>GPUs running deep learning (DL) workloads are frequently underutilized. Collocating multiple DL training tasks on the same GPU can improve utilization but introduces two key risks: (1) out-of-memory (OOM) crashes for newly scheduled tasks, and (2) severe performance interference among co-running tasks, which can negate any throughput gains. These issues reduce system robustness, quality of service, and energy efficiency. We present CARMA, a task-level, collocation-aware resource management system for the server-scale. CARMA addresses collocation challenges via (1) fine-grained monitoring and bookkeeping of GPUs and a collocation risk analysis that filters out the high-risk GPUs; (2) task placement policies that cap GPU utilization to avoid OOMs and limit interference; (3) integration of GPU memory need estimators for DL tasks to minimize OOMs during collocation; and (4) a lightweight recovery method that relaunches jobs crashed due to OOMs. Our evaluation on a DL training workload derived from real-world traces shows that CARMA uses GPUs more efficiently by making more informed collocation decisions: for the best-performing collocation policy, CARMA increases GPU streaming multiprocessor (SM) utilization by 54%, the parallelism achieved per SM by 61%, and memory use by 62%. This results in a $\sim$35% and $\sim$15% reduction in the end-to-end execution time (makespan) and GPU energy consumption, respectively, for this workload.</p><p><h4>cs.DC, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00075'>PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories</a></h3><h3><a href='https://arxiv.org/pdf/2511.00075' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/36</p><p><b>作者：</b>Qianhui Li, Weiya Wang, Qianqi Zhao, Tong Qu, Jing He, Xuhong Qiang, Jingwen Hou, Ke Chen, Bao Zhang, Qi Wang</p><p>Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant storage solution in the era of artificial intelligence. QLC 3D NAND flash stores 4 bit per cell to expand the storage density, resulting in narrower read margins. Constrained to read margins, QLC always suffers from lateral charge migration (LCM), which caused by non-uniform charge density across adjacent memory cells. To suppress charge density gap between cells, there are some algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we observe inter-page data arrangements also approach the suppression. Thus, we proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM applies a long-short term memory (LSTM) neural network to compute a data arrangement probability matrix from input page data pattern. The arrangement is to minimize the global impacts derived from the LCM among wordlines. Since each page data can be arranged only once, we design a transformation from output matrix of LSTM network to non-repetitive sequence generation probability matrix to assist training process. The arranged data pattern can decrease the bit error rate (BER) during data retention. In addition, PDA-LSTM do not need extra flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS. The experiment results show that the PDA-LSTM reduces the average BER by 80.4% compared with strategy without data arrangement, and by 18.4%, 15.2% compared respectively with WBVM and DVDS with code-length 64.</p><p><h4>cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00295'>H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention</a></h3><h3><a href='https://arxiv.org/pdf/2511.00295' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/36</p><p><b>作者：</b>Kosmas Alexandridis, Giorgos Dimitrakopoulos</p><p>Transformers have significantly advanced AI and machine learning through their powerful attention mechanism. However, computing attention on long sequences can become a computational bottleneck. FlashAttention mitigates this by fusing the softmax and matrix operations into a tiled computation pattern that decouples performance from sequence length. Though designed for GPUs, its simplicity also makes it well suited for direct hardware acceleration. To improve hardware implementation, we compute FlashAttention using a mixture of floating-point and fixed-point logarithm domain representations. Floating-point is used to compute attention scores from query and key matrices, while logarithmic computation simplifies the fused computation of softmax normalization and the multiplication with the value matrix. This transformation, called H-FA, replaces vector-wide floating-point multiplication and division operations by additions and subtractions implemented efficiently with fixed-point arithmetic in the logarithm domain. Exponential function evaluations are effectively omitted and fused with the rest operations, and the final result is directly returned to floating-point arithmetic without any additional hardware overhead. Hardware implementation results at 28nm demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction in power, on average, compared to FlashAttention parallel hardware architectures built solely with floating-point datapaths, without hindering performance.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00321'>Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</a></h3><h3><a href='https://arxiv.org/pdf/2511.00321' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/36</p><p><b>作者：</b>Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi</p><p>The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.</p><p><h4>cs.AR, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00316'>PEARL: Power- and Energy-Aware Multicore Intermittent Computing</a></h3><h3><a href='https://arxiv.org/pdf/2511.00316' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/36</p><p><b>作者：</b>Khakim Akhunov, Eren Yildiz, Kasim Sinan Yildirim</p><p>Low-power multicore platforms are suitable for running data-intensive tasks in parallel, but they are highly inefficient for computing on intermittent power. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore Intermittent Computing), a novel systems support that can make existing multicore microcontroller (MCU) platforms suitable for efficient intermittent computing. PEARL achieves this by leveraging only a three-threshold voltage tracking circuit and an external fast non-volatile memory, which multicore MCUs can smoothly interface. PEARL software runtime manages these components and performs energy- and power-aware adaptation of the multicore configuration to introduce minimal backup overheads and boost performance. Our evaluation shows that PEARL outperforms the state-of-the-art solutions by up to 30x and consumes up to 32x less energy.</p><p><h4>cs.ET, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00732'>FeNN-DMA: A RISC-V SoC for SNN acceleration</a></h3><h3><a href='https://arxiv.org/pdf/2511.00732' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/36</p><p><b>作者：</b>Zainab Aizaz, James C. Knight, Thomas Nowotny</p><p>Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than ANNs and are therefore not well-matched to standard accelerators like GPUs and TPUs. Field Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads and here we develop a novel, fully-programmable RISC-V-based system-on-chip (FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show that FeNN-DMA has comparable resource usage and energy requirements to state-of-the-art fixed-function SNN accelerators, yet it is capable of simulating much larger and more complex models. Using this functionality, we demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg Digits and Neuromorphic MNIST tasks.</p><p><h4>cs.NE, cs.AI, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2507.05556'>Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads</a></h3><h3><a href='https://arxiv.org/pdf/2507.05556' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/36</p><p><b>作者：</b>Jumin Kim, Seungmin Baek, Minbok Wi, Hwayong Nam, Michael Jaemin Kim, Sukhan Lee, Kyomin Sohn, Jung Ho Ahn</p><p>Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation method, modifies key DRAM timing parameters, reportedly causing significant performance overheads in simulator-based studies. However, given known discrepancies between simulators and real hardware, real-machine experiments are vital for accurate PRAC performance estimation. We present the first real-machine performance analysis of PRAC. After verifying timing modifications on the latest CPUs using microbenchmarks, our analysis shows that PRAC&#x27;s average and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017 workloads -- up to 9.15x lower than simulator-based reports. Further, we show that the close page policy minimizes this overhead by effectively hiding the elongated DRAM row precharge operations due to PRAC from the critical path.</p><p><h4>cs.AR, cs.CR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.18574'>Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators</a></h3><h3><a href='https://arxiv.org/pdf/2505.18574' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/36</p><p><b>作者：</b>Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao</p><p>Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today&#x27;s computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.</p><p><h4>cs.PL, cs.AI, cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.04528'>HEEPidermis: a versatile SoC for BioZ recording</a></h3><h3><a href='https://arxiv.org/pdf/2509.04528' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/36</p><p><b>作者：</b>Juan Sapriza, Beatrice Grassano, Alessio Naclerio, Filippo Quadri, Tommaso Terzano, David Mallas\&#x27;en, Davide Schiavone, Robin Leplae, J\&#x27;er\&#x27;emie Moullet, Alexandre Levisse, Christoph M\&quot;uller, Mariagrazia Graziano, Mat\&#x27;ias Miguez, David Atienza</p><p>Biological impedance (BioZ) is an information-packed modality that allows for non-invasive monitoring of health and emotional state. Currently, most research involving tissue impedance is based on bulky or fixed-purpose hardware, which limits the scope of research and the possibilities of experiments. In this work, we present HEEPidermis: a System-on-Chip (SoC) which integrates all the blocks needed for tissue impedance measurement, including two 8-bit, arbitrary-signal current DACs, two VCO-based ADCs, and a RISC-V CPU to enable on-chip feature extraction for closed-loop operation. An event-based sub-sampler improves storage and energy efficiency for long-term recording. In addition to the versatile SoC, the digital back-end and behavioral models of the analog front-end are open-source, allowing fast system-level simulations or repurposing. The SoC was taped out on TSMC 65 nm LP process.</p><p><h4>physics.ins-det, cs.AR, physics.med-ph</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00038'>AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios</a></h3><h3><a href='https://arxiv.org/pdf/2511.00038' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/36</p><p><b>作者：</b>Suman Raj, Radhika Mittal, Rajiv Mayani, Pawel Zuk, Anirban Mandal, Michael Zink, Yogesh Simmhan, Ewa Deelman</p><p>Drone fleets equipped with onboard cameras, computer vision, and Deep Neural Network (DNN) models present a powerful paradigm for real-time spatio-temporal decision-making. In wildfire response, such drones play a pivotal role in monitoring fire dynamics, supporting firefighter coordination, and facilitating safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV framework designed for scalable, resilient, and collaborative escape route planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration architecture comprising service drones (SDs) and coordinator drones (CDs), each performing specialized roles. SDs survey fire-affected areas, detect stranded individuals using onboard edge accelerators running fire detection and human pose identification DNN models, and issue requests for assistance. CDs, equipped with lightweight data stores such as Apache IoTDB, dynamically generate optimal ground escape routes and monitor firefighter movements along these routes. The framework proposes a collaborative path-planning approach based on a weighted A* search algorithm, where CDs compute context-aware escape paths. AeroResQ further incorporates intelligent load-balancing and resilience mechanisms: CD failures trigger automated data redistribution across IoTDB replicas, while SD failures initiate geo-fenced re-partitioning and reassignment of spatial workloads to operational SDs. We evaluate AeroResQ using realistic wildfire emulated setup modeled on recent Southern California wildfires. Experimental results demonstrate that AeroResQ achieves a nominal end-to-end latency of &lt;=500ms, much below the 2s request interval, while maintaining over 98% successful task reassignment and completion, underscoring its feasibility for real-time, on-field deployment in emergency response and firefighter safety operations.</p><p><h4>cs.DC, cs.RO</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00263'>COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement</a></h3><h3><a href='https://arxiv.org/pdf/2511.00263' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/36</p><p><b>作者：</b>Jinyuan Chen</p><p>COOL (Chen&#x27;21) is an error-free, information-theoretically secure Byzantine agreement (BA) protocol proven to achieve BA consensus in the synchronous setting for an $\ell$-bit message, with a total communication complexity of $O(\max\{n\ell, nt \log q\})$ bits, four communication rounds in the worst case, and a single invocation of a binary BA, under the optimal resilience assumption $n \geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may behave dishonestly. Here, $q$ denotes the alphabet size of the error correction code used in the protocol.  In this work, we present an adaptive variant of COOL, called OciorACOOL, which achieves error-free, information-theoretically secure BA consensus in the asynchronous setting with total $O(\max\{n\ell, n t \log q\})$ communication bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA protocol, still under the optimal resilience assumption $n \geq 3t + 1$. Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$ error-correction encoding and decoding as COOL, with $k=t/3$.</p><p><h4>cs.DC, cs.CR, cs.IT, math.IT</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00294'>Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum</a></h3><h3><a href='https://arxiv.org/pdf/2511.00294' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/36</p><p><b>作者：</b>Lucas Almeida, Maycon Peixoto</p><p>An Edge-Cloud Continuum integrates edge and cloud resources to provide a flexible and scalable infrastructure. This paradigm can minimize latency by processing data closer to the source at the edge while leveraging the vast computational power of the cloud for more intensive tasks. In this context, module application placement requires strategic allocation plans that align user demands with infrastructure constraints, aiming for efficient resource use. Therefore, we propose Tetris, an application placement strategy that utilizes a heuristic algorithm to distribute computational services across edge and cloud resources efficiently. Tetris prioritizes services based on SLA urgencies and resource efficiency to avoid system overloading. Our results demonstrate that Tetris reduces SLA violations by approximately 76% compared to the baseline method, which serves as a reference point for benchmarking performance in this scenario. Therefore, Tetris offers an effective placement approach for managing latency-sensitive applications in Edge-Cloud Continuum environments, enhancing Quality of Service (QoS) for users.</p><p><h4>cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00603'>EPARA: Parallelizing Categorized AI Inference in Edge Clouds</a></h3><h3><a href='https://arxiv.org/pdf/2511.00603' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/36</p><p><b>作者：</b>Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie</p><p>With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.</p><p><h4>cs.DC, cs.AI, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00796'>AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs</a></h3><h3><a href='https://arxiv.org/pdf/2511.00796' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/36</p><p><b>作者：</b>Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, Binhang Yuan</p><p>Maximizing training throughput and cost-efficiency of RL for LLMs is essential to democratize this advanced technique. One promising but challenging approach is to deploy such a computational workflow over heterogeneous GPUs. Unlike conventional large-scale LLM pretraining, RL training generally decomposes into three coupled stages, i.e., rollout generation, reward computation, and policy/value updates, which exhibit markedly different compute intensities, memory footprints, and communication patterns. Recent research shows that fully asynchronous RL training can disaggregate these stages across disjoint hardware pools without sacrificing training stability, creating a great opportunity for real-world heterogeneous deployment. To this end, we present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that effectively schedules how to execute rollout generation and policy model training over heterogeneous GPUs while enforcing data staleness bounds. Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to select per-stage parallelization strategies and workload assignments given a resource budget, and (ii) a graph-partitioning step that allocates heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound generation and compute-bound optimization to more cost-efficient resources and balances their producer-consumer interactions to avoid both idleness and stale rollout trajectories. On the mathematical reasoning task with various model scales (1.5B, 7B, and 14B), compared to homogeneous deployments of state-of-the-art asynchronous RL systems: (i) When maintaining the same total budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When achieving the same training throughput, AReaL-Hex results in up to 1.46x reduction in training cost.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00807'>FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs</a></h3><h3><a href='https://arxiv.org/pdf/2511.00807' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/36</p><p><b>作者：</b>Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen</p><p>The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query traffic and serving patterns. LLM queries running on advanced GPUs during a high-emission hour at one location can lead to significantly higher carbon footprints versus same queries running on mid-level GPUs at a low-emission time and location. By observing LLM serving requirements and leveraging spatiotemporal computation flexibility, we consider the joint routing and scheduling problem, and propose FREESH to cooperatively run a group of data centers while minimizing user-specified carbon or energy objectives. FREESH identifies the optimal configurations of balanced load serving by matching distinct GPU instance&#x27;s power-throughput characteristics with predictable LLM query length and workloads. To ensure both latency and fairness requirements, FREESH identifies optimized parallelism and query routing schedules together with dynamic GPU frequency scaling for power saving, and Least-Laxity-First (LLF) serving strategy for query scheduling. During the 1-hour serving on production workloads, FREESH reduces energy by 28.6% and emissions by 45.45% together with improvements in SLO attainment and fairness.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01127'>Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks</a></h3><h3><a href='https://arxiv.org/pdf/2511.01127' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/36</p><p><b>作者：</b>Fabio Diniz Rossi</p><p>Traditional task offloading strategies in edge computing often rely on static heuristics or data-intensive machine learning models, which are not always suitable for highly dynamic and resource-constrained environments. In this paper, we propose a novel task-offloading framework based on Spiking Neural Networks inspired by the efficiency and adaptability of biological neural systems. Our approach integrates an SNN-based decision module into edge nodes to perform real-time, energy-efficient task orchestration. We evaluate the model under various IoT workload scenarios using a hybrid simulation environment composed of YAFS and Brian2. The results demonstrate that our SNN-based framework significantly reduces task processing latency and energy consumption while improving task success rates. Compared to traditional heuristic and ML-based strategies, our model achieves up to 26% lower latency, 32% less energy consumption, and 25\% higher success rate under high-load conditions.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01235'>Scalable Maxflow Processing for Dynamic Graphs</a></h3><h3><a href='https://arxiv.org/pdf/2511.01235' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/36</p><p><b>作者：</b>Shruthi Kannappan, Ashwina Kumar, Rupesh Nasre</p><p>The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and combinatorial optimization, aiming to determine the largest possible flow from a designated source node to a sink node within a capacitated flow network. It has extensive applications across diverse domains such as computer networking, transportation systems, and image segmentation. The objective is to maximize the total throughput while respecting edge capacity constraints and maintaining flow conservation at all intermediate vertices.  Among the various algorithms proposed for solving the Max-Flow problem, the Push--Relabel algorithm is particularly notable for its efficiency and suitability for parallelization, owing to its localized vertex-based operations. This property has motivated extensive research into GPU-accelerated Max-Flow computation, leveraging the high degree of parallelism inherent to modern GPU architectures.  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of incrementally recomputing the maximum flow of a dynamic graph following a batch of edge updates. In addition, we introduce a high-performance static GPU algorithm designed for efficiently computing the initial Max-Flow on static graphs. We further describe a series of CUDA-specific implementation optimizations that enhance performance, scalability, and memory efficiency on GPU platforms.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01255'>Design of quasi phase matching crystal based on differential gray wolf algorithm</a></h3><h3><a href='https://arxiv.org/pdf/2511.01255' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/36</p><p><b>作者：</b>He Chen, ZiHua Zheng, JingHua Sun</p><p>This paper focuses on the key problem in the development of nonlinear optical technology, the performance optimization of aperiodically polarized crystals. The performance of the crystal depends on the precise control of the micro distribution of crystal domains, but its optimization belongs to the high-dimensional discrete combination &quot;NP hard&quot; problem. The traditional algorithm has the bottleneck of slow convergence and easy to fall into local optimization, while the heuristic methods such as genetic algorithm are limited by the CPU serial calculation and inefficient. In order to solve the above challenges, this paper proposes the fusion scheme of hwsda hybrid optimization algorithm and GPU parallel acceleration technology: the differential evolution algorithm (DE) is used to realize the global search, and the gray wolf optimization algorithm (GWO) is used to strengthen the local search and convergence speed, and the two coordinate to balance the global and local optimization requirements; At the same time, it relies on GPU multi-core architecture to realize thread level parallel computing and improve optimization efficiency. This scheme effectively breaks through the optimization problem of high-dimensional discrete space, improves the accuracy of crystal domain control, improves the efficiency of quasi phase matching design by hundreds to thousands of times compared with traditional CPU serial computing, provides a new paradigm for the design of complex nonlinear optical devices, and helps promote the performance breakthrough and industrial application of related devices in the fields of quantum optics and laser processing.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01333'>Transformer-Based Sparse CSI Estimation for Non-Stationary Channels</a></h3><h3><a href='https://arxiv.org/pdf/2511.01333' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/36</p><p><b>作者：</b>Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Hassan Rizwan, Sagnik Bhattacharya, Muhammad Ali Jamshed, John M. Cioffi</p><p>Accurate and efficient estimation of Channel State Information (CSI) is critical for next-generation wireless systems operating under non-stationary conditions, where user mobility, Doppler spread, and multipath dynamics rapidly alter channel statistics. Conventional pilot aided estimators incur substantial overhead, while deep learning approaches degrade under dynamic pilot patterns and time varying fading. This paper presents a pilot-aided Flash-Attention Transformer framework that unifies model-driven pilot acquisition with data driven CSI reconstruction through patch-wise self-attention and a physics aware composite loss function enforcing phase alignment, correlation consistency, and time frequency smoothness. Under a standardized 3GPP NR configuration, the proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB in phase invariant normalized mean-square error (NMSE) with markedly lower bit-error rate (BER), while reducing pilot overhead by 16 times. These results demonstrate that attention based architectures enable reliable CSI recovery and enhanced spectral efficiency without compromising link quality, addressing a fundamental bottleneck in adaptive, low-overhead channel estimation for non-stationary 5G and beyond-5G networks.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01420'>Gradient Clock Synchronization with Practically Constant Local Skew</a></h3><h3><a href='https://arxiv.org/pdf/2511.01420' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/36</p><p><b>作者：</b>Christoph Lenzen</p><p>Gradient Clock Synchronization (GCS) is the task of minimizing the local skew, i.e., the clock offset between neighboring clocks, in a larger network. While asymptotically optimal bounds are known, from a practical perspective they have crucial shortcomings:  - Local skew bounds are determined by upper bounds on offset estimation that need to be guaranteed throughout the entire lifetime of the system.  - Worst-case frequency deviations of local oscillators from their nominal rate are assumed, yet frequencies tend to be much more stable in the (relevant) short term.  State-of-the-art deployed synchronization methods adapt to the true offset measurement and frequency errors, but achieve no non-trivial guarantees on the local skew.  In this work, we provide a refined model and novel analysis of existing techniques for solving GCS in this model. By requiring only stability of measurement and frequency errors, we can circumvent existing lower bounds, leading to dramatic improvements under very general conditions. For example, if links exhibit a uniform worst-case estimation error of $\Delta$ and a change in estimation errors of $\delta\ll \Delta$ on relevant time scales, we bound the local skew by $O(\Delta+\delta \log D)$ for networks of diameter $D$, effectively ``breaking&#x27;&#x27; the established $\Omega(\Delta\log D)$ lower bound, which holds when $\delta=\Delta$. Similarly, we show how to limit the influence of local oscillators on $\delta$ to scale with the change of frequency of an individual oscillator on relevant time scales, rather than a worst-case bound over all oscillators and the lifetime of the system.  Moreover, we show how to ensure self-stabilization in this challenging setting. Last, but not least, we extend all of our results to the scenario of external synchronization, at the cost of a limited increase in stabilization time.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01573'>Adaptive Multidimensional Quadrature on Multi-GPU Systems</a></h3><h3><a href='https://arxiv.org/pdf/2511.01573' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/36</p><p><b>作者：</b>Melanie Tonarelli, Simone Riva, Pietro Benedusi, Fabrizio Ferrandi, Rolf Krause</p><p>We introduce a distributed adaptive quadrature method that formulates multidimensional integration as a hierarchical domain decomposition problem on multi-GPU architectures. The integration domain is recursively partitioned into subdomains whose refinement is guided by local error estimators. Each subdomain evolves independently on a GPU, which exposes a significant load imbalance as the adaptive process progresses. To address this challenge, we introduce a decentralised load redistribution schemes based on a cyclic round-robin policy. This strategy dynamically rebalance subdomains across devices through non-blocking, CUDA-aware MPI communication that overlaps with computation. The proposed strategy has two main advantages compared to a state-of-the-art GPU-tailored package: higher efficiency in high dimensions; and improved robustness w.r.t the integrand regularity and the target accuracy.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01843'>LARK - Linearizability Algorithms for Replicated Keys in Aerospike</a></h3><h3><a href='https://arxiv.org/pdf/2511.01843' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/36</p><p><b>作者：</b>Andrew Goodng, Kevin Porter, Thomas Lopatic, Ashish Shinde, Sunil Sayyaparaju, Srinivasan Seshadri, V. Srinivasan</p><p>We present LARK (Linearizability Algorithms for Replicated Keys), a synchronous replication protocol that achieves linearizability while minimizing latency and infrastructure cost, at significantly higher availability than traditional quorum-log consensus. LARK introduces Partition Availability Conditions (PAC) that reason over the entire database cluster rather than fixed replica sets, improving partition availability under independent failures by roughly 3x when tolerating one failure and 10x when tolerating two. Unlike Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs, enabling immediate partition readiness after leader changes -- with at most a per-key duplicate-resolution round trip when the new leader lacks the latest copy. Under equal storage budgets -- where both systems maintain only f+1 data copies to tolerate f failures -- LARK continues committing through data-node failures while log-based protocols must pause commits for replica rebuilding. These properties also enable zero-downtime rolling restarts even when maintaining only two copies. We provide formal safety arguments and a TLA+ specification, and we demonstrate through analysis and experiments that LARK achieves significant availability gains.</p><p><h4>cs.DC, cs.DB</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00037'>Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</a></h3><h3><a href='https://arxiv.org/pdf/2511.00037' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/36</p><p><b>作者：</b>Riya Gupta, Alexander Chowdhury, Sahil Nalawade</p><p>Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.</p><p><h4>cs.CV, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00205'>Fix: externalizing network I/O in serverless computing</a></h3><h3><a href='https://arxiv.org/pdf/2511.00205' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/36</p><p><b>作者：</b>Yuhan Deng, Akshay Srivatsan, Sebastian Ingino, Francis Chua, Yasmine Mitchell, Matthew Vilaysack, Keith Winstein</p><p>We describe a system for serverless computing where users, programs,  and the underlying platform share a common representation of a  computation: a deterministic procedure, run in an environment  of well-specified data or the outputs of other computations. This  representation externalizes I/O: data movement over the network is  performed exclusively by the platform. Applications can describe the  precise data needed at each stage, helping the provider schedule  tasks and network transfers to reduce starvation. The design  suggests an end-to-end argument for outsourced computing, shifting  the service model from ``pay-for-effort&#x27;&#x27; to ``pay-for-results.&#x27;&#x27;</p><p><h4>cs.OS, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00279'>LongCat-Flash-Omni Technical Report</a></h3><h3><a href='https://arxiv.org/pdf/2511.00279' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>31/36</p><p><b>作者：</b>Meituan LongCat Team, Bairui Wang,  Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing, Xuezhi Cao, Xunliang Cai, Yang Yang, Yanli Tan, Yao Yao, Yerui Sun, Yi Chen, Yifan Lu, Yin Gong, Yining Zhang, Yitian Chen, Yiyang Gan, Yuchen Tang, Yuchen Xie, Yueqian Wang, Yuewen Zheng, Yufei Zhang, Yufeng Zhong, Yulei Qian, Yuqi Peng, Yuwei Jiang, Zeyang Hu, Zheng Zhang, Zhengkun Tian, Zhiqing Hong, Zhixiong Zeng, Zhuqi Mi, Ziran Li, Ziwen Wang, Ziyi Zhao, Ziyuan Zhuang, Zizhe Zhao</p><p>We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p><p><h4>cs.MM, cs.AI, cs.CL, cs.DC, cs.LG, cs.SD</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00336'>Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems</a></h3><h3><a href='https://arxiv.org/pdf/2511.00336' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>32/36</p><p><b>作者：</b>Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, Rajkumar Buyya</p><p>The rapid growth of Internet of Medical Things (IoMT) devices has resulted in significant security risks, particularly the risk of malware attacks on resource-constrained devices. Conventional deep learning methods are impractical due to resource limitations, while Federated Learning (FL) suffers from high communication overhead and vulnerability to non-IID (heterogeneous) data. In this paper, we propose a split learning (SL) based framework for IoT malware detection through image-based classification. By dividing the neural network training between the clients and an edge server, the framework reduces computational burden on resource-constrained clients while ensuring data privacy. We formulate a joint optimization problem that balances computation cost and communication efficiency by using a game-theoretic approach for attaining better training performance. Experimental evaluations show that the proposed framework outperforms popular FL methods in terms of accuracy (+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less resource consumption (33.83%). These results establish the potential of SL as a scalable and secure paradigm for next-generation IoT security.</p><p><h4>cs.CR, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00823'>TINC: Trusted Intelligent NetChain</a></h3><h3><a href='https://arxiv.org/pdf/2511.00823' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>33/36</p><p><b>作者：</b>Qi Xia, Hu Xia, Isaac Amankona Obiri, Adjei-Arthur Bonsu, Grace Mupoyi Ntuala, Ansu Badjie, Tienin Bole Wilfried, Jiaqin Liu, Lan Ma, Jianbin Gao, Feng Yao</p><p>Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.</p><p><h4>cs.NI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00140'>Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration</a></h3><h3><a href='https://arxiv.org/pdf/2511.00140' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>34/36</p><p><b>作者：</b>Tahmid Hasan Sakib, Yago Romano Martinez, Carter Brady, Syed Rafay Hasan, Terry N. Guo</p><p>This paper presents a proof-of-concept supply chain attack against the Secure ROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle platform. A Trojan-infected Debian package modifies core ROS 2 security commands to exfiltrate newly generated keystore credentials via DNS in base64-encoded chunks to an attacker-controlled nameserver. Possession of these credentials enables the attacker to rejoin the SROS 2 network as an authenticated participant and publish spoofed control or perception messages without triggering authentication failures. We evaluate this capability on a secure ROS 2 Humble testbed configured for a four-stop-sign navigation routine using an Intel RealSense camera for perception. Experimental results show that control-topic injections can cause forced braking, sustained high-speed acceleration, and continuous turning loops, while perception-topic spoofing can induce phantom stop signs or suppress real detections. The attack generalizes to any data distribution service (DDS)-based robotic system using SROS 2, highlighting the need for both supply chain integrity controls and runtime semantic validation to safeguard autonomous systems against insider and impersonation threats.</p><p><h4>cs.CR, cs.OS, cs.RO, cs.SY, eess.SY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.00363'>Fast Networks for High-Performance Distributed Trust</a></h3><h3><a href='https://arxiv.org/pdf/2511.00363' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>35/36</p><p><b>作者：</b>Yicheng Liu, Rafail Ostrovsky, Scott Shenker, Sam Kumar</p><p>Organizations increasingly need to collaborate by performing a computation on their combined dataset, while keeping their data hidden from each other. Certain kinds of collaboration, such as collaborative data analytics and AI, require a level of performance beyond what current cryptographic techniques for distributed trust can provide. This is because the organizations run software in different trust domains, which can require them to communicate over WANs or the public Internet. In this paper, we explore how to instead run such applications using fast datacenter-type LANs. We show that, by carefully redesigning distributed trust frameworks for LANs, we can achieve up to order-of-magnitude better performance than na\&quot;ively using a LAN. Then, we develop deployment models for Distributed But Proximate Trust (DBPT) that allow parties to use a LAN while remaining physically and logically distinct. These developments make secure collaborative data analytics and AI significantly more practical and set new research directions for developing systems and cryptographic theory for high-performance distributed trust.</p><p><h4>cs.CR, cs.NI, cs.OS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.18829'>LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS</a></h3><h3><a href='https://arxiv.org/pdf/2505.18829' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>36/36</p><p><b>作者：</b>Kai Mei, Xi Zhu, Hang Gao, Shuhang Lin, Yongfeng Zhang</p><p>We present AIOS 1.0, a novel platform designed to advance computer-use agent (CUA) capabilities through environmental contextualization. While existing approaches primarily focus on building more powerful agent frameworks or enhancing agent models, we identify a fundamental limitation: the semantic disconnect between how language models understand the world and how computer interfaces are structured. AIOS 1.0 addresses this challenge by transforming computers into contextual environments that language models can natively comprehend, implementing a Model Context Protocol (MCP) server architecture to abstract computer states and actions. This approach effectively decouples interface complexity from decision complexity, enabling agents to reason more effectively about computing environments. To demonstrate our platform&#x27;s effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark, outperforming several specialized agent frameworks despite its simple architecture. Our results suggest that contextualizing computer environments for language models represents a promising direction for developing more capable computer-use agents and advancing toward AI that can interact with digital systems.</p><p><h4>cs.AI, cs.HC, cs.OS</h4></p></div><hr>
</body>
</html>
