<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-13</h1>
<div><h3><a href='https://arxiv.org/abs/2510.08769'>Prioritizing Latency with Profit: A DRL-Based Admission Control for 5G Network Slices</a></h3><h3><a href='https://arxiv.org/pdf/2510.08769' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/25</p><p><b>作者：</b>Proggya Chakraborty, Aaquib Asrar, Jayasree Sengupta, Sipra Das Bit</p><p>5G networks enable diverse services such as eMBB, URLLC, and mMTC through network slicing, necessitating intelligent admission control and resource allocation to meet stringent QoS requirements while maximizing Network Service Provider (NSP) profits. However, existing Deep Reinforcement Learning (DRL) frameworks focus primarily on profit optimization without explicitly accounting for service delay, potentially leading to QoS violations for latency-sensitive slices. Moreover, commonly used epsilon-greedy exploration of DRL often results in unstable convergence and suboptimal policy learning. To address these gaps, we propose DePSAC -- a Delay and Profit-aware Slice Admission Control scheme. Our DRL-based approach incorporates a delay-aware reward function, where penalties due to service delay incentivize the prioritization of latency-critical slices such as URLLC. Additionally, we employ Boltzmann exploration to achieve smoother and faster convergence. We implement and evaluate DePSAC on a simulated 5G core network substrate with realistic Network Slice Request (NSLR) arrival patterns. Experimental results demonstrate that our method outperforms the DSARA baseline in terms of overall profit, reduced URLLC slice delays, improved acceptance rates, and improved resource consumption. These findings validate the effectiveness of the proposed DePSAC in achieving better QoS-profit trade-offs for practical 5G network slicing scenarios.</p><p><h4>cs.NI, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09271'>Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains</a></h3><h3><a href='https://arxiv.org/pdf/2510.09271' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/25</p><p><b>作者：</b>Alison Gon\c{c}alves Schemitt (PUCRS), Henrique Fan da Silva (UNIPAMPA), Roben Castagna Lunardi (PUCRS, IFRS), Diego Kreutz (UNIPAMPA), Rodrigo Brand\~ao Mansilha (UNIPAMPA), Avelino Francisco Zorzo (PUCRS)</p><p>The advent of quantum computing threatens the security of traditional encryption algorithms, motivating the development of post-quantum cryptography (PQC). In 2024, the National Institute of Standards and Technology (NIST) standardized several PQC algorithms, marking an important milestone in the transition toward quantum-resistant security. Blockchain systems fundamentally rely on cryptographic primitives to guarantee data integrity and transaction authenticity. However, widely used algorithms such as ECDSA, employed in Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks. Although adopting PQC is essential for long-term security, its computational overhead in blockchain environments remains largely unexplored. In this work, we propose a methodology for benchmarking both PQC and traditional cryptographic algorithms in blockchain contexts. We measure signature generation and verification times across diverse computational environments and simulate their impact at scale. Our evaluation focuses on PQC digital signature schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across security levels 1 to 5, comparing them to ECDSA, the current standard in Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only minor performance overhead at security level 1, while in some scenarios they significantly outperform ECDSA at higher security levels. For instance, ML-DSA achieves a verification time of 0.14 ms on an ARM-based laptop at level 5, compared to 0.88 ms for ECDSA. We also provide an open-source implementation to ensure reproducibility and encourage further research.</p><p><h4>cs.CR, cs.ET, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09371'>A Framework for Distributed Resource Allocation in Quantum Networks</a></h3><h3><a href='https://arxiv.org/pdf/2510.09371' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/25</p><p><b>作者：</b>Nitish K. Panigrahy, Leonardo Bacciottini, C. V. Hollot, Emily A. Van Milligen, Matheus Guedes de Andrade, Nageswara S. V. Rao, Gayane Vardoyan, Don Towsley</p><p>We introduce a distributed resource allocation framework for the Quantum Internet that relies on feedback-based, fully decentralized coordination to serve multiple co-existing applications. We develop quantum network control algorithms under the mathematical framework of Quantum Network Utility Maximization (QNUM), where utility functions quantify network performance by mapping entanglement rate and quality into a joint optimization objective. We then introduce QPrimal-Dual, a decentralized, scalable algorithm that solves QNUM by strategically placing network controllers that operate using local state information and limited classical message exchange. We prove global asymptotic stability for concave, separable utility functions, and provide sufficient conditions for local stability for broader non-concave cases. To reduce control overhead and account for quantum memory decoherence, we also propose schemes that locally approximate global quantities and prevent congestion in the network. We evaluate the performance of our approach via simulations in realistic quantum network architectures. Results show that QPrimalDual significantly outperforms baseline allocation strategies, scales with network size, and is robust to latency and decoherence. Our observations suggest that QPrimalDual could be a practical, high-performance foundation for fully distributed resource allocation in quantum networks.</p><p><h4>quant-ph, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.03243'>Prompt-Aware Scheduling for Low-Latency LLM Serving</a></h3><h3><a href='https://arxiv.org/pdf/2510.03243' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/25</p><p><b>作者：</b>Yiheng Tao, Yihe Zhang, Matthew T. Dearing, Xin Wang, Yuping Fan, Zhiling Lan</p><p>Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.</p><p><h4>cs.LG, cs.AI, cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08873'>Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits</a></h3><h3><a href='https://arxiv.org/pdf/2510.08873' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/25</p><p><b>作者：</b>Haoran Jin, Jirong Yang, Yunpeng Liu, Barry Lyu, Kangqi Zhang, Nathaniel Bleier</p><p>Modern AI acceleration faces a fundamental challenge: conventional assumptions about memory requirements, batching effectiveness, and latency-throughput tradeoffs are systemwide generalizations that ignore the heterogeneous computational patterns of individual neural network operators. However, going towards network-level customization and operator-level heterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While chiplet-based approaches have been proposed to amortize NRE costs, reuse opportunities remain limited without carefully identifying which chiplets are truly necessary. This paper introduces Mozart, a chiplet ecosystem and accelerator codesign framework that systematically constructs low cost bespoke application-specific integrated circuits (BASICs). BASICs leverage operator-level disaggregation to explore chiplet and memory heterogeneity, tensor fusion, and tensor parallelism, with place-and-route validation ensuring physical implementability. The framework also enables constraint-aware system-level optimization across deployment contexts ranging from datacenter inference serving to edge computing in autonomous vehicles. The evaluation confirms that with just 8 strategically selected chiplets, Mozart-generated composite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy, energy-cost product, energy-delay product (EDP), and energy-delay-cost product compared to traditional homogeneous accelerators. For datacenter LLM serving, Mozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In speculative decoding, Mozart delivers throughput improvements of 24.6-58.6% while reducing energy consumption by 38.6-45.6%. For autonomous vehicle perception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under real-time constraints.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08940'>A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing</a></h3><h3><a href='https://arxiv.org/pdf/2510.08940' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/25</p><p><b>作者：</b>Abel Beyene, Zhongpan Wu, Yunus Dawji, Karim Hammad, Ebrahim Ghafar-Zadeh, Sebastian Magierowski</p><p>Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing importance in several life sciences fields as their small footprints enable a broader range of use cases than their larger, stationary counterparts. However, as currently designed, they lack sufficient embedded computing to process the large volume of measurements generated by their internal sensory system. As a consequence, they rely on external devices for additional processing capability. This dependence on external processing places a significant communication burden on the sequencer&#x27;s embedded electronics. Moreover, it also prevents a truly mobile solution for sequencing in real-time. Anticipating next-generation machines that include suitably advanced processing, we present a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide semiconductor (CMOS). Our design, based on a general-purpose reduced instruction set computing (RISC-V) core, also includes accelerators for DNA detection that allow our system to demonstrate a 13X performance improvement over commercial embedded multicore processors combined with a near 3000X boost in energy efficiency.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09010'>HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization</a></h3><h3><a href='https://arxiv.org/pdf/2510.09010' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/25</p><p><b>作者：</b>Yipu Zhang, Chaofang Ma, Jinming Ge, Lin Jiang, Jiang Xu, Wei Zhang</p><p>Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction method, delivering high-quality results for AR/VR applications. While quantization methods and hardware accelerators have been proposed to enhance NeRF&#x27;s computational efficiency, existing approaches face crucial limitations. Current quantization methods operate without considering hardware architecture, resulting in sub-optimal solutions within the vast design space encompassing accuracy, latency, and model size. Additionally, existing NeRF accelerators heavily rely on human experts to explore this design space, making the optimization process time-consuming, inefficient, and unlikely to discover optimal solutions. To address these challenges, we introduce HERO, a reinforcement learning framework performing hardware-aware quantization for NeRF. Our framework integrates a NeRF accelerator simulator to generate real-time hardware feedback, enabling fully automated adaptation to hardware constraints. Experimental results demonstrate that HERO achieves 1.31-1.33 $\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a more compact model size compared to CAQ, a previous state-of-the-art NeRF quantization framework. These results validate our framework&#x27;s capability to effectively navigate the complex design space between hardware and algorithm requirements, discovering superior quantization policies for NeRF implementation. Code is available at https://github.com/ypzhng/HERO.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09339'>Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge</a></h3><h3><a href='https://arxiv.org/pdf/2510.09339' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/25</p><p><b>作者：</b>Sebastian Magierowski, Zhongpan Wu, Abel Beyene, Karim Hammad</p><p>Miniature DNA sequencing hardware has begun to succeed in mobile contexts, driving demand for efficient machine learning at the edge. This domain leverages deep learning techniques familiar from speech and time-series analysis for both low-level signal processing and high-level genomic interpretation. Unlike audio, however, nanopore sequencing presents raw data rates over 100X higher, requiring more aggressive compute and memory handling. In this paper, we present a CMOS system-on-chip (SoC) designed for mobile genetic analysis. Our approach combines a multi-core RISC-V processor with tightly coupled accelerators for deep learning and bioinformatics. A hardware/software co-design strategy enables energy-efficient operation across a heterogeneous compute fabric, targeting real-time, on-device genome analysis. This work exemplifies the integration of deep learning, edge computing, and domain-specific hardware to advance next-generation mobile genomics.</p><p><h4>cs.AR, cs.ET</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08757'>LOTION: Smoothing the Optimization Landscape for Quantized Training</a></h3><h3><a href='https://arxiv.org/pdf/2510.08757' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/25</p><p><b>作者：</b>Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</p><p>Optimizing neural networks for quantized objectives is fundamentally challenging because the quantizer is piece-wise constant, yielding zero gradients everywhere except at quantization thresholds where the derivative is undefined. Most existing methods deal with this issue by relaxing gradient computations with techniques like Straight Through Estimators (STE) and do not provide any guarantees of convergence. In this work, taking inspiration from Nesterov smoothing, we approximate the quantized loss surface with a continuous loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision \textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the raw quantized loss with its expectation under unbiased randomized-rounding noise. In this framework, standard optimizers are guaranteed to converge to a local minimum of the loss surface. Moreover, when using noise derived from stochastic rounding, we show that the global minima of the original quantized loss are preserved. We empirically demonstrate that this method outperforms standard QAT on synthetic testbeds and on 150M- and 300M- parameter language models.</p><p><h4>cs.LG, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08842'>Maple: A Multi-agent System for Portable Deep Learning across Clusters</a></h3><h3><a href='https://arxiv.org/pdf/2510.08842' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/25</p><p><b>作者：</b>Molang Wu, Zhao Zhang</p><p>Training deep learning (DL) models across Graphics Processing Unit (GPU) clusters is technically challenging. One aspect is that users have to compose command lines to adapt to the heterogeneous launchers, schedulers, affinity options, DL framework arguments, and environment variables. Composing correct command lines is error-prone and can easily frustrate users, impeding research or wasting resources. In this work, we present Maple, a multi-agent system that generates correct DL command lines with users&#x27; natural language input. Maple consists of four agents with the functionalities of information extraction, template retrieval, command line verification, and error correction. We evaluate Maple on nine GPU clusters across national computing centers in the U.S., five representative deep learning model families, and four commonly used parallel DL training paradigms. Our experiments also cover schedulers of SLURM and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command lines across the 567 test cases. Leverage multiple language models with an aggregated size of 10B parameters, Maple delivers comparable performance to the state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results highlight Maple&#x27;s practical value in enabling portable and scalable distributed DL across heterogeneous HPC environments.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09163'>Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors</a></h3><h3><a href='https://arxiv.org/pdf/2510.09163' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/25</p><p><b>作者：</b>Alessandro Ottaviano, Andrino Meli, Paul Scheffler, Giovanni Bambini, Robert Balas, Davide Rossi, Andrea Bartolini, Luca Benini</p><p>Managing energy and thermal profiles is critical for many-core HPC processors with hundreds of application-class processing elements (PEs). Advanced model predictive control (MPC) delivers state-of-the-art performance but requires solving an online optimization problem over a thousand times per second (1 kHz control bandwidth), with computational and memory demands scaling with PE count. Traditional MPC approaches execute the controller on the PEs, but operating system overheads create jitter and limit control bandwidth. Running MPC on dedicated on-chip controllers enables fast, deterministic control but raises concerns about area and power overhead. In this work, we tackle these challenges by proposing a hardware-software codesign of a lightweight MPC controller, based on an operator-splitting quadratic programming solver and an embedded multi-core RISC-V controller. Key innovations include pruning weak thermal couplings to reduce model memory and ahead-of-time scheduling for efficient parallel execution of sparse triangular systems arising from the optimization problem. The proposed controller achieves sub-millisecond latency when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x higher energy efficiency than a single-core baseline. Operating within a compact less than 1 MiB memory footprint, it consumes as little as 325 mW while occupying less than 1.5% of a typical HPC processor&#x27;s die area.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.15562'>Lower Bounds for $k$-Set Agreement in Fault-Prone Networks</a></h3><h3><a href='https://arxiv.org/pdf/2508.15562' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/25</p><p><b>作者：</b>Pierre Fraigniaud, Minh Hang Nguyen, Ami Paz, Ulrich Schmid, Hugo Rincon Galeana</p><p>We develop a new lower bound for k-set agreement in synchronous message-passing systems connected by an arbitrary directed communication network, where up to t processes may crash. Our result thus generalizes the t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri, Herlihy, Lynch, and Tuttle [JACM&#x27;00]. Moreover, it generalizes two lower bounds for oblivious algorithms in synchronous systems connected by an arbitrary undirected communication network known to the processes, namely, the domination number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and Travers [TCS&#x27;21] for failure-free processes, and the radius-based lower bound in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS&#x27;24].  Our topological proof non-trivially generalizes and extends the connectivity-based approach for the complete network, as presented in the book by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable carrier maps that, starting from a shellable input complex, determine the evolution of the protocol complex: During the first t/k rounds, carrier maps that crash exactly k processes per round are used, ensuring high connectivity of their images. A Sperner&#x27;s lemma style argument is used to prove that k-set agreement is still impossible by that round. From round t/k+1 up to our lower bound, we employ a novel carrier map that maintains high connectivity. Our proof also provides a strikingly simple lower bound for k-set agreement in synchronous systems with an arbitrary communication network with initial crashes. We express the resulting additional agreement overhead via an appropriately defined radius of the communication graphs. Finally, we prove that the usual input pseudosphere complex for k-set agreement can be replaced by an exponentially smaller input complex based on Kuhn triangulations, which we prove to be also shellable.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08874'>Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication</a></h3><h3><a href='https://arxiv.org/pdf/2510.08874' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/25</p><p><b>作者：</b>Benjamin Brock, Renato Golin</p><p>Many important applications across science, data analytics, and AI workloads depend on distributed matrix multiplication. Prior work has developed a large array of algorithms suitable for different problem sizes and partitionings including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is that existing algorithms are limited to a subset of partitionings. Multiple algorithm implementations are required to support the full space of possible partitionings. If no algorithm implementation is available for a particular set of partitionings, one or more operands must be redistributed, increasing communication costs. This paper presents a universal one-sided algorithm for distributed matrix multiplication that supports all combinations of partitionings and replication factors. Our algorithm uses slicing (index arithmetic) to compute the sets of overlapping tiles that must be multiplied together. This list of local matrix multiplies can then either be executed directly, or reordered and lowered to an optimized IR to maximize overlap. We implement our algorithm using a high-level C++-based PGAS programming framework that performs direct GPU-to-GPU communication using intra-node interconnects. We evaluate performance for a wide variety of partitionings and replication factors, finding that our work is competitive with PyTorch DTensor, a highly optimized distributed tensor library targeting AI models.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.19078'>Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices</a></h3><h3><a href='https://arxiv.org/pdf/2508.19078' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/25</p><p><b>作者：</b>Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu</p><p>Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model quantization, computation offloading, or expert pruning. However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based LLMs across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) quantization-based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while preserving accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08700'>Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System</a></h3><h3><a href='https://arxiv.org/pdf/2510.08700' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/25</p><p><b>作者：</b>Zhuolun Li, Haluk Sonmezler, Faiza Shirazi, Febin Shaji, Tymoteusz Mroczkowski, Dexter Lardner, Matthew Alain Camus, Evangelos Pournaras</p><p>Ensuring ballot secrecy is critical for fair and trustworthy electronic voting systems, yet achieving strong secrecy guarantees in decentralized, large-scale elections remains challenging. This paper proposes the concept of collectively secure voting, in which voters themselves can opt in as secret holders to protect ballot secrecy. A practical blockchain-based collectively secure voting system is designed and implemented. Our design strikes a balance between strong confidentiality guarantees and real-world applicability. The proposed system combines threshold cryptography and smart contracts to ensure ballots remain confidential during voting, while all protocol steps remain transparent and verifiable. Voters can use the system without prior blockchain knowledge through an intuitive user interface that hides underlying complexity. To evaluate this approach, a user testing is conducted. Results show a high willingness to act as secret holders, reliable participation in share release, and high security confidence in the proposed system. The findings demonstrate that voters can collectively maintain secrecy and that such a practical deployment is feasible.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08803'>Man-Made Heuristics Are Dead. Long Live Code Generators!</a></h3><h3><a href='https://arxiv.org/pdf/2510.08803' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/25</p><p><b>作者：</b>Rohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, Daehyeok Kim</p><p>Policy design for various systems controllers has conventionally been a manual process, with domain experts carefully tailoring heuristics for the specific instance in which the policy will be deployed. In this paper, we re-imagine policy design via a novel automated search technique fueled by recent advances in generative models, specifically Large Language Model (LLM)-driven code generation. We outline the design and implementation of PolicySmith, a framework that applies LLMs to synthesize instance-optimal heuristics. We apply PolicySmith to two long-standing systems policies - web caching and congestion control, highlighting the opportunities unraveled by this LLM-driven heuristic search. For caching, PolicySmith discovers heuristics that outperform established baselines on standard open-source traces. For congestion control, we show that PolicySmith can generate safe policies that integrate directly into the Linux kernel.</p><p><h4>cs.OS, cs.DC, cs.LG, cs.NE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08839'>Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction</a></h3><h3><a href='https://arxiv.org/pdf/2510.08839' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/25</p><p><b>作者：</b>Motahare Mounesan, Sourya Saha, Houchao Gan, Md. Nurul Absur, Saptarshi Debroy</p><p>Real-time multi-view 3D reconstruction is a mission-critical application for key edge-native use cases, such as fire rescue, where timely and accurate 3D scene modeling enables situational awareness and informed decision-making. However, the dynamic and unpredictable nature of edge resource availability introduces disruptions, such as degraded image quality, unstable network links, and fluctuating server loads, which challenge the reliability of the reconstruction pipeline. In this work, we present a reinforcement learning (RL)-based edge resource management framework for reliable 3D reconstruction to ensure high quality reconstruction within a reasonable amount of time, despite the system operating under a resource-constrained and disruption-prone environment. In particular, the framework adopts two cooperative Q-learning agents, one for camera selection and one for server selection, both of which operate entirely online, learning policies through interactions with the edge environment. To support learning under realistic constraints and evaluate system performance, we implement a distributed testbed comprising lab-hosted end devices and FABRIC infrastructure-hosted edge servers to emulate smart city edge infrastructure under realistic disruption scenarios. Results show that the proposed framework improves application reliability by effectively balancing end-to-end latency and reconstruction quality in dynamic environments.</p><p><h4>cs.LG, cs.AI, cs.CV, cs.DC, cs.GR, cs.MM</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08863'>Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly</a></h3><h3><a href='https://arxiv.org/pdf/2510.08863' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/25</p><p><b>作者：</b>Deep Bodra, Sushil Khairnar</p><p>The rise of distributed applications and cloud computing has created a demand for scalable, high-performance key-value storage systems. This paper presents a performance evaluation of three prominent NoSQL key-value stores: Redis, Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB) framework. We conducted extensive experiments across three distinct workload patterns (read-heavy, write-heavy), and balanced while systematically varying client concurrency from 1 to 32 clients. Our evaluation methodology captures both latency, throughput, and memory characteristics under realistic operational conditions, providing insights into the performance trade-offs and scalability behaviour of each system</p><p><h4>cs.DB, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.08976'>Hierarchical Scheduling for Multi-Vector Image Retrieval</a></h3><h3><a href='https://arxiv.org/pdf/2510.08976' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/25</p><p><b>作者：</b>Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen</p><p>To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications. However, conventional retrieval approaches often suffer from limited retrieval accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images. They still suffer from sub-optimal accuracy and efficiency, overlooking alignment between the query and varying image objects and redundant fine-grained image segments. In this work, we present an efficient scheduling framework for image retrieval - HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple intermediate granularities for varying image objects to enhance alignment. Second, we minimize redundancy in retrieval by leveraging cross-hierarchy similarity consistency and hierarchy sparsity to minimize unnecessary matching computation. Furthermore, we configure parameters for each dataset automatically for practicality across diverse scenarios. Our empirical study shows that, HiMIR not only achieves substantial accuracy improvements but also reduces computation by up to 3.5 times over the existing MVR system.</p><p><h4>cs.CV, cs.DC, cs.IR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.09143'>Multiparty equality in the local broadcast model</a></h3><h3><a href='https://arxiv.org/pdf/2510.09143' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/25</p><p><b>作者：</b>Louis Esperet, Jean-Florent Raymond</p><p>In this paper we consider the multiparty equality problem in graphs, where every vertex of a graph $G$ is given an input, and the goal of the vertices is to decide whether all inputs are equal. We study this problem in the local broadcast model, where a message sent by a vertex is received by all its neighbors and the total cost of a protocol is the sum of the lengths of the messages sent by the vertices. This setting was studied by Khan and Vaidya, who gave in 2021 a protocol achieving a 4-approximation in the general case.  We study this multiparty communication problem through the lens of network topology. We design a new protocol for 2-connected graphs, whose efficiency relies on the notion of total vertex cover in graph theory. This protocol outperforms the aforementioned 4-approximation in a number of cases. To demonstrate its applicability, we apply it to obtain optimal or asymptotically optimal protocols for several natural network topologies such as cycles, hypercubes, and grids. On the way we also provide new bounds of independent interest on the size of total vertex covers in regular graphs.</p><p><h4>math.CO, cs.CC, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.09764'>FAST: An Efficient Scheduler for All-to-All GPU Communication</a></h3><h3><a href='https://arxiv.org/pdf/2505.09764' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/25</p><p><b>作者：</b>Yiran Lei, Dongjoo Lee, Liangyu Zhao, Daniar Kurniawan, Chanmyeong Kim, Heetaek Jeong, Changsu Kim, Hyeonseong Choi, Liangcheng Yu, Arvind Krishnamurthy, Justine Sherry, Eriko Nurvitadhi</p><p>All-to-All(v) communication is a critical primitive in modern machine learning workloads, particularly mixture-of-experts (MoE) models. Unfortunately, efficient scheduling is challenging due to workload skew, heterogeneous two-tier fabrics, and incast congestion, compounded by the dynamic nature of MoE workloads, where traffic shifts every few hundred milliseconds. Existing schedulers are hardly scalable, incurring seconds to hours of synthesis time, making them impractical. We present FAST, an efficient All-to-All(v) scheduler. FAST addresses skew through intra-server rebalancing and enforces balanced, one-to-one scale-out transfers that avoid incast. Evaluated extensively on both NVIDIA H200 and AMD MI300X clusters, FAST consistently outperforms state-of-the-art solutions on skewed workloads while reducing synthesis time by orders of magnitude.</p><p><h4>cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2507.20424'>Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning</a></h3><h3><a href='https://arxiv.org/pdf/2507.20424' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/25</p><p><b>作者：</b>Tolga Dimlioglu, Anna Choromanska</p><p>We study centralized distributed data parallel training of deep neural networks (DNNs), aiming to improve the trade-off between communication efficiency and model performance of the local gradient methods. To this end, we revisit the flat-minima hypothesis, which suggests that models with better generalization tend to lie in flatter regions of the loss landscape. We introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and demonstrate its strong correlation with the generalization gap of DNNs. We incorporate an efficient relaxation of this measure into the distributed training objective as a lightweight regularizer that encourages workers to collaboratively seek wide minima. The regularizer exerts a pushing force that counteracts the consensus step pulling the workers together, giving rise to the Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF outperforms other communication-efficient approaches and achieves better generalization performance than local gradient methods and synchronous gradient averaging, while maintaining communication efficiency. In addition, our loss landscape visualizations confirm the ability of DPPF to locate flatter minima. On the theoretical side, we show that DPPF guides workers to span flat valleys, with the final valley width governed by the interplay between push and pull strengths, and that its pull-push dynamics is self-stabilizing. We further provide generalization guarantees linked to the valley width and prove convergence in the non-convex setting.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.07922'>SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening</a></h3><h3><a href='https://arxiv.org/pdf/2510.07922' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/25</p><p><b>作者：</b>Murtaza Rangwala, Farag Azzedin, Richard O. Sinnott, Rajkumar Buyya</p><p>Decentralized Federated Learning (DFL) enables privacy-preserving collaborative training without centralized servers, but remains vulnerable to Byzantine attacks where malicious clients submit corrupted model updates. Existing Byzantine-robust DFL defenses rely on similarity-based neighbor screening that requires every client to exchange and compare complete high-dimensional model vectors with all neighbors in each training round, creating prohibitive communication and computational costs that prevent deployment at web scale. We propose SketchGuard, a general framework that decouples Byzantine filtering from model aggregation through sketch-based neighbor screening. SketchGuard compresses $d$-dimensional models to $k$-dimensional sketches ($k \ll d$) using Count Sketch for similarity comparisons, then selectively fetches full models only from accepted neighbors, reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| + d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \le |N_i|$ is the accepted neighbor count. We establish rigorous convergence guarantees in both strongly convex and non-convex settings, proving that Count Sketch compression preserves Byzantine resilience with controlled degradation bounds where approximation errors introduce only a $(1+O(\epsilon))$ factor in the effective threshold parameter. Comprehensive experiments across multiple datasets, network topologies, and attack scenarios demonstrate that SketchGuard maintains identical robustness to state-of-the-art methods while reducing computation time by up to 82% and communication overhead by 50-70% depending on filtering effectiveness, with benefits scaling multiplicatively with model dimensionality and network connectivity. These results establish the viability of sketch-based compression as a fundamental enabler of robust DFL at web scale.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.16293'>Robust LLM Training Infrastructure at ByteDance</a></h3><h3><a href='https://arxiv.org/pdf/2509.16293' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/25</p><p><b>作者：</b>Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, Liang Xiang</p><p>The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.</p><p><h4>cs.LG, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.03298'>CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models</a></h3><h3><a href='https://arxiv.org/pdf/2510.03298' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/25</p><p><b>作者：</b>Dongqi Zheng, Wenjin Fu</p><p>We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.</p><p><h4>cs.LG, cs.CL, cs.DC</h4></p></div><hr>
</body>
</html>
