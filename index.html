<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-28</h1>
<div><h3><a href='https://arxiv.org/abs/2510.21865'>Prefetching Cache Optimization Using Graph Neural Networks: A Modular Framework and Conceptual Analysis</a></h3><h3><a href='https://arxiv.org/pdf/2510.21865' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/38</p><p><b>作者：</b>F. I. Qowy</p><p>Caching and prefetching techniques are fundamental to modern computing, serving to bridge the growing performance gap between processors and memory. Traditional prefetching strategies are often limited by their reliance on predefined heuristics or simplified statistical models, which fail to capture the complex, non-linear dependencies in modern data access patterns. This paper introduces a modular framework leveraging Graph Neural Networks (GNNs) to model and predict access patterns within graph-structured data, focusing on web navigation and hierarchical file systems. The toolchain consists of: a route mapper for extracting structural information, a graph constructor for creating graph representations, a walk session generator for simulating user behaviors, and a gnn prefetch module for training and inference. We provide a detailed conceptual analysis showing how GNN-based approaches can outperform conventional methods by learning intricate dependencies. This work offers both theoretical foundations and a practical, replicable pipeline for future research in graph-driven systems optimization.</p><p><h4>cs.PF, cs.LG, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22763'>Iterative Layer Pruning for Efficient Translation Inference</a></h3><h3><a href='https://arxiv.org/pdf/2510.22763' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/38</p><p><b>作者：</b>Yasmin Moslem, Muhammad Hazim Al Farouq, John D. Kelleher</p><p>Large language models (LLMs) have transformed many areas of natural language processing, including machine translation. However, efficient deployment of LLMs remains challenging due to their intensive computational requirements. In this paper, we address this challenge and present our submissions to the Model Compression track at the Conference on Machine Translation (WMT 2025). In our experiments, we investigate iterative layer pruning guided by layer importance analysis. We evaluate this method using the Aya-Expanse-8B model for translation from Czech to German, and from English to Egyptian Arabic. Our approach achieves substantial reductions in model size and inference time, while maintaining the translation quality of the baseline models.</p><p><h4>cs.CL, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22909'>Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions</a></h3><h3><a href='https://arxiv.org/pdf/2510.22909' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/38</p><p><b>作者：</b>Zongshun Zhang, Ibrahim Matta</p><p>Edge intelligent applications like VR/AR and language model based chatbots have become widespread with the rapid expansion of IoT and mobile devices. However, constrained edge devices often cannot serve the increasingly large and complex deep learning (DL) models. To mitigate these challenges, researchers have proposed optimizing and offloading partitions of DL models among user devices, edge servers, and the cloud. In this setting, users can take advantage of different services to support their intelligent applications. For example, edge resources offer low response latency. In contrast, cloud platforms provide low monetary cost computation resources for computation-intensive workloads. However, communication between DL model partitions can introduce transmission bottlenecks and pose risks of data leakage. Recent research aims to balance accuracy, computation delay, transmission delay, and privacy concerns. They address these issues with model compression, model distillation, transmission compression, and model architecture adaptations, including internal classifiers. This survey contextualizes the state-of-the-art model offloading methods and model adaptation techniques by studying their implication to a multi-objective optimization comprising inference latency, data privacy, and resource monetary cost.</p><p><h4>cs.DC, cs.AI, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.08343'>A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving</a></h3><h3><a href='https://arxiv.org/pdf/2508.08343' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/38</p><p><b>作者：</b>Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral</p><p>With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads.</p><p><h4>cs.PF, cs.AI, cs.CL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2310.14283'>Bandwidth Efficient Livestreaming in Mobile Wireless Networks: A Peer-to-Peer ACIDE Solution</a></h3><h3><a href='https://arxiv.org/pdf/2310.14283' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/38</p><p><b>作者：</b>Andrei Negulescu, Weijia Shang</p><p>In mobile wireless networks, livestreaming in high user density areas presents two typical challenges: the wireless bandwidth is depleted and the number of users is limited. In this study, a media distribution model utilizing peer to peer communications, Active Control in an Intelligent and Distributed Environment, is proposed for bandwidth efficient livestreaming. The basic idea is to group users with identical livestream interest in a cluster of n peers. Instead of sending n copies of a livestream package, only one copy is sent to the cluster. A package is divided into n blocks. Each user receives one block from the base station and the remaining n-1 blocks from the other peers. Two optimization problems are addressed. The first problem is minimizing the bandwidth needed to guarantee a continuous live media play on all peers. A solution is proposed to find the optimal block sizes such that the wireless bandwidth is minimized. The second problem is maximizing the number of peers admitted to a cluster, given a fixed wireless bandwidth. This problem is NP-complete and a greedy strategy is proposed to calculate a feasible solution for peer selection. The proposed model improves the bandwidth efficiency and allows more users to be served.</p><p><h4>cs.NI, cs.DC, cs.PF, cs.SY, eess.SY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2411.05323'>TraDE: Network and Traffic-aware Adaptive Scheduling for Microservices Under Dynamics</a></h3><h3><a href='https://arxiv.org/pdf/2411.05323' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/38</p><p><b>作者：</b>Ming Chen, Muhammed Tawfiqul Islam, Maria Rodriguez Read, Rajkumar Buyya</p><p>The transition from monolithic architecture to microservices has enhanced flexibility in application design and its scalable execution. This approach typically uses a computing cluster managed by a container orchestration platform to deploy microservices. However, this shift introduces significant challenges, particularly in the efficient scheduling of containerized services. These challenges are compounded by unpredictable scenarios such as dynamic incoming workloads with various execution traffic and variable communication delays among cluster nodes. Existing works often overlook the real-time traffic impacts of dynamic requests on running microservices, as well as the varied communication delays across cluster nodes. Consequently, even optimally deployed microservices could suffer from significant performance degradation over time. To address these issues, we propose a network and traffic-aware adaptive scheduling framework, TraDE, which can adaptively redeploy microservice instances to maintain desired performance amid changing traffic and network conditions within the hosting cluster. We have implemented TraDE as an extension to the Kubernetes platform. Additionally, we deployed realistic microservice applications in a real compute cluster and conducted extensive experiments to assess our framework&#x27;s performance in various scenarios. The results demonstrate the effectiveness of TraDE in rescheduling running microservices to enhance end-to-end performance while maintaining a high goodput ratio. Compared with the existing method NetMARKS, TraDE outperforms it by reducing the average response time of the application by up to 48.3%, and improving the throughput by up to 1.2-1.5x across workloads while maintaining a goodput ratio of 95.36%, and showing robust adaptive capability to meet QoS targets under sustained workloads and dynamic networking conditions.</p><p><h4>cs.NI, cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2503.05530'>Leveraging Approximate Caching for Faster Retrieval-Augmented Generation</a></h3><h3><a href='https://arxiv.org/pdf/2503.05530' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/38</p><p><b>作者：</b>Shai Bergman, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos, Ji Zhang</p><p>Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing the reliance on expensive vector database lookups. To efficiently scale, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question-answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically-skewed MedRAG workload reduces database calls by 77.2% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our results demonstrate that approximate caching is a practical and effective strategy for optimizing RAG-based systems.</p><p><h4>cs.DB, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2507.10367'>FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline</a></h3><h3><a href='https://arxiv.org/pdf/2507.10367' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/38</p><p><b>作者：</b>Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen</p><p>Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system&#x27;s production environment with 10,000 NPUs for one year and has been open-sourced.</p><p><h4>cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.21745'>Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis</a></h3><h3><a href='https://arxiv.org/pdf/2510.21745' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/38</p><p><b>作者：</b>Eashan Wadhwa, Shanker Shreejith</p><p>Excessive switching activity is a primary contributor to dynamic power dissipation in modern FPGAs, where fine-grained configurability amplifies signal toggling and associated capacitance. Conventional low-power techniques -- gating, clock-domain partitioning, and placement-aware netlist rewrites - either require intrusive design changes or offer diminishing returns as device densities grow. In this work, we present Simopt-power, a simulator-driven optimisation framework that leverages simulation analysis to identify and selectively reconfigure high-toggle paths. By feeding activity profiles back into a lightweight transformation pass, Simopt-power judiciously inserts duplicate truth table logic using Shannon Decomposition principle and relocates critical nets, thereby attenuating unnecessary transitions without perturbing functional behaviour. We evaluated this framework on open-source RTLLM benchmark, with Simopt-power achieves an average switching-induced power reduction of ~9\% while incurring only ~9\% additional LUT-equivalent resources for arithmetic designs. These results demonstrate that coupling simulation insights with targeted optimisations can yield a reduced dynamic power, offering a practical path toward using simulation metadata in the FPGA-CAD flow.</p><p><h4>cs.AR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22087'>QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture</a></h3><h3><a href='https://arxiv.org/pdf/2510.22087' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/38</p><p><b>作者：</b>Shvetank Prakash, Andrew Cheng, Arya Tschand, Mark Mazumder, Varun Gohil, Jeffrey Ma, Jason Yik, Zishen Wan, Jessica Quaye, Elisavet Lydia Alvanaki, Avinash Kumar, Chandrashis Mazumdar, Tuhin Khare, Alexander Ingare, Ikechukwu Uchendu, Radhika Ghosal, Abhishek Tyagi, Chenyu Wang, Andrea Mattia Garavagno, Sarah Gu, Alice Guo, Grace Hur, Luca Carloni, Tushar Krishna, Ankita Nayak, Amir Yazdanbakhsh, Vijay Janapa Reddi</p><p>The field of computer architecture, which bridges high-level software abstractions and low-level hardware implementations, remains absent from current large language model (LLM) evaluations. To this end, we present QuArch (pronounced &#x27;quark&#x27;), the first benchmark designed to facilitate the development and evaluation of LLM knowledge and reasoning capabilities specifically in computer architecture. QuArch provides a comprehensive collection of 2,671 expert-validated question-answer (QA) pairs covering various aspects of computer architecture, including processor design, memory systems, and interconnection networks. Our evaluation reveals that while frontier models possess domain-specific knowledge, they struggle with skills that require higher-order thinking in computer architecture. Frontier model accuracies vary widely (from 34% to 72%) on these advanced questions, highlighting persistent gaps in architectural reasoning across analysis, design, and implementation QAs. By holistically assessing fundamental skills, QuArch provides a foundation for building and measuring LLM capabilities that can accelerate innovation in computing systems. With over 140 contributors from 40 institutions, this benchmark represents a community effort to set the standard for architectural reasoning in LLM evaluation.</p><p><h4>cs.AR, cs.AI, cs.LG, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22627'>RAMAN: Resource-efficient ApproxiMate Posit Processing for Algorithm-Hardware Co-desigN</a></h3><h3><a href='https://arxiv.org/pdf/2510.22627' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/38</p><p><b>作者：</b>Mohd Faisal Khan, Mukul Lokhande, Santosh Kumar Vishvakarma</p><p>Edge-AI applications still face considerable challenges in enhancing computational efficiency in resource-constrained environments. This work presents RAMAN, a resource-efficient and approximate posit(8,2)-based Multiply-Accumulate (MAC) architecture designed to improve hardware efficiency within bandwidth limitations. The proposed REAP (Resource-Efficient Approximate Posit) MAC engine, which is at the core of RAMAN, uses approximation in the posit multiplier to achieve significant area and power reductions with an impact on accuracy. To support diverse AI workloads, this MAC unit is incorporated in a scalable Vector Execution Unit (VEU), which permits hardware reuse and parallelism among deep neural network layers. Furthermore, we propose an algorithm-hardware co-design framework incorporating approximation-aware training to evaluate the impact of hardware-level approximation on application-level performance. Empirical validation on FPGA and ASIC platforms shows that the proposed REAP MAC achieves up to 46% in LUT savings and 35.66% area, 31.28% power reduction, respectively, over the baseline Posit Dot-Product Unit (PDPU) design, while maintaining high accuracy (98.45%) for handwritten digit recognition. RAMAN demonstrates a promising trade-off between hardware efficiency and learning performance, making it suitable for next-generation edge intelligence.</p><p><h4>cs.AR, cs.NE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22674'>Approximate Signed Multiplier with Sign-Focused Compressor for Edge Detection Applications</a></h3><h3><a href='https://arxiv.org/pdf/2510.22674' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/38</p><p><b>作者：</b>L. Hemanth Krishna, Srinivasu Bodapati, Sreehari Veeramachaneni, BhaskaraRao Jammu, Noor Mahammad Sk</p><p>This paper presents an approximate signed multiplier architecture that incorporates a sign-focused compressor, specifically designed for edge detection applications in machine learning and signal processing. The multiplier incorporates two types of sign-focused compressors: A + B + C + 1 and A + B + C + D + 1. Both exact and approximate compressor designs are utilized, with a focus on efficiently handling constant value &quot;1&quot; and negative partial products, which frequently appear in the partial product matrices of signed multipliers. To further enhance efficiency, the lower N - 1 columns of the partial product matrix are truncated, followed by an error compensation mechanism. Experimental results show that the proposed 8-bit approximate multiplier achieves a 29.21% reduction in power delay product (PDP) and a 14.39% reduction in power compared to the best of existing multipliers. The proposed multiplier is integrated into a custom convolution layer and performs edge detection, demonstrating its practical utility in real-world applications.</p><p><h4>cs.AR, cs.IT, eess.IV, math.IT</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22046'>HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology</a></h3><h3><a href='https://arxiv.org/pdf/2510.22046' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/38</p><p><b>作者：</b>Daniel G. P. Petrini, Braz Izaias da Silva Junior</p><p>We present a case study applying the SpecC methodology within a system-level hardware/software co-design flow to a PCM-to-PWM converter, the core of a Class-D audio amplifier. The converter was modeled and explored with SpecC methodology to derive an HW/SW partition. Using system-level estimates and fast functional simulation, we evaluated mappings that meet real-time constraints while reducing estimated cost of an all-hardware solution and avoiding the expense of a purely software implementation on a high-end processor. Despite the design&#x27;s moderate complexity, the results underline the value of system-level co-design for early architectural insight, rapid validation, and actionable cost/performance trade-offs. [Original work from 2005; formatting revised in 2025, with no changes to the results.]</p><p><h4>cs.AI, cs.AR, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22661'>RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography</a></h3><h3><a href='https://arxiv.org/pdf/2510.22661' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/38</p><p><b>作者：</b>Malik Imran, Safiullah Khan, Zain Ul Abideen, Ciara Rafferty, Ayesha Khalid, Muhammad Rashid, Maire O&#x27;Neill</p><p>Post-quantum multivariate public key cryptography (MPKC) schemes resist quantum threats but require heavy operations, such as rejection sampling, which challenge resource-limited devices. Prior hardware designs have addressed various aspects of MPKC signature generation. However, rejection sampling remains largely unexplored in such contexts. This paper presents RejSCore, a lightweight hardware accelerator for rejection sampling in post-quantum cryptography. It specifically targets the QR-UOV scheme, which is a prominent candidate under the second-round of the National Institute of Standards and Technology (NIST) additional digital signature standardization process. The architecture includes an AES-CTR-128-based pseudorandom number generator. Moreover, a lightweight iterative method is employed in rejection sampling, offering reduced resource consumption and area overhead while slightly increasing latency. The performance of RejSCore is comprehensively evaluated on Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and 565 MHz, respectively. Using the QR-UOV parameters for security level I ($q = 127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525 clock cycles. The ADP and PDP evaluations confirm RejSCore&#x27;s suitability for deployment in resource-constrained and security-critical environments.</p><p><h4>cs.CR, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23472'>BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement</a></h3><h3><a href='https://arxiv.org/pdf/2510.23472' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/38</p><p><b>作者：</b>Ke Xue, Ruo-Tong Chen, Rong-Xi Tan, Xi Lin, Yunqi Shi, Siyuan Xu, Mingxuan Yuan, Chao Qian</p><p>Chip placement is a vital stage in modern chip design as it has a substantial impact on the subsequent processes and the overall quality of the final chip. The use of black-box optimization (BBO) for chip placement has a history of several decades. However, early efforts were limited by immature problem formulations and inefficient algorithm designs. Recent progress has shown the effectiveness and efficiency of BBO for chip placement, proving its potential to achieve state-of-the-art results. Despite these advancements, the field lacks a unified, BBO-specific benchmark for thoroughly assessing various problem formulations and BBO algorithms. To fill this gap, we propose BBOPlace-Bench, the first benchmark designed specifically for evaluating and developing BBO algorithms for chip placement tasks. It integrates three problem formulations of BBO for chip placement, and offers a modular, decoupled, and flexible framework that enables users to seamlessly implement, test, and compare their own algorithms. BBOPlace-Bench integrates a wide variety of existing BBO algorithms, including simulated annealing (SA), evolutionary algorithms (EAs), and Bayesian optimization (BO). Experimental results show that the problem formulations of mask-guided optimization and hyperparameter optimization exhibit superior performance than the sequence pair problem formulation, while EAs demonstrate better overall performance than SA and BO, especially in high-dimensional search spaces, and also achieve state-of-the-art performance compared to the mainstream chip placement methods. BBOPlace-Bench not only facilitates the development of efficient BBO-driven solutions for chip placement but also broadens the practical application scenarios (which are urgently needed) for the BBO community. The code of BBOPlace-Bench is available at https://github.com/lamda-bbo/BBOPlace-Bench.</p><p><h4>cs.LG, cs.AI, cs.AR, cs.NE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23519'>Architecting Scalable Trapped Ion Quantum Computers using Surface Codes</a></h3><h3><a href='https://arxiv.org/pdf/2510.23519' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/38</p><p><b>作者：</b>Scott Jones (University of Cambridge), Prakash Murali (University of Cambridge)</p><p>Trapped ion (TI) qubits are a leading quantum computing platform. Current TI systems have less than 60 qubits, but a modular architecture known as the Quantum Charge-Coupled Device (QCCD) is a promising path to scale up devices. There is a large gap between the error rates of near-term systems ($10^{-3}$ to $10^{-4}$) and the requirements of practical applications (below $10^{-9}$). To bridge this gap, we require Quantum Error Correction (QEC) to build \emph{logical qubits} that are composed of multiple physical qubits. While logical qubits have been demonstrated on TI qubits, these demonstrations are restricted to small codes and systems. There is no clarity on how QCCD systems should be designed to implement practical-scale QEC. This paper studies how surface codes, a standard QEC scheme, can be implemented efficiently on QCCD-based systems. To examine how architectural parameters of a QCCD system can be tuned for surface codes, we develop a near-optimal topology-aware compilation method that outperforms existing QCCD compilers by an average of 3.8X in terms of logical clock speed. We use this compiler to examine how hardware trap capacity, connectivity and electrode wiring choices can be optimised for surface code implementation. In particular, we demonstrate that small traps of two ions are surprisingly ideal from both a performance-optimal and hardware-efficiency standpoint. This result runs counter to prior intuition that larger traps (20-30 ions) would be preferable, and has the potential to inform design choices for upcoming systems.</p><p><h4>quant-ph, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2503.19447'>Anvil: A General-Purpose Timing-Safe Hardware Description Language</a></h3><h3><a href='https://arxiv.org/pdf/2503.19447' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/38</p><p><b>作者：</b>Jason Zhijingcheng Yu, Aditya Ranjan Jha, Umang Mathur, Trevor E. Carlson, Prateek Saxena</p><p>Expressing hardware designs using hardware description languages (HDLs) routinely involves using stateless signals whose values change according to their underlying registers. Unintended behaviours can arise when the stored values in these underlying registers are mutated while their dependent signals are expected to remain constant across multiple cycles. Such timing hazards are common because, with a few exceptions, existing HDLs lack abstractions for values that remain unchanged over multiple clock cycles, delegating this responsibility to hardware designers. Designers must then carefully decide whether a value should remain unchanged, sometimes even across hardware modules. This paper proposes Anvil, an HDL which statically prevents timing hazards with a novel type system. Anvil is the only HDL we know of that guarantees timing safety, i.e., absence of timing hazards, without sacrificing expressiveness for cycle-level timing control or dynamic timing behaviours. Unlike many HLS languages that abstract away the differences between registers and signals, Anvil&#x27;s type system exposes them fully while capturing the timing relationships between register value mutations and signal usages to enforce timing safety. This, in turn, enables safe composition of communicating hardware modules by static enforcement of timing contracts that encode timing constraints on shared signals. Such timing contracts can be specified parametric on abstract time points that can vary during run-time, allowing the type system to statically express dynamic timing behaviour. We have implemented Anvil and successfully used it to implement key timing-sensitive modules, comparing them against open-source SystemVerilog counterparts to demonstrate the practicality and expressiveness of the generated hardware.</p><p><h4>cs.AR, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.24929'>Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI</a></h3><h3><a href='https://arxiv.org/pdf/2509.24929' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/38</p><p><b>作者：</b>Hongwei Zhao, Vianney Lapotre, Guy Gogniat</p><p>Fault injection attacks exploit physical disturbances to compromise the functionality and security of integrated circuits. As System on Chip (SoC) architectures grow in complexity, the vulnerability of on chip communication fabrics has become increasingly prominent. Buses, serving as interconnects among various IP cores, represent potential vectors for fault-based exploitation. In this study, we perform simulation-driven fault injection across three mainstream bus protocols Wishbone, AXI Lite, and AXI. We systematically examine fault success rates, spatial vulnerability distributions, and timing dependencies to characterize how faults interact with bus-level transactions. The results uncover consistent behavioral patterns across protocols, offering practical insights for both attack modeling and the development of resilient SoC designs.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2410.15742'>DeepVigor+: Scalable and Accurate Semi-Analytical Fault Resilience Analysis for Deep Neural Network</a></h3><h3><a href='https://arxiv.org/pdf/2410.15742' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/38</p><p><b>作者：</b>Mohammad Hasan Ahmadilivani, Jaan Raik, Masoud Daneshtalab, Maksim Jenihhin</p><p>The growing exploitation of Machine Learning (ML) in safety-critical applications necessitates rigorous safety analysis. Hardware reliability assessment is a major concern with respect to measuring the level of safety in ML-based systems. Quantifying the reliability of emerging ML models, including Convolutional Neural Networks (CNNs), is highly complex due to their enormous size in terms of the number of parameters and computations. Conventionally, Fault Injection (FI) is applied to perform a reliability measurement. However, performing FI on modern-day CNNs is prohibitively time-consuming if an acceptable confidence level is to be achieved. To speed up FI for large CNNs, statistical FI (SFI) has been proposed, but its runtimes are still considerably long.  In this work, we introduce DeepVigor+, a scalable, fast, and accurate semi-analytical method as an efficient alternative for reliability measurement in CNNs. DeepVigor+ implements a fault propagation analysis model and attempts to acquire Vulnerability Factors (VFs) as reliability metrics in an optimal way. The results indicate that DeepVigor+ obtains VFs for CNN models with an error less than $1\%$, i.e., the objective in SFI, but with $14.9$ up to $26.9$ times fewer simulations than the best-known state-of-the-art SFI. DeepVigor+ enables an accurate reliability analysis for large and deep CNNs within a few minutes, rather than achieving the same results in days or weeks.</p><p><h4>cs.LG, cs.AR, eess.SP</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.06309'>Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</a></h3><h3><a href='https://arxiv.org/pdf/2502.06309' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/38</p><p><b>作者：</b>Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen</p><p>As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.</p><p><h4>cs.LG, cs.AR, math.OC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22309'>When Agents are Powerful: Black Hole Search in Time-Varying Graphs</a></h3><h3><a href='https://arxiv.org/pdf/2510.22309' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/38</p><p><b>作者：</b>Tanvir Kaur, Ashish Saxena</p><p>A black hole is a harmful node in a graph that destroys any resource entering it, making its identification a critical task. In the \emph{Black Hole Search (BHS)} problem, a team of agents operates on a graph $G$ with the objective that at least one agent must survive and correctly identify an edge incident to the black hole. Prior work has addressed BHS in arbitrary dynamic graphs under the restrictive \emph{face-to-face} communication, where agents can exchange information only when co-located. This constraint significantly increases the number of agents required to solve the problem. In this work, we strengthen the capabilities of agents in two ways: (i) granting them \emph{global communication}, enabling interaction regardless of location, and (ii) equipping them with \emph{1-hop visibility}, allowing each agent to observe its immediate neighborhood. These enhancements lead to more efficient solutions for the BHS problem in dynamic graphs.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2403.09445'>How to Evaluate Distributed Coordination Systems? -- A Survey and Analysis</a></h3><h3><a href='https://arxiv.org/pdf/2403.09445' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/38</p><p><b>作者：</b>Bekir Turkkan, Elvis Rodrigues, Tevfik Kosar, Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas</p><p>Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to the lack of standard benchmarking and evaluation tools for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this study, we analyze and compare the evaluation mechanisms for known and widely used consensus algorithms, distributed coordination services, and distributed applications built on top of these services. We identify the most important requirements of distributed coordination service benchmarking, such as the metrics and parameters for the evaluation of the performance, scalability, availability, and consistency of these systems. Finally, we discuss why the existing benchmarks fail to address the complex requirements of distributed coordination system evaluation.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.02024'>NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs</a></h3><h3><a href='https://arxiv.org/pdf/2506.02024' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/38</p><p><b>作者：</b>Haeun Lee, Omin Kwon, Yeonhong Park, Jae W. Lee</p><p>Meeting service-level objectives (SLOs) in Large Language Models (LLMs) serving is critical, but managing the high variability in load presents a significant challenge. Recent advancements in FP8 inference, backed by native hardware support, offer a potential solution: executing FP16 models by default, while switching to FP8 models during sudden load surges to achieve higher throughput at the cost of a slight quality degradation. Although this approach facilitates effective SLO management, it introduces additional memory overhead due to storing two versions of the same model. In response, this paper proposes NestedFP, an LLM serving technique that supports both FP16 and FP8 models in a memory efficient manner by overlaying FP8 parameters onto FP16 parameters, allowing both models to share the same FP16 memory footprint. By leveraging a compact data format for the overlay and a specialized GEMM kernel optimized for this format, NestedFP ensures minimal degradation in both model quality and inference throughput across both FP8 and FP16 modes. NestedFP provides a flexible platform for dynamic, SLO-aware precision selection. The code is available at https://github.com/SNU-ARC/NestedFP.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22434'>Separation of Unconscious Robots with Obstructed Visibility</a></h3><h3><a href='https://arxiv.org/pdf/2510.22434' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/38</p><p><b>作者：</b>Prajyot Pyati, Navjot Kaur, Saswata Jana, Adri Bhattacharya, Partha Sarathi Mandal</p><p>We study a recently introduced \textit{unconscious} mobile robot model, where each robot is associated with a \textit{color}, which is visible to other robots but not to itself. The robots are autonomous, anonymous, oblivious and silent, operating in the Euclidean plane under the conventional \textit{Look-Compute-Move} cycle. A primary task in this model is the \textit{separation problem}, where unconscious robots sharing the same color must separate from others, forming recognizable geometric shapes such as circles, points, or lines. All prior works model the robots as \textit{transparent}, enabling each to know the positions and colors of all other robots. In contrast, we model the robots as \textit{opaque}, where a robot can obstruct the visibility of two other robots, if it lies on the line segment between them. Under this obstructed visibility, we consider a variant of the separation problem in which robots, starting from any arbitrary initial configuration, are required to separate into concentric semicircles. We present a collision-free algorithm that solves the separation problem under a semi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots. The robots agree on one coordinate axis but have no knowledge of $n$.</p><p><h4>cs.DC, cs.RO</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23503'>Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.23503' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/38</p><p><b>作者：</b>Fatemeh Zahra Safaeipour, Jacob Chakareski, Morteza Hashemi</p><p>Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networks, and the edge device seeks to optimize inference performance under energy and delay constraints. The inference process can be split between the edge device and an edge server, thereby achieving collaborative inference over wireless networks. We formulate an inference utility optimization problem subject to energy and delay constraints, and propose a novel solution called Bayes-Split-Edge, which leverages Bayesian optimization for collaborative split inference over wireless edge networks. Our solution jointly optimizes the transmission power and the neural network split point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition function that balances inference task utility, sample efficiency, and constraint violation penalties. We evaluate our approach using the VGG19 model on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world mMobile wireless channel datasets. Numerical results demonstrate that Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to standard Bayesian optimization and achieves near-linear convergence. It also outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and Proximal Policy Optimization (PPO), while matching exhaustive search performance under tight constraints. These results confirm that the proposed framework provides a sample-efficient solution requiring maximum 20 function evaluations and constraint-aware optimization for wireless split inference in edge computing systems.</p><p><h4>cs.DC, cs.LG, eess.SP</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.21710'>A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.21710' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/38</p><p><b>作者：</b>Lorenzo Porcelli</p><p>Instant payment infrastructures have stringent performance requirements, processing millions of transactions daily with zero-downtime expectations. Traditional monitoring approaches fail to bridge the gap between technical infrastructure metrics and business process visibility. We introduce a novel feature engineering approach based on processing times computed between consecutive ISO 20022 message exchanges, creating a compact representation of system state. By applying anomaly detection to these features, we enable early failure detection and localization, allowing incident classification. Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system, using both real-world incidents and controlled simulations, demonstrates the approach&#x27;s effectiveness in detecting diverse anomaly patterns and provides inherently interpretable explanations that enable operators to understand the business impact. By mapping features to distinct processing phases, the resulting framework differentiates between internal and external payment system issues, significantly reduces investigation time, and bridges observability gaps in distributed systems where transaction state is fragmented across multiple entities.</p><p><h4>cs.LG, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.21950'>Heaven &amp; Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus</a></h3><h3><a href='https://arxiv.org/pdf/2510.21950' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/38</p><p><b>作者：</b>Nnamdi Daniel Aghanya, Romain Leemans</p><p>We study Heaven-Hell dynamics, a model for network consensus. A known result establishes an exact one-step convergence threshold for systems with a single uniform hub: the per-node inbound hub weight W suffices if and only if W &gt;= maxrest, the maximum non-hub inbound mass. We develop scale laws and operational refinements that make this threshold robust to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. Our contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds improving on classical worst-case guarantees, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are mechanized in Coq, with experiments on rings, grids, scale-free graphs, and heterogeneous weighted graphs validating tightness and gap closures</p><p><h4>cs.SI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22149'>Power to the Clients: Federated Learning in a Dictatorship Setting</a></h3><h3><a href='https://arxiv.org/pdf/2510.22149' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/38</p><p><b>作者：</b>Mohammadsajad Alipour, Mohammad Mohammadi Amiri</p><p>Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model&#x27;s convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.</p><p><h4>cs.LG, cs.AI, cs.CL, cs.CR, cs.CV, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22986'>CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs</a></h3><h3><a href='https://arxiv.org/pdf/2510.22986' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/38</p><p><b>作者：</b>Junjie Huang, Minghua He, Jinyang Liu, Yintong Huo, Domenico Bianculli, Michael R. Lyu</p><p>Log-based anomaly detection (LogAD) is critical for maintaining the reliability and availability of large-scale online service systems. While machine learning, deep learning, and large language models (LLMs)-based methods have advanced the LogAD, they often suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, limiting their practicality for real-time, high-volume log analysis. In contrast, rule-based systems offer efficiency and transparency, but require significant manual effort and are difficult to scale across diverse and evolving environments. In this paper, We present CodeAD, a novel framework that automatically synthesizes lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a hierarchical clustering and anchor-grounded sampling strategy to construct representative contrastive log windows, enabling LLMs to discern discriminative anomaly patterns. To ensure robustness and generalizability, CodeAD employs an agentic workflow that iteratively generates, tests, repairs, and refines the rules until it meets correctness and abstraction requirements. The synthesized rules are interpretable, lightweight, and directly executable on raw logs, supporting efficient and transparent online anomaly detection. Our comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1 score over the state-of-the-art baselines, while processing large datasets up to 4x faster and at a fraction of the cost (total LLM invocation cost under 4 USD per dataset). These results highlight CodeAD as a practical and scalable solution for online monitoring systems, enabling interpretable, efficient, and automated LogAD in real-world environment.</p><p><h4>cs.SE, cs.DC, cs.MA</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23019'>Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks</a></h3><h3><a href='https://arxiv.org/pdf/2510.23019' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/38</p><p><b>作者：</b>Gurpreet Singh, Keshav Sood, P. Rajalakshmi, Yong Xiang</p><p>Federated learning (FL) offers a privacy-preserving paradigm for machine learning, but its application in intrusion detection systems (IDS) within IoT networks is challenged by severe class imbalance, non-IID data, and high communication overhead.These challenges severely degrade the performance of conventional FL methods in real-world network traffic classification. To overcome these limitations, we propose Sentinel, a personalized federated IDS (pFed-IDS) framework that incorporates a dual-model architecture on each client, consisting of a personalized teacher and a lightweight shared student model. This design effectively balances deep local adaptation with efficient global model consensus while preserving client privacy by transmitting only the compact student model, thus reducing communication costs. Sentinel integrates three key mechanisms to ensure robust performance: bidirectional knowledge distillation with adaptive temperature scaling, multi-faceted feature alignment, and class-balanced loss functions. Furthermore, the server employs normalized gradient aggregation with equal client weighting to enhance fairness and mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD benchmark datasets demonstrate that Sentinel significantly outperforms state-of-the-art federated methods, establishing a new performance benchmark, especially under extreme data heterogeneity, while maintaining communication efficiency.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23053'>AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</a></h3><h3><a href='https://arxiv.org/pdf/2510.23053' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>31/38</p><p><b>作者：</b>Zhiyu Wang, Suman Raj, Rajkumar Buyya</p><p>Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed&#x27;s practical applicability for large-scale UAV-MEC deployments.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23330'>The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model</a></h3><h3><a href='https://arxiv.org/pdf/2510.23330' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>32/38</p><p><b>作者：</b>Keiya Hirashima, Michiko S. Fujii, Takayuki R. Saitoh, Naoto Harada, Kentaro Nomura, Kohji Yoshikawa, Yutaka Hirai, Tetsuro Asano, Kana Moriwaki, Masaki Iwasawa, Takashi Okamoto, Junichiro Makino</p><p>A major goal of computational astrophysics is to simulate the Milky Way Galaxy with sufficient resolution down to individual stars. However, the scaling fails due to some small-scale, short-timescale phenomena, such as supernova explosions. We have developed a novel integration scheme of $N$-body/hydrodynamics simulations working with machine learning. This approach bypasses the short timesteps caused by supernova explosions using a surrogate model, thereby improving scalability. With this method, we reached 300 billion particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking through the billion-particle barrier currently faced by state-of-the-art simulations. This resolution allows us to perform the first star-by-star galaxy simulation, which resolves individual stars in the Milky Way Galaxy. The performance scales over $10^4$ CPU cores, an upper limit in the current state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA CUDA GPUs.</p><p><h4>astro-ph.GA, cs.DC, cs.LG, physics.comp-ph</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.23408'>AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines</a></h3><h3><a href='https://arxiv.org/pdf/2510.23408' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>33/38</p><p><b>作者：</b>Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer</p><p>Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.</p><p><h4>cs.AI, cs.DC, cs.ET, cs.LG, cs.MA</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.01603'>Unlocking True Elasticity for the Cloud-Native Era with Dandelion</a></h3><h3><a href='https://arxiv.org/pdf/2505.01603' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>34/38</p><p><b>作者：</b>Tom Kuchler, Pinghe Li, Yazhuo Zhang, Lazar Cvetkovi\&#x27;c, Boris Goranov, Tobias Stocker, Leon Thomm, Simone Kalbermatter, Tim Notter, Andrea Lattuada, Ana Klimovic</p><p>Elasticity is fundamental to cloud computing, as it enables quickly allocating resources to match the demand of each workload as it arrives, rather than pre-provisioning resources to meet performance objectives. However, even serverless platforms -- which boot sandboxes in 10s to 100s of milliseconds -- are not sufficiently elastic to avoid over-provisioning expensive resources. Today&#x27;s FaaS platforms rely on pre-provisioning many idle sandboxes in memory to reduce the occurrence of slow, cold starts. A key obstacle for high elasticity is booting a guest OS and configuring features like networking in sandboxes, which are required to expose an isolated POSIX-like interface to user functions. Our key insight is that redesigning the interface for applications in the cloud-native era enables co-designing a much more efficient and elastic execution system. Now is a good time to rethink cloud abstractions as developers are building applications to be cloud-native. Cloud-native applications typically consist of user-provided compute logic interacting with cloud services (for storage, AI inference, query processing, etc) exposed over REST APIs. Hence, we propose Dandelion, an elastic cloud platform with a declarative programming model that expresses applications as DAGs of pure compute functions and higher-level communication functions. Dandelion can securely execute untrusted user compute functions in lightweight sandboxes that cold start in hundreds of microseconds, since pure functions do not rely on extra software environments such as a guest OS. Dandelion makes it practical to boot a sandbox on-demand for each request, decreasing performance variability by two to three orders of magnitude compared to Firecracker and reducing committed memory by 96% on average when running the Azure Functions trace.</p><p><h4>cs.DC, cs.OS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.26529'>CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations</a></h3><h3><a href='https://arxiv.org/pdf/2509.26529' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>35/38</p><p><b>作者：</b>Shangshu Qian, Lin Tan, Yongle Zhang</p><p>Recent studies have revealed that self-sustaining cascading failures in distributed systems frequently lead to widespread outages, which are challenging to contain and recover from. Existing failure detection techniques struggle to expose such failures prior to deployment, as they typically require a complex combination of specific conditions to be triggered. This challenge stems from the inherent nature of cascading failures, as they typically involve a sequence of fault propagations, each activated by distinct conditions.  This paper presents CSnake, a fault injection framework to expose self-sustaining cascading failures in distributed systems. CSnake uses the novel idea of causal stitching, which causally links multiple single-fault injections in different tests to simulate complex fault propagation chains. To identify these chains, CSnake designs a counterfactual causality analysis of fault propagations - fault causality analysis (FCA): FCA compares the execution trace of a fault injection run with its corresponding profile run (i.e., same test w/o the injection) and identifies any additional faults triggered, which are considered to have a causal relationship with the injected fault.  To address the large search space of fault and workload combinations, CSnake employs a three-phase allocation protocol of test budget that prioritizes faults with unique and diverse causal consequences, increasing the likelihood of uncovering conditional fault propagations. Furthermore, to avoid incorrectly connecting fault propagations from workloads with incompatible conditions, CSnake performs a local compatibility check that approximately checks the compatibility of the path constraints associated with connected fault propagations with low overhead.  CSnake detected 15 bugs that cause self-sustaining cascading failures in five systems, five of which have been confirmed with two fixed.</p><p><h4>cs.DC, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.05109'>Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices</a></h3><h3><a href='https://arxiv.org/pdf/2510.05109' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>36/38</p><p><b>作者：</b>Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee</p><p>Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks&#x27;&#x27; (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly half a day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.</p><p><h4>cs.DC, cs.AI, cs.CL, eess.SP</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2302.10883'>Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions</a></h3><h3><a href='https://arxiv.org/pdf/2302.10883' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>37/38</p><p><b>作者：</b>Mahdi Ghafourian, Bilgesu Sumer, Ruben Vera-Rodriguez, Julian Fierrez, Ruben Tolosana, Aythami Moralez, Els Kindt</p><p>Biometric recognition as an efficient and hard-to-forge way of identification and verification has become an indispensable part of the current digital world. The fast evolution of this technology has been a strong incentive for integration into many applications. Meanwhile, blockchain, the decentralized ledger technology, has been widely received by both research and industry in the past few years, and it is being increasingly deployed today in many different applications, such as money transfer, IoT, healthcare, or logistics. Recently, researchers have started to speculate on the pros and cons and what the best applications would be when these two technologies cross paths. This paper provides a survey of the research literature on the combination of blockchain and biometrics and includes a first legal analysis of this integration based on GDPR to shed light on challenges and potentials. Although the integration of blockchain technology into the biometric sector is still in its infancy, with a growing body of literature discussing specific applications and advanced technological setups, this paper aims to provide a holistic understanding of blockchain applicability in biometrics. Based on published studies, this article discusses, among others, practical examples combining blockchain and biometrics for novel applications in PKI systems, distributed trusted services, and identity management. Challenges and limitations when combining blockchain and biometrics that motivate future work will also be discussed; e.g., blockchain networks at their current stage may not be efficient or economical for some real-time biometric applications. Finally, we also discuss key legal aspects of the EU General Data Protection Regulation (GDPR) related to this combination of technologies (blockchain and biometrics); for example, accountability, immutability, anonymity, and data protection elements.</p><p><h4>cs.CV, cs.CR, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.22869'>Jenga: Responsive Tiered Memory Management without Thrashing</a></h3><h3><a href='https://arxiv.org/pdf/2510.22869' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>38/38</p><p><b>作者：</b>Rohan Kadekodi, Haoran Peng, Gilbert Bernstein, Michael D. Ernst, Baris Kasikci</p><p>A heterogeneous memory has a single address space with fast access to some addresses (a fast tier of DRAM) and slow access to other addresses (a capacity tier of CXL-attached memory or NVM). A tiered memory system aims to maximize the number of accesses to the fast tier via page migrations between the fast and capacity tiers. Unfortunately, previous tiered memory systems can perform poorly due to (1) allocating hot and cold objects in the same page and (2) abrupt changes in hotness measurements that lead to thrashing.  This paper presents Jenga, a tiered memory system that addresses both problems. Jenga&#x27;s memory allocator uses a novel context-based page allocation strategy. Jenga&#x27;s accurate measurements of page hotness enable it to react to memory access behavior changes in a timely manner while avoiding thrashing. Compared to the best previous tiered memory system, Jenga runs memory-intensive applications 28% faster across 10 applications, when the fast tier capacity matches the working set size, at a CPU overhead of &lt;3% of a single core and a memory overhead of &lt;0.3%</p><p><h4>cs.ET, cs.OS</h4></p></div><hr>
</body>
</html>
