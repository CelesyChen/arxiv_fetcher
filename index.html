<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-21</h1>
<div><h3><a href='https://arxiv.org/abs/2510.15878'>Putting the Context back into Memory</a></h3><h3><a href='https://arxiv.org/pdf/2510.15878' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/42</p><p><b>作者：</b>David A. Roberts</p><p>Requests arriving at main memory are often different from what programmers can observe or estimate by using CPU-based monitoring. Hardware cache prefetching, memory request scheduling and interleaving cause a loss of observability that limits potential data movement and tiering optimizations. In response, memory-side telemetry hardware like page access heat map units (HMU) and page prefetchers were proposed to inform Operating Systems with accurate usage data. However, it is still hard to map memory activity to software program functions and objects because of the decoupled nature of host processors and memory devices. Valuable program context is stripped out from the memory bus, leaving only commands, addresses and data. Programmers have expert knowledge of future data accesses, priorities, and access to processor state, which could be useful hints for runtime memory device optimization. This paper makes context visible at memory devices by encoding any user-visible state as detectable packets in the memory read address stream, in a nondestructive manner without significant capacity overhead, drivers or special access privileges. We prototyped an end-to-end system with metadata injection that can be reliably detected and decoded from a memory address trace, either by a host processor, or a memory module. We illustrate a use case with precise code execution markers and object address range tracking. In the future, real time metadata decoding with near-memory computing (NMC) could provide customized telemetry and statistics to users, or act on application hints to perform functions like prioritizing requests, remapping data and reconfiguring devices.</p><p><h4>cs.AR, cs.OS, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15912'>Latency Based Tiling</a></h3><h3><a href='https://arxiv.org/pdf/2510.15912' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/42</p><p><b>作者：</b>Jack Cashman</p><p>Latency Based Tiling provides a systems based approach to deriving approximate tiling solution that maximizes locality while maintaining a fast compile time. The method uses triangular loops to characterize miss ratio scaling of a machine avoiding prefetcher distortion. Miss ratio scaling captures the relationship between data access latency and working set size with sharp increases in latency indicating the data footprint exceeds capacity from a cache level. Through these noticeable increases in latency we can determine an approximate location for L1, L2, and L3 memory sizes. These sizes are expected to be under approximations of a systems true memory sizes which is in line with our expectations given the shared nature of cache in a multi process system as described in defensive loop tiling. Unlike auto tuning, which can be effective but prohibitively slow, Latency Based Tiling achieves negligible compile time overhead. The implementation in Rust enables a hardware agnostic approach which combined with a cache timing based techniques, yields a portable, memory safe system running wherever Rust is supported. The tiling strategy is applied to a subset of the polyhedral model, where loop nestings are tiled based on both the derived memory hierarchy and the observed data footprint per iteration.</p><p><h4>cs.PL, cs.AR, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15927'>UPMEM Unleashed: Software Secrets for Speed</a></h3><h3><a href='https://arxiv.org/pdf/2510.15927' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/42</p><p><b>作者：</b>Krystian Chmielewski, Jaros{\l}aw {\L}awnicki, Uladzislau Lukyanau, Tadeusz Kobus, Maciej Maciejewski</p><p>Developing kernels for Processing-In-Memory (PIM) platforms poses unique challenges in data management and parallel programming on limited processing units. Although software development kits (SDKs) for PIM, such as the UPMEM SDK, provide essential tools, these emerging platforms still leave significant room for performance optimization. In this paper, we reveal surprising inefficiencies in UPMEM software stack and play with non-standard programming techniques. By making simple modifications to the assembly generated by the UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x in integer multiplication, depending on the data type. We also demonstrate that bit-serial processing of low precision data is a viable option for UPMEM: in INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup over the baseline. Minor API extensions for PIM allocation that account for the non-uniform memory access (NUMA) architecture of the server further improve the consistency and throughput of host-PIM data transfers by up to 2.9x. Finally, we show that, when the matrix is preloaded into PIM, our optimized kernels outperform a dual-socket CPU server by over 3x for INT8 generalized matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized INT8 GEMV kernel outperforms the baseline 3.5x.</p><p><h4>cs.AR, cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16393'>Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades</a></h3><h3><a href='https://arxiv.org/pdf/2510.16393' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/42</p><p><b>作者：</b>Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Salvatore Trani</p><p>We investigate the exploitation of both lexical and neural relevance signals for ad-hoc passage retrieval. Our exploration involves a large-scale training dataset in which dense neural representations of MS-MARCO queries and passages are complemented and integrated with 253 hand-crafted lexical features extracted from the same corpus. Blending of the relevance signals from the two different groups of features is learned by a classical Learning-to-Rank (LTR) model based on a forest of decision trees. To evaluate our solution, we employ a pipelined architecture where a dense neural retriever serves as the first stage and performs a nearest-neighbor search over the neural representations of the documents. Our LTR model acts instead as the second stage that re-ranks the set of candidates retrieved by the first stage to enhance effectiveness. The results of reproducible experiments conducted with state-of-the-art dense retrievers on publicly available resources show that the proposed solution significantly enhances the end-to-end ranking performance while relatively minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of up to 11% with an increase in average query latency of only 4.3%. This confirms the advantage of seamlessly combining two distinct families of signals that mutually contribute to retrieval effectiveness.</p><p><h4>cs.IR, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.17158'>Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2510.17158' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/42</p><p><b>作者：</b>Daniel Nichols, Konstantinos Parasyris, Charles Jekel, Abhinav Bhatele, Harshitha Menon</p><p>Language models are now prevalent in software engineering with many developers using them to automate tasks and accelerate their development. While language models have been tremendous at accomplishing complex software engineering tasks, there are still many areas where they fail to deliver desirable results, for instance code performance related tasks. Tasks like optimization depend on many complex data from the environment, hardware, etc. that are not directly represented in source code. Recent efforts have seen large improvements in general code modeling tasks using chain-of-thought style reasoning, but these models still fail to comprehend how the environment interacts with code performance. In this paper we propose a methodology to train language models that can interact with performance tools during their reasoning process. We then demonstrate how this methodology can be used to train a state-of-the-art GPU kernel optimization model.</p><p><h4>cs.DC, cs.PF, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.17505'>Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums</a></h3><h3><a href='https://arxiv.org/pdf/2510.17505' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/42</p><p><b>作者：</b>Jaeyeon Won, Willow Ahrens, Joel S. Emer, Saman Amarasinghe</p><p>Programming high-performance sparse GPU kernels is notoriously difficult, requiring both substantial effort and deep expertise. Sparse compilers aim to simplify this process, but existing systems fall short in two key ways. First, they are primarily designed for CPUs and rarely produce high-performance GPU code. Second, when computations involve both sparse and dense regions, these compilers often fail to optimize the dense portions effectively. In this paper, we propose a new approach for expressing sparse computations. We start from format-agnostic Einsums over sparse tensors and rewrite them into format-conscious indirect Einsums, which explicitly encode format information by mapping sparse data and metadata onto dense tensor operations through indirect indexing. To execute indirect Einsums, we introduce the Insum compiler, which generates efficient GPU code for these Einsums by lowering to the PyTorch compiler, extended to better support Tensor Core-enabled indirect Einsums. We also present two fixed-length sparse formats, GroupCOO and BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach achieves 1.14x to 3.81x speedups across a range of sparse GPU applications while reducing lines of code by 202x to 4491x compared to hand-written implementations.</p><p><h4>cs.PL, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15872'>Multimodal Chip Physical Design Engineer Assistant</a></h3><h3><a href='https://arxiv.org/pdf/2510.15872' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/42</p><p><b>作者：</b>Yun-Da Tsai, Chang-Yu Chao, Liang-Yeh Shen, Tsung-Han Lin, Haoyu Yang, Mark Ho, Yi-Chen Lu, Wen-Hao Liu, Shou-De Lin, Haoxing Ren</p><p>Modern chip physical design relies heavily on Electronic Design Automation (EDA) tools, which often struggle to provide interpretable feedback or actionable guidance for improving routing congestion. In this work, we introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this gap by not only predicting congestion but also delivering human-interpretable design suggestions. Our method combines automated feature generation through MLLM-guided genetic prompting with an interpretable preference learning framework that models congestion-relevant tradeoffs across visual, tabular, and textual inputs. We compile these insights into a &quot;Design Suggestion Deck&quot; that surfaces the most influential layout features and proposes targeted optimizations. Experiments on the CircuitNet benchmark demonstrate that our approach outperforms existing models on both accuracy and explainability. Additionally, our design suggestion guidance case study and qualitative analyses confirm that the learned preferences align with real-world design principles and are actionable for engineers. This work highlights the potential of MLLMs as interactive assistants for interpretable and context-aware physical design optimization.</p><p><h4>cs.AR, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15880'>Opportunities and Challenges for 3D Systems and Their Design</a></h3><h3><a href='https://arxiv.org/pdf/2510.15880' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/42</p><p><b>作者：</b>Philip Emma, Eren Kurshan</p><p>Although it is not a new concept, 3D integration increasingly receives widespread interest and focus as lithographic scaling becomes more challenging, and as the ability to make miniature vias greatly improves. Like Moores law, 3D integration improves density. With improvements in packaging density, however, come the challenges associated with its inherently higher power density. And though it acts somewhat as a scaling accelerator, the vertical integration also poses new challenges to design and manufacturing technologies. The placement of circuits, vias, and macros in the planes of a 3D stack must be co-designed across layers (or must conform to new standards) so that, when assembled, they have correct spatial correspondence. Each layer, although perhaps being a mere functional slice through a system (and we can slice the system in many different ways), must be independently testable so that we can systematically test and diagnose subsystems before and after final assembly. When those layers are assembled, they must come together in a way that enables a sensible yield and facilitates testing the finished product. To make the most of 3D integration, we should articulate the leverages of 3D systems (other researchers offer a more complete treatment elsewhere). Then we can enumerate and elucidate many of the new challenges posed by the design, assembly, and test of 3D systems.</p><p><h4>cs.AR, cs.ET</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15902'>Fully Automated Verification Framework for Configurable IPs: From Requirements to Results</a></h3><h3><a href='https://arxiv.org/pdf/2510.15902' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/42</p><p><b>作者：</b>Shuhang Zhang, Jelena Radulovic, Thorsten Dworzak</p><p>The increasing competition in the semiconductor industry has created significant pressure to reduce chip prices while maintaining quality and reliability. Functional verification, particularly for configurable IPs, is a major contributor to development costs due to its complexity and resource-intensive nature. To address this, we propose a fully automated framework for requirements driven functional verification. The framework automates key processes, including vPlan generation, testbench creation, regression execution, and reporting in a requirements management tool, drastically reducing verification effort. This approach accelerates development cycles, minimizes human error, and enhances coverage, offering a scalable and efficient solution to the challenges of verifying configurable IPs.</p><p><h4>cs.AR, cs.ET</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15882'>FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern</a></h3><h3><a href='https://arxiv.org/pdf/2510.15882' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/42</p><p><b>作者：</b>Ao Shen, Rui Zhang, Junping Zhao</p><p>As large language models (LLMs) continue to scale, multi-node deployment has become a necessity. Consequently, communication has become a critical performance bottleneck. Current intra-node communication libraries, like NCCL, typically make use of a single interconnect such as NVLink. This approach creates performance ceilings, especially on hardware like the H800 GPU where the primary interconnect&#x27;s bandwidth can become a bottleneck, and leaves other hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable Network Interface Cards (NICs) largely idle during intensive workloads. We propose FlexLink, the first collective communication framework to the best of our knowledge designed to systematically address this by aggregating these heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance communication fabric. FlexLink employs an effective two-stage adaptive load balancing strategy that dynamically partitions communication traffic across all available links, ensuring that faster interconnects are not throttled by slower ones. On an 8-GPU H800 server, our design improves the bandwidth of collective operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL baseline, respectively. This gain is achieved by offloading 2-22% of the total communication traffic to the previously underutilized PCIe and RDMA NICs. FlexLink provides these improvements as a lossless, drop-in replacement compatible with the NCCL API, ensuring easy adoption.</p><p><h4>cs.AR, cs.AI, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15893'>Accelerating Frontier MoE Training with 3D Integrated Optics</a></h3><h3><a href='https://arxiv.org/pdf/2510.15893' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/42</p><p><b>作者：</b>Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh</p><p>The unabated growth in AI workload demands is driving the need for concerted advances in compute, memory, and interconnect performance. As traditional semiconductor scaling slows, high-speed interconnects have emerged as the new scaling engine, enabling the creation of larger logical GPUs by linking many GPUs into a single, low-latency, high-bandwidth compute domain. While initial scale-up fabrics leveraged copper interconnects for their power and cost advantages, the maximum reach of passive electrical interconnects (approximately 1 meter) effectively limits the scale-up domain to within a single rack. The advent of 3D-stacked optics and logic offers a transformative, power-efficient scale-up solution for connecting hundreds of GPU packages (thousands of GPUs) across multiple data center racks. This work explores the design tradeoffs of scale-up technologies and demonstrates how frontier LLMs necessitate novel photonic solutions to achieve aggressive power and performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and switches within the scale-up domain when training Frontier Mixture of Experts (MoE) models exceeding one trillion parameters. Our results show that the substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X increase in scale-up capability. This affords new opportunities for multi-dimensional parallelism within the scale-up domain and results in a 2.7X reduction in time-to-train, unlocking unprecedented model scaling.</p><p><h4>cs.AR, cs.AI, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15884'>Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I</a></h3><h3><a href='https://arxiv.org/pdf/2510.15884' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/42</p><p><b>作者：</b>Faizan A Khattak, Mantas Mikaitis</p><p>Numerical features of matrix multiplier hardware units in NVIDIA and AMD data centre GPUs have recently been studied. Features such as rounding, normalisation, and internal precision of the accumulators are of interest. In this paper, we extend the methodology for analysing those features, to consumer-grade NVIDIA GPUs by implementing an architecture-independent test scheme for various input and output precision formats. Unlike current approaches, the proposed test vector generation method neither performs an exhaustive search nor relies on hard-coded {constants that are device-specific, yet remains applicable to a wide range of mixed-precision formats. We have applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada Lovelace architecture) graphics cards and determined numerical features of matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating point formats and binary16 and binary32 IEEE 754 output formats. Our methodology allowed us to determine that} the numerical features of RTX-3060, a consumer-grade GPU, are identical to those of the A100, a data centre GPU. We do not expect our code to require any changes for performing analysis of matrix multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future successors, and any input/output format combination, including the latest 8-bit floating-point formats.</p><p><h4>cs.AR, cs.MS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15885'>ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices</a></h3><h3><a href='https://arxiv.org/pdf/2510.15885' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/42</p><p><b>作者：</b>Dingcui Yu, Zonghuan Yan, Jialin Liu, Yumiao Zhao, Yanyun Wang, Xinghui Duan, Yina Lv, Liang Shi</p><p>To facilitate the understanding and efficient enhancement of software and hardware design for consumer-grade zoned flash storage, ConZone is proposed as the first emulator designed to model the resource constraints and architectural features typical of such systems. It incorporates essential components commonly deployed in consumer-grade devices, including limited logical to physical mapping caches, constrained write buffers, and hybrid flash media management. However, ConZone cannot be mounted with the file system due to the lack of in-place update capability, which is required by the metadata area of F2FS. To improve the usability of the emulator, ConZone+ extends ConZone with support for a block interface. We also provide a script to help the deployment and introduces several enhancements over the original version. Users can explore the internal architecture of consumer-grade zoned flash storage and integrate their optimizations with system software using ConZone+. We validate the accuracy of ConZone+ by comparing a hardware architecture representative of consumer-grade zoned flash storage and comparing it with the state-of-the-art. In addition, we conduct several case studies using ConZone+ to investigate the design of zoned storage and explore the inadequacies of the current file system.</p><p><h4>cs.AR, cs.OS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15887'>basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I</a></h3><h3><a href='https://arxiv.org/pdf/2510.15887' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/42</p><p><b>作者：</b>Hyun Woo Kang, Ji Woong Choi</p><p>This paper introduces BASIC_RV32s, an open-source framework providing a practical microarchitectural roadmap for the RISC-V RV32I architecture, addressing the gap between theoretical knowledge and hardware implementation. Following the classic Patterson and Hennessy methodology, the design evolves from a basic single-cycle core to a 5-stage pipelined core design with full hazard forwarding, dynamic branch prediction, and exception handling. For verification, the final core design is integrated into a System-on-Chip (SoC) with Universal Asynchronous Receiver-Transmitter (UART) communication implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving 1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50 MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level logic block diagrams, and development logs under MIT license on GitHub, BASIC_RV32s offers a reproducible instructional pathway for the open-source hardware ecosystem.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15888'>Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol</a></h3><h3><a href='https://arxiv.org/pdf/2510.15888' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/42</p><p><b>作者：</b>Konstantinos Kafousis</p><p>Hardware Transactional Memory (HTM) allows lock-free programming as easy as with traditional coarse-grain locks or similar, while benefiting from the performance advantages of fine-grained locking. Many HTM implementations have been proposed, but they have not received widespread adoption because of their high hardware complexity, their need for additions to the Instruction Set Architecture (ISA), and often for modifications to the cache coherence protocol.  We show that HTM can be implemented without adding new instructions -- merely by extending the semantics of two existing, Load-Linked and Store-Conditional. Also, our proposed design does not modify or extend standard coherence protocols. We further propose to drastically simplify the implementation of HTM -- confined to modifications in the L1 Data Cache only -- by restricting it to applications where the write set plus the read set of each transaction do not exceed a small number of cache lines. We also propose two alternative mechanisms to guarantee forward progress, both based on detecting retrial attempts.  We simulated our proposed design in Gem5, and we used it to implement several popular concurrent data structures, showing that a maximum of eight (8) words (cache lines) suffice for the write plus read sets. We provide a detailed explanation of selected implementations, clarifying the intended usage of our HTM from a programmer&#x27;s perspective. We evaluated our HTM under varying contention levels to explore its scalability limits. The results indicate that our HTM provides good performance in concurrent data structures when contention is spread across multiple nodes: in such cases, the percentage of aborts relative to successful commits is very low. In the atomic fetch-and-increment benchmark for multiple shared counters, the results show that, under low-congestion, our HTM improves performance relative to the TTS lock.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15897'>DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms</a></h3><h3><a href='https://arxiv.org/pdf/2510.15897' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/42</p><p><b>作者：</b>Kien Le Trung, Truong-Son Hy</p><p>Chip placement, the task of determining optimal positions of circuit modules on a chip canvas, is a critical step in the VLSI design flow that directly impacts performance, power consumption, and routability. Traditional methods rely on analytical optimization or reinforcement learning, which struggle with hard placement constraints or require expensive online training for each new circuit design. To address these limitations, we introduce DiffPlace, a framework that formulates chip placement as a conditional denoising diffusion process, enabling transferable placement policies that generalize to unseen circuit netlists without retraining. DiffPlace leverages the generative capabilities of diffusion models to efficiently explore the vast space of placement while conditioning on circuit connectivity and relative quality metrics to identify optimal solutions globally. Our approach combines energy-guided sampling with constrained manifold diffusion to ensure placement legality, achieving extremely low overlap across all experimental scenarios. Our method bridges the gap between optimization-based and learning-based approaches, offering a practical path toward automated, high-quality chip placement for modern VLSI design. Our source code is publicly available at: https://github.com/HySonLab/DiffPlace/</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15907'>Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions</a></h3><h3><a href='https://arxiv.org/pdf/2510.15907' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/42</p><p><b>作者：</b>Era Thaqi, Dennis Eigner, Arman Ferdowsi, Ulrich Schmid</p><p>We propose a novel approach to symbolic timing analysis for digital integrated circuits based on recently developed analytic delay formulas for 2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a fixed order of the transitions of all input and internal signals of a circuit, our framework computes closed-form analytic delay expressions for all the internal signal transition times that depend on (i) the symbolic transition times of the relevant input signals and (ii) the model parameters of the relevant gates. The resulting formulas facilitate per-transition timing analysis without any simulation, by instantiating the symbolic input transition times and the gate parameters. More importantly, however, they also enable an \emph{analytic} study of the dependencies of certain timing properties on input signals and gate parameters. For instance, differentiating a symbolic delay expression with respect to a gate parameter or input transition time enables sensitivity analysis. As a proof of concept, we implement our approach using the computer algebra system SageMath and apply it to the NOR-gate version of the c17 slack benchmark circuit.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15908'>Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations</a></h3><h3><a href='https://arxiv.org/pdf/2510.15908' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/42</p><p><b>作者：</b>Hana Chitsaz, Johnson Umeike, Amirmahdi Namjoo, Babak N. Safa, Bahar Asgari</p><p>Finite element simulations are essential in biomechanics, enabling detailed modeling of tissues and organs. However, architectural inefficiencies in current hardware and software stacks limit performance and scalability, especially for iterative tasks like material parameter identification. As a result, workflows often sacrifice fidelity for tractability. Reconfigurable hardware, such as FPGAs, offers a promising path to domain-specific acceleration without the cost of ASICs, but its potential in biomechanics remains underexplored. This paper presents Belenos, a comprehensive workload characterization of finite element biomechanics using FEBio, a widely adopted simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal that smaller workloads experience moderate front-end stalls, typically around 13.1%, whereas larger workloads are dominated by significant back-end bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%. Complementary gem5 sensitivity studies identify optimal hardware configurations for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline, memory, or branch predictor settings can degrade performance by up to 37.1%. These findings underscore the need for architecture-aware co-design to efficiently support biomechanical simulation workloads.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15899'>LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models</a></h3><h3><a href='https://arxiv.org/pdf/2510.15899' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/42</p><p><b>作者：</b>Kiran Thorat, Jiahui Zhao, Yaotian Liu, Amit Hasan, Hongwu Peng, Xi Xie, Bin Lei, Caiwen Ding</p><p>Large Language Models (LLMs) are gaining prominence in various fields, thanks to their ability to generate high- quality content from human instructions. This paper delves into the field of chip design using LLMs, specifically in Power- Performance-Area (PPA) optimization and the generation of accurate Verilog codes for circuit designs. We introduce a novel framework VeriPPA designed to optimize PPA and generate Verilog code using LLMs. Our method includes a two-stage process where the first stage focuses on improving the functional and syntactic correctness of the generated Verilog codes, while the second stage focuses on optimizing the Verilog codes to meet PPA constraints of circuit designs, a crucial element of chip design. Our framework achieves an 81.37% success rate in syntactic correctness and 62.06% in functional correctness for code genera- tion, outperforming current state-of-the-art (SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework achieves 99.56% syntactic correctness and 43.79% functional correctness, also surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57% for functional correctness. Furthermore, Our framework able to optimize the PPA of the designs. These results highlight the potential of LLMs in handling complex technical areas and indicate an encouraging development in the automation of chip design processes.</p><p><h4>cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15926'>TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs</a></h3><h3><a href='https://arxiv.org/pdf/2510.15926' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/42</p><p><b>作者：</b>Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang</p><p>With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need. However, this is challenging because of their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as low as 1.58~bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected long latency of the prefill stage. We present \textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for low-power edge FPGAs that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates several novel techniques, including (1) a table-lookup-based ternary matrix multiplication (TLMM) engine utilizing grouped activations and online precomputation for low resource utilization and high throughput; (2) a fine-grained analytic URAM-based weight buffer management scheme for efficient loading and compute engine access; (3) a streaming dataflow architecture that fuses floating-point element-wise operations with linear computations to hide latency; (4) a reversed-reordered prefill stage attention with fused attention operations for high memory efficiency; and (5) a resource-efficient specialized decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to 25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for 64--128 token prompts, marking a significant energy-efficiency advancement in LLM inference on edge FPGAs.</p><p><h4>cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15904'>NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme</a></h3><h3><a href='https://arxiv.org/pdf/2510.15904' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/42</p><p><b>作者：</b>Subhradip Chakraborty, Ankur Singh, Xuming Chen, Gourav Datta, Akhilesh R. Jaiswal</p><p>The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.27%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.</p><p><h4>cs.AR, cs.SY, eess.IV, eess.SY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15906'>FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures</a></h3><h3><a href='https://arxiv.org/pdf/2510.15906' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/42</p><p><b>作者：</b>Yunsheng Bai, Ghaith Bany Hamad, Chia-Tung Ho, Syed Suhaib, Haoxing Ren</p><p>Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug&#x27;s applicability from academic benchmarks to industrial designs.</p><p><h4>cs.AR, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15910'>SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs</a></h3><h3><a href='https://arxiv.org/pdf/2510.15910' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/42</p><p><b>作者：</b>Marvin Fuchs, Lukas Scheller, Timo Muscheid, Oliver Sander, Luis E. Ardila-Perez</p><p>Modern heterogeneous System-on-Chip (SoC) devices integrate advanced components into a single package, offering powerful capabilities while also introducing significant complexity. To manage these sophisticated devices, firmware and software developers need powerful development tools. However, as these tools become increasingly complex, they often lack adequate support, resulting in a steep learning curve and challenging troubleshooting. To address this, this work introduces System-on-Chip blocks (SoCks), a flexible and expandable build framework that reduces complexity by partitioning the SoC image into high-level units called blocks. SoCks builds each firmware and software block in an encapsulated way, independently from other components of the image, thereby reducing dependencies to a minimum. While some information exchange between the blocks is unavoidable to ensure seamless runtime integration, this interaction is standardized via interfaces. A small number of dependencies and well-defined interfaces simplify the reuse of existing block implementations and facilitate seamless substitution between versions-for instance, when choosing root file systems for the embedded Linux operating system. Additionally, this approach facilitates the establishment of a decentralized and partially automated development flow through Continuous Integration and Continuous Delivery (CI/CD). Measurement results demonstrate that SoCks can build a complete SoC image up to three times faster than established tools.</p><p><h4>cs.AR, hep-ex</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15914'>VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts</a></h3><h3><a href='https://arxiv.org/pdf/2510.15914' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/42</p><p><b>作者：</b>Jiayu Zhao, Song Chen</p><p>Large language models (LLMs) have demonstrated strong capabilities in generating Verilog code from natural language descriptions. However, Verilog code inherently encodes structural information of hardware circuits. Effectively leveraging this structural information to enhance the functional and syntactic correctness of LLM-generated Verilog code remains a significant challenge. To address this challenge, we propose VeriGRAG , a novel framework that extracts structural graph embeddings from Verilog code using graph neural networks (GNNs). A multimodal retriever then selects the graph embeddings most relevant to the given generation task, which are aligned with the code modality through the VeriFormer module to generate structure-aware soft prompts. Our experiments demonstrate that VeriGRAG substantially improves the correctness of Verilog code generation, achieving state-of-the-art or superior performance across both VerilogEval and RTLLM benchmarks.</p><p><h4>cs.AR, cs.AI, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.15917'>Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding</a></h3><h3><a href='https://arxiv.org/pdf/2510.15917' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/42</p><p><b>作者：</b>Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang</p><p>Existing storage systems lack visibility into workload intent, limiting their ability to adapt to the semantics of modern, large-scale data-intensive applications. This disconnect leads to brittle heuristics and fragmented, siloed optimizations. To address these limitations, we propose Intent-Driven Storage Systems (IDSS), a vision for a new paradigm where large language models (LLMs) infer workload and system intent from unstructured signals to guide adaptive and cross-layer parameter reconfiguration. IDSS provides holistic reasoning for competing demands, synthesizing safe and efficient decisions within policy guardrails. We present four design principles for integrating LLMs into storage control loops and propose a corresponding system architecture. Initial results on FileBench workloads show that IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components such as caching and prefetching. These findings suggest that, when constrained by guardrails and embedded within structured workflows, LLMs can function as high-level semantic optimizers, bridging the gap between application goals and low-level system control. IDSS points toward a future in which storage systems are increasingly adaptive, autonomous, and aligned with dynamic workload demands.</p><p><h4>cs.AR, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16284'>Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI</a></h3><h3><a href='https://arxiv.org/pdf/2510.16284' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/42</p><p><b>作者：</b>Di Zhang</p><p>Bootstrapping is a powerful statistical resampling technique for estimating the sampling distribution of an estimator. However, its computational cost becomes prohibitive for large datasets or a high number of resamples. This paper presents a theoretical analysis and design of parallel bootstrapping algorithms using the Message Passing Interface (MPI). We address two key challenges: high communication overhead and memory constraints in distributed environments. We propose two novel strategies: 1) Local Statistic Aggregation, which drastically reduces communication by transmitting sufficient statistics instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number Generation, which enables distributed resampling when the entire dataset cannot be stored on a single process. We develop analytical models for communication and computation complexity, comparing our methods against naive baseline approaches. Our analysis demonstrates that the proposed methods offer significant reductions in communication volume and memory usage, facilitating scalable parallel bootstrapping on large-scale systems.</p><p><h4>cs.DC, cs.MS, cs.NA, math.NA, stat.CO</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16415'>MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2510.16415' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/42</p><p><b>作者：</b>Rizhen Hu, Yutong He, Ran Yan, Mou Sun, Binghang Yuan, Kun Yuan</p><p>As distributed optimization scales to meet the demands of Large Language Model (LLM) training, hardware failures become increasingly non-negligible. Existing fault-tolerant training methods often introduce significant computational or memory overhead, demanding additional resources. To address this challenge, we propose Memory- and Computation-efficient Fault-tolerant Optimization (MeCeFO), a novel algorithm that ensures robust training with minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its training task to a neighboring node while employing memory- and computation-efficient algorithmic optimizations to minimize the extra workload imposed on the neighboring node handling both tasks. MeCeFO leverages three key algorithmic designs: (i) Skip-connection, which drops the multi-head attention (MHA) module during backpropagation for memory- and computation-efficient approximation; (ii) Recomputation, which reduces activation memory in feedforward networks (FFNs); and (iii) Low-rank gradient approximation, enabling efficient estimation of FFN weight matrix gradients. Theoretically, MeCeFO matches the convergence rate of conventional distributed training, with a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and T is the number of iterations. Empirically, MeCeFO maintains robust performance under high failure rates, incurring only a 4.18% drop in throughput, demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA approaches. Codes are available at https://github.com/pkumelon/MeCeFO.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16418'>FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference</a></h3><h3><a href='https://arxiv.org/pdf/2510.16418' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/42</p><p><b>作者：</b>Jian Ma, Xinchen Lyu, Jun Jiang, Longhao Zou, Chenshan Ren, Qimei Cui, Xiaofeng Tao</p><p>Collaborative large language model (LLM) inference enables real-time, privacy-preserving AI services on resource-constrained edge devices by partitioning computational workloads between client devices and edge servers. However, this paradigm is severely hindered by communication bottlenecks caused by the transmission of high-dimensional intermediate activations, exacerbated by the autoregressive decoding structure of LLMs, where bandwidth consumption scales linearly with output length. Existing activation compression methods struggle to simultaneously achieve high compression ratios, low reconstruction error, and computational efficiency. This paper proposes FourierCompress, a novel, layer-aware activation compression framework that exploits the frequency-domain sparsity of LLM activations. We rigorously demonstrate that activations from the first Transformer layer exhibit strong smoothness and energy concentration in the low-frequency domain, making them highly amenable to near-lossless compression via the Fast Fourier Transform (FFT). FourierCompress transforms activations into the frequency domain, retains only a compact block of low-frequency coefficients, and reconstructs the signal at the server using conjugate symmetry, enabling seamless hardware acceleration on DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10 commonsense reasoning datasets demonstrate that FourierCompress preserves performance remarkably close to the uncompressed baseline, outperforming Top-k, QR, and SVD. FourierCompress bridges the gap between communication efficiency (an average 7.6x reduction in activation size), near-lossless inference (less than 0.3% average accuracy loss), and significantly faster compression (achieving over 32x reduction in compression time compared to Top-k via hardware acceleration) for edge-device LLM inference.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16890'>Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++</a></h3><h3><a href='https://arxiv.org/pdf/2510.16890' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/42</p><p><b>作者：</b>Ji\v{r}\&#x27;i Klepl, Martin Kruli\v{s}, Maty\&#x27;a\v{s} Brabec</p><p>Message Passing Interface (MPI) has been a well-established technology in the domain of distributed high-performance computing for several decades. However, one of its greatest drawbacks is a rather ancient pure-C interface. It lacks many useful features of modern languages (namely C++), like basic type-checking or support for generic code design. In this paper, we propose a novel abstraction for MPI, which we implemented as an extension of the C++ Noarr library. It follows Noarr paradigms (first-class layout and traversal abstraction) and offers layout-agnostic design of MPI applications. We also implemented a layout-agnostic distributed GEMM kernel as a case study to demonstrate the usability and syntax of the proposed abstraction. We show that the abstraction achieves performance comparable to the state-of-the-art MPI C++ bindings while allowing for a more flexible design of distributed applications.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16896'>FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.16896' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/42</p><p><b>作者：</b>Yiming Hu</p><p>Two-Phase Triple Modular Redundancy TMR divides redundancy operations into two stages, omitting part of the computation during fault-free operation to reduce energy consumption. However, it becomes ineffective under permanent faults, limiting its reliability in critical systems. To address this, Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty cores, tolerating both transient and permanent faults. Yet, its reliance on additional hardware increases system complexity and reduces fault tolerance when multiple cores or auxiliary modules fail. This paper proposes an integrated fault-tolerant architecture for interconnected multicore systems. By constructing a stability metric to identify reliable machines and performing periodic diagnostics, the method enables permanent fault isolation and adaptive task scheduling without extra hardware. Experimental results show that it reduces task workload by approximately 30% compared to baseline TMR and achieves superior fault coverage and isolation accuracy, significantly improving both reliability and energy efficiency.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16946'>Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure</a></h3><h3><a href='https://arxiv.org/pdf/2510.16946' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>31/42</p><p><b>作者：</b>Erfan Darzi, Aldo Pareja, Shreeanant Bharadwaj</p><p>Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is critical for maintaining performance predictability and resource utilization, yet existing monitoring tools lack the granularity for root cause analysis in shared computing environments. We introduce an eBPF-based telemetry system that provides unified host-side monitoring of GPU workloads, correlating eBPF-derived host metrics with GPU-internal events for holistic system observability. The system achieves 81--88\% diagnostic accuracy, detects spikes within 5 seconds, and completes root cause analysis in 6--8 seconds, operating with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning workloads, the system identifies root causes including NIC contention, PCIe pressure, and CPU interference, enabling operational debugging for multi-tenant GPU infrastructure without requiring cluster-wide instrumentation.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.17639'>On the Universality of Round Elimination Fixed Points</a></h3><h3><a href='https://arxiv.org/pdf/2510.17639' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>32/42</p><p><b>作者：</b>Alkida Balliu, Sebastian Brandt, Ole Gabsdil, Dennis Olivetti, Jukka Suomela</p><p>Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC 2020] has drawn attention to the following open question: are round elimination fixed points a universal technique for proving lower bounds? That is, given a locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds in the deterministic LOCAL model, can we always find a relaxation $\Pi&#x27;$ of $\Pi$ that is a nontrivial fixed point for the round elimination technique [see STOC 2016, PODC 2019]? If yes, then a key part of distributed computational complexity would be also decidable.  The key obstacle so far has been a certain family of homomorphism problems [ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is based on Marks&#x27; technique [J.AMS 2016].  We develop a new technique for constructing round elimination lower bounds systematically. Using so-called tripotent inputs we show that the aforementioned homomorphism problems indeed admit a lower bound proof that is based on round elimination fixed points. Hence we eliminate the only known obstacle for the universality of round elimination.  Yet we also present a new obstacle: we show that there are some problems with inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is based on relaxations to nontrivial round elimination fixed points. Hence round elimination cannot be a universal technique for problems with inputs (but it might be universal for problems without inputs).  We also prove the first fully general lower bound theorem that is applicable to any problem, with or without inputs, that is a fixed point in round elimination. Prior results of this form were only able to handle certain very restricted inputs.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16497'>Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages</a></h3><h3><a href='https://arxiv.org/pdf/2510.16497' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>33/42</p><p><b>作者：</b>Pacome Simon Mbonimpa, Diane Tuyizere, Azizuddin Ahmed Biyabani, Ozan K. Tonguz</p><p>This paper presents a novel framework for speech transcription and synthesis, leveraging edge-cloud parallelism to enhance processing speed and accessibility for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful language processing tools for these widely spoken languages in East African countries with limited technological infrastructure. The framework utilizes the Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and text-to-speech (TTS) translation. The architecture uses a cascading mechanism that distributes the model inference workload between the edge device and the cloud, thereby reducing latency and resource usage, benefiting both ends. On the edge device, our approach achieves a memory usage compression of 9.5% for the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with a 1 MB/s network bandwidth, the system can process a 270-character text in less than a minute for both speech-to-text and text-to-speech transcription. Using real-world survey data from Kenya, it is shown that the cascaded edge-cloud architecture proposed could easily serve as an excellent platform for STT and TTS transcription with good accuracy and response time.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16606'>Reimagining RDMA Through the Lens of ML</a></h3><h3><a href='https://arxiv.org/pdf/2510.16606' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>34/42</p><p><b>作者：</b>Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz</p><p>As distributed machine learning (ML) workloads scale to thousands of GPUs connected by ultra-high-speed inter-connects, tail latency in collective communication has emerged as a primary bottleneck. Prior RDMA designs, like RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While effective for general-purpose workloads, these mechanisms introduce complexity and latency that scale poorly, where even rare packet losses or delays can consistently degrade system performance. We introduce Celeris, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML&#x27;s tolerance for lost or partial data. Celeris removes retransmissions and in-order delivery from the RDMA NIC, enabling best-effort transport that exploits the robustness of ML workloads. It retains congestion control (e.g., DCQCN) and manages communication with software-level mechanisms such as adaptive timeouts and data prioritization, while shifting loss recovery to the ML pipeline (e.g., using the Hadamard Transform). Early results show that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by 67%, and nearly doubles NIC resilience to faults -- delivering a resilient, scalable transport tailored for ML at cluster scale.</p><p><h4>cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16933'>Tutoring LLM into a Better CUDA Optimizer</a></h3><h3><a href='https://arxiv.org/pdf/2510.16933' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>35/42</p><p><b>作者：</b>Maty\&#x27;a\v{s} Brabec, Ji\v{r}\&#x27;i Klepl, Michal T\&quot;opfer, Martin Kruli\v{s}</p><p>Recent leaps in large language models (LLMs) caused a revolution in programming tools (like GitHub Copilot) that can help with code generation, debugging, and even performance optimization. In this paper, we focus on the capabilities of the most recent reasoning models to generate optimized CUDA code for predefined, well-known tasks. Our objective is to determine which types of code optimizations and parallel patterns the LLMs can perform by themselves and whether they can be improved by tutoring (providing more detailed hints and guidelines in the prompt). The generated solutions were evaluated both automatically (for correctness and speedup) and manually (code reviews) to provide a more detailed perspective. We also tried an interactive approach where the LLM can fix its previous mistakes within a session. The results indicate that LLMs are quite skilled coders; however, they require tutoring to reach optimized solutions provided by parallel computing experts.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16024'>On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation</a></h3><h3><a href='https://arxiv.org/pdf/2510.16024' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>36/42</p><p><b>作者：</b>Abdulrahman Alhaidari, Balaji Palanisamy, Prashant Krishnamurthy</p><p>Billions of dollars are lost every year in DeFi platforms by transactions exploiting business logic or accounting vulnerabilities. Existing defenses focus on static code analysis, public mempool screening, attacker contract detection, or trusted off-chain monitors, none of which prevents exploits submitted through private relays or malicious contracts that execute within the same block. We present the first decentralized, fully on-chain learning framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce cost, (ii) propagates verified model updates to Layer-1, and (iii) enables gas-bounded, low-latency inference inside smart contracts. A novel Proof-of-Improvement (PoIm) protocol governs the training process and verifies each decentralized micro update as a self-verifying training transaction. Updates are accepted by \textit{PoIm} only if they demonstrably improve at least one core metric (e.g., accuracy, F1-score, precision, or recall) on a public benchmark without degrading any of the other core metrics, while adversarial proposals get financially penalized through an adaptable test set for evolving threats. We develop quantization and loop-unrolling techniques that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs (with support for formally verified decision tree inference) within the Ethereum block gas limit, while remaining bit-exact to their off-chain counterparts, formally proven in Z3. We curate 298 unique real-world exploits (2020 - 2025) with 402 exploit transactions across eight EVM chains, collectively responsible for \$3.74 B in losses.</p><p><h4>cs.CR, cs.AI, cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16067'>A Multi-Cloud Framework for Zero-Trust Workload Authentication</a></h3><h3><a href='https://arxiv.org/pdf/2510.16067' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>37/42</p><p><b>作者：</b>Saurabh Deochake, Ryan Murphy, Jeremiah Gearheart</p><p>Static, long-lived credentials for workload authentication create untenable security risks that violate Zero-Trust principles. This paper presents a multi-cloud framework using Workload Identity Federation (WIF) and OpenID Connect (OIDC) for secretless authentication. Our approach uses cryptographically-verified, ephemeral tokens, allowing workloads to authenticate without persistent private keys and mitigating credential theft. We validate this framework in an enterprise-scale Kubernetes environment, which significantly reduces the attack surface. The model offers a unified solution to manage workload identities across disparate clouds, enabling future implementation of robust, attribute-based access control.</p><p><h4>cs.CR, cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16087'>Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments</a></h3><h3><a href='https://arxiv.org/pdf/2510.16087' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>38/42</p><p><b>作者：</b>Sabbir M Saleh, Nazim Madhavji, John Steinbacher</p><p>Security is becoming a pivotal point in cloud platforms. Several divisions, such as business organisations, health care, government, etc., have experienced cyber-attacks on their infrastructures. This research focuses on security issues within Continuous Integration and Deployment (CI/CD) pipelines in a cloud platform as a reaction to recent cyber breaches. This research proposes a blockchain-based solution to enhance CI/CD pipeline security. This research aims to develop a framework that leverages blockchain&#x27;s distributed ledger technology and tamper-resistant features to improve CI/CD pipeline security. The goal is to emphasise secure software deployment by integrating threat modelling frameworks and adherence to coding standards. It also aims to employ tools to automate security testing to detect publicly disclosed vulnerabilities and flaws, such as an outdated version of Java Spring Framework, a JavaScript library from an unverified source, or a database library that allows SQL injection attacks in the deployed software through the framework.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16694'>CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning</a></h3><h3><a href='https://arxiv.org/pdf/2510.16694' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>39/42</p><p><b>作者：</b>Anthony DiMaggio, Raghav Sharma, Gururaj Saileshwar</p><p>Secure federated learning (FL) preserves data privacy during distributed model training. However, deploying such frameworks across heterogeneous devices results in performance bottlenecks, due to straggler clients with limited computational or network capabilities, slowing training for all participating clients. This paper introduces the first straggler mitigation technique for secure aggregation with deep neural networks. We propose CLIP, a client-side invariant neuron pruning technique coupled with network-aware pruning, that addresses compute and network bottlenecks due to stragglers during training with minimal accuracy loss. Our technique accelerates secure FL training by 13% to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an accuracy impact of between 1.3% improvement to 2.6% reduction.</p><p><h4>cs.LG, cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.16736'>Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices</a></h3><h3><a href='https://arxiv.org/pdf/2510.16736' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>40/42</p><p><b>作者：</b>Patrizio Dazzi, William Guglielmo, Franco Maria Nardini, Raffaele Perego, Salvatore Trani</p><p>This paper investigates the usage of FPGA devices for energy-efficient exact kNN search in high-dimension latent spaces. This work intercepts a relevant trend that tries to support the increasing popularity of learned representations based on neural encoder models by making their large-scale adoption greener and more inclusive. The paper proposes two different energy-efficient solutions adopting the same FPGA low-level configuration. The first solution maximizes system throughput by processing the queries of a batch in parallel over a streamed dataset not fitting into the FPGA memory. The second minimizes latency by processing each kNN incoming query in parallel over an in-memory dataset. Reproducible experiments on publicly available image and text datasets show that our solution outperforms state-of-the-art CPU-based competitors regarding throughput, latency, and energy consumption. Specifically, experiments show that the proposed FPGA solutions achieve the best throughput in terms of queries per second and the best-observed latency with scale-up factors of up to 16.6X. Similar considerations can be made regarding energy efficiency, where results show that our solutions can achieve up to 11.9X energy saving w.r.t. strong CPU-based competitors.</p><p><h4>cs.IR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2504.17984'>Proto: A Guided Journey through Modern OS Construction</a></h3><h3><a href='https://arxiv.org/pdf/2504.17984' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>41/42</p><p><b>作者：</b>Wonkyo Choe, Rongxiang Wang, Afsara Benazir, Felix Xiaozhu Lin</p><p>Proto is a new instructional OS that runs on commodity, portable hardware. It showcases modern features, including per-app address spaces, threading, commodity filesystems, USB, DMA, multicore support, self-hosted debugging, and a window manager. It supports rich applications such as 2D/3D games, music and video players, and a blockchain miner. Unlike traditional instructional systems, Proto emphasizes engaging, media-rich apps that go beyond basic terminal programs. Our method breaks down a full-featured OS into a set of incremental, self-contained prototypes. Each prototype introduces a minimal set of OS mechanisms, driven by the needs of specific apps. The construction process then progressively enables these apps by bringing up one mechanism at a time. Proto enables a wider audience to experience building a self-contained software system used in daily life</p><p><h4>cs.OS, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2408.04898'>Object as a Service: Simplifying Cloud-Native Development through Serverless Object Abstraction</a></h3><h3><a href='https://arxiv.org/pdf/2408.04898' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>42/42</p><p><b>作者：</b>Pawissanutt Lertpongrujikorn, Mohsen Amini Salehi</p><p>The function-as-a-service (FaaS) paradigm is envisioned as the next generation of cloud computing systems that mitigate the burden for cloud-native application developers by abstracting them from cloud resource management. However, it does not deal with the application data aspects. As such, developers have to intervene and undergo the burden of managing the application data, often via separate cloud storage services. To further streamline cloud-native application development, in this work, we propose a new paradigm, known as Object as a Service (OaaS) that encapsulates application data and functions into the cloud object abstraction. OaaS relieves developers from resource and data management burden while offering built-in optimization features. Inspired by OOP, OaaS incorporates access modifiers and inheritance into the serverless paradigm that: (a) prevents developers from compromising the system via accidentally accessing underlying data; and (b) enables software reuse in cloud-native application development. Furthermore, OaaS natively supports dataflow semantics. It enables developers to define function workflows while transparently handling data navigation, synchronization, and parallelism issues. To establish the OaaS paradigm, we develop a platform named Oparaca that offers state abstraction for structured and unstructured data with consistency and fault-tolerant guarantees. We evaluated Oparaca under real-world settings against state-of-the-art platforms with respect to the imposed overhead, scalability, and ease of use. The results demonstrate that the object abstraction provided by OaaS can streamline flexible and scalable cloud-native application development with an insignificant overhead on the underlying serverless system.</p><p><h4>cs.DC, cs.OS, cs.SE</h4></p></div><hr>
</body>
</html>
