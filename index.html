<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-11-07</h1>
<div><h3><a href='https://arxiv.org/abs/2511.03586'>PerfDojo: Automated ML Library Generation for Heterogeneous Architectures</a></h3><h3><a href='https://arxiv.org/pdf/2511.03586' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/24</p><p><b>作者：</b>Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler</p><p>The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM&#x27;s ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.</p><p><h4>cs.PF, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02897'>Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework</a></h3><h3><a href='https://arxiv.org/pdf/2511.02897' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/24</p><p><b>作者：</b>Clyde Meli, Vitezslav Nezval, Zuzana Kominkova Oplatkova, Victor Buttigieg, Anthony Spiteri Staines</p><p>Different bitstring representations can yield varying computational performance. This work compares three bitstring implementations in C++: std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their performance is benchmarked in the context of concatenation within a Linear Genetic Programming system. Benchmarks were conducted on three platforms (macOS, Linux, and Windows MSYS2) to assess platform specific performance variations. The results show that the custom direct implementation delivers the fastest performance on Linux and Windows, while std::bitset performs best on macOS. Although consistently slower, boost::dynamic_bitset remains a viable and flexible option. These findings highlight the influence of compiler optimisations and system architecture on performance, providing practical guidance for selecting the optimal method based on platform and application requirements.</p><p><h4>cs.NE, cs.AI, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03327'>Exploring Topologies in Quantum Annealing: A Hardware-Aware Perspective</a></h3><h3><a href='https://arxiv.org/pdf/2511.03327' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/24</p><p><b>作者：</b>Mario Bifulco, Luca Roversi</p><p>Quantum Annealing (QA) offers a promising framework for solving NP-hard optimization problems, but its effectiveness is constrained by the topology of the underlying quantum hardware. Solving an optimization problem $P$ via QA involves a hardware-aware circuit compilation which requires representing $P$ as a graph $G_P$ and embedding it into the hardware connectivity graph $G_Q$ that defines how qubits connect to each other in a QA-based quantum processing unit (QPU).  Minor Embedding (ME) is a possible operational form of this hardware-aware compilation. ME heuristically builds a map that associates each node of $G_P$ -- the logical variables of $P$ -- to a chain of adjacent nodes in $G_Q$ by means of one of its minors, so that the arcs of $G_P$ are preserved as physical connections among qubits in $G_Q$.  The static topology of hardwired qubits can clearly lead to inefficient compilations because $G_Q$ cannot be a clique, currently. We propose a methodology and a set of criteria to evaluate how the hardware topology $G_Q$ can negatively affect the embedded problem, thus making the quantum optimization more sensible to noise.  We evaluate the result of ME across two QPU topologies: Zephyr graphs (used in current D-Wave systems) and Havel-Hakimi graphs, which allow controlled variation of the average node degree. This enables us to study how the ratio `number of nodes/number of incident arcs per node&#x27; affects ME success rates to map $G_P$ into a minor of $G_Q$.  Our findings, obtained through ME executed on classical, i.e. non-quantum, architectures, suggest that Havel-Hakimi-based topologies, on average, require shorter qubit chains in the minor of $G_P$, exhibiting smoother scaling of the largest embeddable $G_P$ as the QPU size increases. These characteristics indicate their potential as alternative designs for QA-based QPUs.</p><p><h4>quant-ph, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.21600'>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</a></h3><h3><a href='https://arxiv.org/pdf/2505.21600' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/24</p><p><b>作者：</b>Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang</p><p>Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs&#x27; reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.</p><p><h4>cs.CL, cs.AI, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03079'>LogicSparse: Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators</a></h3><h3><a href='https://arxiv.org/pdf/2511.03079' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/24</p><p><b>作者：</b>Changhong Li, Biswajit Basu, Shreejith Shanker</p><p>FPGAs have been shown to be a promising platform for deploying Quantised Neural Networks (QNNs) with high-speed, low-latency, and energy-efficient inference. However, the complexity of modern deep-learning models limits the performance on resource-constrained edge devices. While quantisation and pruning alleviate these challenges, unstructured sparsity remains underexploited due to irregular memory access. This work introduces a framework that embeds unstructured sparsity into dataflow accelerators, eliminating the need for dedicated sparse engines and preserving parallelism. A hardware-aware pruning strategy is introduced to improve efficiency and design flow further. On LeNet-5, the framework attains 51.6 x compression and 1.23 x throughput improvement using only 5.12% of LUTs, effectively exploiting unstructured sparsity for QNN acceleration.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03203'>An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM</a></h3><h3><a href='https://arxiv.org/pdf/2511.03203' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/24</p><p><b>作者：</b>Deyang Yu, Chenchen Liu, Chuanjie Zhang, Xiao Fang, Weisheng Zhao</p><p>The application of Magnetic Random-Access Memory (MRAM) in computing-in-memory (CIM) has gained significant attention. However, existing designs often suffer from high energy consumption due to their reliance on complex analog circuits for computation. In this work, we present a Spin-Orbit- Torque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking processing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid series-parallel cell structure to efficiently support matrix-vector multiplication (MVM). Signal information is (en) decoded as spikes using lightweight circuits, eliminating the need for conventional area- and powerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in 28nm technology, and experimental results show that it achieves a peak energy efficiency of 243.6 TOPS/W, significantly outperforming existing designs.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03427'>Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics</a></h3><h3><a href='https://arxiv.org/pdf/2511.03427' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/24</p><p><b>作者：</b>Florentia Afentaki, Maha Shatta, Konstantinos Balaskas, Georgios Panagopoulos, Georgios Zervakis, Mehdi B. Tahoori</p><p>Flexible Electronics (FE) have emerged as a promising alternative to silicon-based technologies, offering on-demand low-cost fabrication, conformality, and sustainability. However, their large feature sizes severely limit integration density, imposing strict area and power constraints, thus prohibiting the realization of Machine Learning (ML) circuits, which can significantly enhance the capabilities of relevant near-sensor applications. Support Vector Machines (SVMs) offer high accuracy in such applications at relatively low computational complexity, satisfying FE technologies&#x27; constraints. Existing SVM designs rely solely on linear or Radial Basis Function (RBF) kernels, forcing a trade-off between hardware costs and accuracy. Linear kernels, implemented digitally, minimize overhead but sacrifice performance, while the more accurate RBF kernels are prohibitively large in digital, and their analog realization contains inherent functional approximation. In this work, we propose the first mixed-kernel and mixed-signal SVM design in FE, which unifies the advantages of both implementations and balances the cost/accuracy trade-off. To that end, we introduce a co-optimization approach that trains our mixed-kernel SVMs and maps binary SVM classifiers to the appropriate kernel (linear/RBF) and domain (digital/analog), aiming to maximize accuracy whilst reducing the number of costly RBF classifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art single-kernel linear SVMs, and reduce area and power by 108x and 17x on average compared to digital RBF implementations.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.19233'>FPGA-based Emulation and Device-Side Management for CXL-based Memory Tiering Systems</a></h3><h3><a href='https://arxiv.org/pdf/2502.19233' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/24</p><p><b>作者：</b>Yiqi Chen, Xiping Dong, Zhe Zhou, Zhao Wang, Jie Zhang, Guangyu Sun</p><p>The Compute Express Link (CXL) technology facilitates the extension of CPU memory through byte-addressable SerDes links and cascaded switches, creating complex heterogeneous memory systems where CPU access to various endpoints differs in latency and bandwidth. Effective tiered memory management is essential for optimizing system performance in such systems. However, designing an effective memory tiering system for CXL-extended heterogeneous memory faces challenges: 1) Existing evaluation methods, such as NUMA-based emulation and full-system simulations like GEM5, are limited in assessing hardware-based tiered memory management solutions and handling real-world workloads at scale. 2) Previous memory tiering systems struggle to simultaneously achieve high resolution, low overhead, and high flexibility and compatibility.  In this study, we first introduce HeteroBox, a configurable emulation platform that leverages real CXL-enabled FPGAs to emulate the performance of various CXL memory architectures. HeteroBox allows one to configure a memory space with multiple regions, each exhibiting distinct CPU-access latency and bandwidth. HeteroBox helps assess the performance of both software-managed and hardware-managed memory tiering systems with high efficiency and fidelity. Based on HeteroBox, we further propose HeteroMem, a hardware-managed memory tiering system that operates on the device side. HeteroMem creates an abstraction layer between the CPU and device memory, effectively monitoring data usage and migrating data to faster memory tiers, thus hiding device-side heterogeneity from the CPU. Evaluations with real-world applications show that HeteroMem delivers high performance while keeping heterogeneous memory management fully transparent to the CPU, achieving a 5.1\% to 16.2\% performance improvement over existing memory tiering solutions.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02866'>LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models</a></h3><h3><a href='https://arxiv.org/pdf/2511.02866' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/24</p><p><b>作者：</b>Ahmad Tahmasivand, Noureldin Zahran, Saba Al-Sayouri, Mohammed Fouda, Khaled N. Khasawneh</p><p>This paper presents LM-Fix, a lightweight detection and rapid recovery framework for faults in large language models (LLMs). Existing integrity approaches are often heavy or slow for modern LLMs. LM-Fix runs a short test-vector pass and uses hash-guided checks to detect bit-flip faults, then repairs them locally without a full reload. Across multiple models, it detects over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster than reloading. These results show a practical, low-overhead solution to keep LLMs reliable in production</p><p><h4>cs.SE, cs.AI, cs.AR, cs.CR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03092'>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</a></h3><h3><a href='https://arxiv.org/pdf/2511.03092' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/24</p><p><b>作者：</b>Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</p><p>The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.</p><p><h4>cs.AI, cs.AR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03341'>LaMoS: Enabling Efficient Large Number Modular Multiplication through SRAM-based CiM Acceleration</a></h3><h3><a href='https://arxiv.org/pdf/2511.03341' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/24</p><p><b>作者：</b>Haomin Li, Fangxin Liu, Chenyang Guan, Zongwu Wang, Li Jiang, Haibing Guan</p><p>Barrett&#x27;s algorithm is one of the most widely used methods for performing modular multiplication, a critical nonlinear operation in modern privacy computing techniques such as homomorphic encryption (HE) and zero-knowledge proofs (ZKP). Since modular multiplication dominates the processing time in these applications, computational complexity and memory limitations significantly impact performance. Computing-in-Memory (CiM) is a promising approach to tackle this problem. However, existing schemes currently suffer from two main problems: 1) Most works focus on low bit-width modular multiplication, which is inadequate for mainstream cryptographic algorithms such as elliptic curve cryptography (ECC) and the RSA algorithm, both of which require high bit-width operations; 2) Recent efforts targeting large number modular multiplication rely on inefficient in-memory logic operations, resulting in high scaling costs for larger bit-widths and increased latency. To address these issues, we propose LaMoS, an efficient SRAM-based CiM design for large-number modular multiplication, offering high scalability and area efficiency. First, we analyze the Barrett&#x27;s modular multiplication method and map the workload onto SRAM CiM macros for high bit-width cases. Additionally, we develop an efficient CiM architecture and dataflow to optimize large-number modular multiplication. Finally, we refine the mapping scheme for better scalability in high bit-width scenarios using workload grouping. Experimental results show that LaMoS achieves a $7.02\times$ speedup and reduces high bit-width scaling costs compared to existing SRAM-based CiM designs.</p><p><h4>cs.CR, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03697'>AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing</a></h3><h3><a href='https://arxiv.org/pdf/2511.03697' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/24</p><p><b>作者：</b>Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen</p><p>Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit&#x27;s design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.</p><p><h4>cs.LG, cs.AI, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.10409'>AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</a></h3><h3><a href='https://arxiv.org/pdf/2508.10409' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/24</p><p><b>作者：</b>Zihao Chen, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Xinyi Yang, Mingjie Zhou, Zhuoyao Du, Xu Yan, Zhouyang Wu, Zhenyu Xu, Jiangli Huang, Li Shang, Xuan Zeng, Fan Yang</p><p>In this paper, we propose AnalogSeeker, an effort toward an open-source foundation language model for analog circuit design, with the aim of integrating domain knowledge and giving design assistance. To overcome the scarcity of data in this field, we employ a corpus collection strategy based on the domain knowledge framework of analog circuits. High-quality, accessible textbooks across relevant subfields are systematically curated and cleaned into a textual domain corpus. To address the complexity of knowledge of analog circuits, we introduce a granular domain knowledge distillation method. Raw, unlabeled domain corpus is decomposed into typical, granular learning nodes, where a multi-agent framework distills implicit knowledge embedded in unstructured text into question-answer data pairs with detailed reasoning processes, yielding a fine-grained, learnable dataset for fine-tuning. To address the unexplored challenges in training analog circuit foundation models, we explore and share our training methods through both theoretical analysis and experimental validation. We finally establish a fine-tuning-centric training paradigm, customizing and implementing a neighborhood self-constrained supervised fine-tuning algorithm. This approach enhances training outcomes by constraining the perturbation magnitude between the model&#x27;s output distributions before and after training. In practice, we train the Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04% accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark, with a 15.67% point improvement over the original model and is competitive with mainstream commercial models. Furthermore, AnalogSeeker also shows effectiveness in the downstream operational amplifier design task. AnalogSeeker is open-sourced at https://huggingface.co/analogllm/analogseeker for research use.</p><p><h4>cs.AR, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03029'>Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project</a></h3><h3><a href='https://arxiv.org/pdf/2511.03029' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/24</p><p><b>作者：</b>Kajol Kulkarni, Samuel Kemmler, Anna Schwarz, Gulcin Gedik, Yanxiang Chen, Dimitrios Papageorgiou, Ioannis Kavroulakis, Roman Iakymchuk</p><p>Energy efficiency has emerged as a central challenge for modern high-performance computing (HPC) systems, where escalating computational demands and architectural complexity have led to significant energy footprints. This paper presents the collective experience of the EuroHPC JU Center of Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing energy consumption across major European HPC systems. We briefly review key methodologies and tools for energy measurement as well as define metrics for reporting results. Through case studies using representative CFD applications (waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution and time-to-solution metrics on diverse architectures, including CPU- and GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our results highlight the advantages of accelerators and mixed-precision techniques for reducing energy consumption while maintaining computational accuracy. Finally, we advocate the need to facilitate energy measurements on HPC systems in order to raise awareness, teach the community, and take actions toward more sustainable exascale computing.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03293'>UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM</a></h3><h3><a href='https://arxiv.org/pdf/2511.03293' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/24</p><p><b>作者：</b>Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu</p><p>Large Language Models (LLMs) are increasingly deployed on edge devices with Neural Processing Units (NPUs), yet the decode phase remains memory-intensive, limiting performance. Processing-in-Memory (PIM) offers a promising solution, but co-executing NPU-PIM systems face challenges such as data layout mismatches, bandwidth loss, and redundant storage. To address these issues, we propose UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major, tile-based layout and a configurable DRAM mapping strategy to ensure compatibility with NPU computation while maximizing PIM efficiency -- without introducing extra memory overhead or bandwidth loss. Comprehensive evaluations on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving end-to-end LLM inference efficiency on edge devices.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03609'>Stone Duality Proofs for Colorless Distributed Computability Theorems</a></h3><h3><a href='https://arxiv.org/pdf/2511.03609' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/24</p><p><b>作者：</b>Cameron Calk, Emmanuel Godard</p><p>We introduce a new topological encoding by spectral spaces of executions of  round-based full-information adversaries, a model of distributed computations that is functorially presented and that  contains many message adversaries. We give a characterization of the solvability of colorless tasks against compact adversaries.  Message adversaries are distributed  models that are known to be very expressive despite being  round-based and crash-free. Colorless tasks are  an important class of distributed tasks. For a colorless task, the  specification does not depend upon the multiplicity of input or  output values, like the ubiquitous agreement tasks.  Therefore, our result is a significant  step toward unifying topological methods in distributed computing.  The main insight is to consider global states obtained after finite executions of a distributed protocol  not as abstract  simplicial complexes as previously done, but as spectral  spaces, considering the Alexandrov topology on the faces poset. Given  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$  by projective limit in the category of spectral spaces. We derive a new general distributed computability  theorem using Stone duality: there exists an algorithm solving a colorless task $(\mathcal I,\mathcal O,\Delta)$  against the compact adversary $\mathcal M$ if and only if there exists a spectral  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible with $\Delta$.  From this general characterization are derived many known colorless computability  theorems.  Quite surprisingly, colored and uncolored models have the same  computability power (they solve the same tasks). Our new proofs give  topological reasons for this equivalence, previously known through  algorithmic reductions.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03662'>A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries</a></h3><h3><a href='https://arxiv.org/pdf/2511.03662' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/24</p><p><b>作者：</b>Yannis Coutouly, Emmanuel Godard</p><p>Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$. The solvability of a colorless task on the Iterated Immediate Snapshot model (IIS) has been characterized by the Colorless Computability Theorem \cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001, Most\&#x27;efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced \emph{condition-based adversaries}. This setting considers a particular adversary that will be applied only to a subset of input configurations. In this setting, they studied the $k$-set agreement task with condition-based $t$-resilient adversaries and obtained a sufficient condition on the conditions that make $k$-Set Agreement solvable. In this paper we have three contributions:  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent} adversaries, which means that the adversaries can change depending on the input configuration.  - We show that core-resilient adversaries of $IIS_n$ have the same computability power as the core-resilient adversaries of $IIS_n$ where crashes only happen at the start.  - Using the two previous contributions, we provide a necessary and sufficient characterization of the condition-based, core-dependent adversaries that can solve $k$-Set Agreement. We also distinguish four settings that may appear when presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later section, we present structural properties on the carrier map $\Delta$. Such properties allow simpler proof, without changing the computability power of the task. Most of the proofs in this article leverage the topological framework used in distributed computing by using simple geometric constructions.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03286'>Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots</a></h3><h3><a href='https://arxiv.org/pdf/2511.03286' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/24</p><p><b>作者：</b>Ehud Shapiro</p><p>Global digital platforms are software systems designed to serve entire populations, with some already serving billions of people. We propose atomic transactions-based multiagent transition systems and protocols as a formal framework to study them; introduce essential agents -- minimal sets of agents the removal of which makes communication impossible; and show that the cardinality of essential agents partitions all global platforms into four classes:  1. Centralised -- one (the server)  2. Decentralised -- finite $&gt;1$ (bootstrap nodes)  3. Federated -- infinite but not universal (all servers)  4. Grassroots -- universal (all agents)  Our illustrative formal example is a global social network, for which we provide centralised, decentralised, federated, and grassroots specifications via multiagent atomic transactions, and prove they satisfy basic correctness properties. We discuss informally additional global platforms -- currencies, ``sharing economy&#x27;&#x27; apps, AI, and more. While this may be the first characterisation of centralised, decentralised, and federated global platforms, grassroots platforms have been formally defined previously, but using different notions. Here, we prove that their original definition implies that all agents are essential, placing grassroots platforms in a distinct class within the broader formal context that includes all global platforms. This work provides the first mathematical framework for classifying any global platform -- existing or imagined -- by providing a multiagent atomic-transactions specification of it and determining the cardinality of the minimal set of essential agents in the ensuing multiagent protocol. It thus</p><p><h4>cs.DC, cs.MA, cs.SE, cs.SI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03533'>Investigating the Impact of Isolation on Synchronized Benchmarks</a></h3><h3><a href='https://arxiv.org/pdf/2511.03533' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/24</p><p><b>作者：</b>Nils Japke, Furat Hamdan, Diana Baumann, David Bermbach</p><p>Benchmarking in cloud environments suffers from performance variability from multi-tenant resource contention. Duet benchmarking mitigates this by running two workload versions concurrently on the same VM, exposing them to identical external interference. However, intra-VM contention between synchronized workloads necessitates additional isolation mechanisms.  This work evaluates three such strategies: cgroups and CPU pinning, Docker containers, and Firecracker MicroVMs. We compare all strategies with an unisolated baseline experiment, by running benchmarks with a duet setup alongside a noise generator. This noise generator &quot;steals&quot; compute resources to degrade performance measurements.  All experiments showed different latency distributions while under the effects of noise generation, but results show that process isolation generally lowered false positives, except for our experiments with Docker containers. Even though Docker containers rely internally on cgroups and CPU pinning, they were more susceptible to performance degradation due to noise influence. Therefore, we recommend to use process isolation for synchronized workloads, with the exception of Docker containers.</p><p><h4>cs.DC, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.09397'>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</a></h3><h3><a href='https://arxiv.org/pdf/2506.09397' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/24</p><p><b>作者：</b>Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos</p><p>The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.</p><p><h4>cs.DC, cs.AI, cs.LG, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.02184'>Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes</a></h3><h3><a href='https://arxiv.org/pdf/2505.02184' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/24</p><p><b>作者：</b>Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor</p><p>While large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel codes through a multi-stage, iterative approach integrating runtime power profiling, energy-aware prompting, self-correcting feedback loops, and an LLM-as-a-Judge agent for automated screening of code solutions. We introduce energy-reduction@k, a novel metric that quantifies expected energy reduction when generating k code candidates and selecting the most energy-efficient, enabling systematic evaluation of multi-attempt generation strategies. Evaluating 20 HeCBench applications and two miniApps on NVIDIA A100 and AMD MI100 GPUs, a single run (k=1) with LASSI-EE delivers refactored parallel codes with an average 29% expected energy reduction at an 81% pass rate, representing a 2.8x improvement over vanilla LLM prompting. Multiple runs (k=3) achieve an average 48% expected energy reduction at a 97% pass rate. These results are consistent across devices, demonstrating LASSI-EE&#x27;s effectiveness across diverse hardware architectures.</p><p><h4>cs.AI, cs.DC, cs.PL, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.12144'>Proof-of-Social-Capital: A Consensus Protocol Replacing Stake for Social Capital</a></h3><h3><a href='https://arxiv.org/pdf/2505.12144' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/24</p><p><b>作者：</b>Juraj Mariani, Ivan Homoliak</p><p>Consensus protocols used today in blockchains often rely on computational power or financial stakes - scarce resources. We propose a novel protocol using social capital - trust and influence from social interactions - as a non-transferable staking mechanism to ensure fairness and decentralization. The methodology integrates zero-knowledge proofs, verifiable credentials, a Whisk-like leader election, and an incentive scheme to prevent Sybil attacks and encourage engagement. The theoretical framework would enhance privacy and equity, though unresolved issues like off-chain bribery require further research. This work offers a new model aligned with modern social media behavior and lifestyle, with applications in finance, providing a practical insight for decentralized system development.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.23210'>FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model</a></h3><h3><a href='https://arxiv.org/pdf/2506.23210' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/24</p><p><b>作者：</b>Taehwan Yoon, Bongjun Choi, Wesley De Neve</p><p>Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.</p><p><h4>cs.LG, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.17341'>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a></h3><h3><a href='https://arxiv.org/pdf/2508.17341' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/24</p><p><b>作者：</b>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas</p><p>The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.</p><p><h4>cs.LG, cs.CR, cs.CY, cs.DC, cs.ET</h4></p></div><hr>
</body>
</html>
