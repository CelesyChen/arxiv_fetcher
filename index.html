<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-11-08</h1>
<div><h3><a href='https://arxiv.org/abs/2511.03809'>One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA</a></h3><h3><a href='https://arxiv.org/pdf/2511.03809' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/30</p><p><b>作者：</b>Fran\c{c}ois Belias, Naser Ezzati-Jivan, Foutse Khomh</p><p>Adaptive batch size methods aim to accelerate neural network training, but existing approaches apply identical adaptation strategies across all architectures, assuming a one-size-fits-all solution. We introduce DEBA (Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors gradient variance, gradient norm variation and loss variation to guide batch size adaptations. Through systematic evaluation across six architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on CIFAR-10 and CIFAR-100, with five random seeds per configuration, we demonstrate that the architecture fundamentally determines adaptation efficacy. Our findings reveal that: (1) lightweight and medium-depth architectures (MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks (ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in speedup, while deep residual networks (ResNet-50) exhibit high variance and occasional degradation; (3) already-stable architectures (ViT-B16) show minimal speedup (6%) despite maintaining accuracy, indicating that adaptation benefits vary with baseline optimization characteristics. We introduce a baseline characterization framework using gradient stability metrics (stability score, gradient norm variation) that predicts which architectures will benefit from adaptive scheduling. Our ablation studies reveal critical design choices often overlooked in prior work: sliding window statistics (vs. full history) and sufficient cooldown periods (5+ epochs) between adaptations are essential for success. This work challenges the prevailing assumption that adaptive methods generalize across architectures and provides the first systematic evidence that batch size adaptation requires an architecture-aware design.</p><p><h4>cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03866'>OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms</a></h3><h3><a href='https://arxiv.org/pdf/2511.03866' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/30</p><p><b>作者：</b>Arijit Bhattacharjee, Ali TehraniJamsaz, Le Chen, Niranjan Hasabnis, Mihai Capota, Nesreen Ahmed, Ali Jannesari</p><p>Recent advances in large language models (LLMs) have significantly accelerated progress in code translation, enabling more accurate and efficient transformation across programming languages. While originally developed for natural language processing, LLMs have shown strong capabilities in modeling programming language syntax and semantics, outperforming traditional rule-based systems in both accuracy and flexibility. These models have streamlined cross-language conversion, reduced development overhead, and accelerated legacy code migration. In this paper, we introduce OMPILOT, a novel domain-specific encoder-decoder transformer tailored for translating C++ code into OpenMP, enabling effective shared-memory parallelization. OMPILOT leverages custom pre-training objectives that incorporate the semantics of parallel constructs and combines both unsupervised and supervised learning strategies to improve code translation robustness. Unlike previous work that focused primarily on loop-level transformations, OMPILOT operates at the function level to capture a wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel composite metric specifically crafted to assess the correctness and quality of OpenMP parallel constructs, addressing limitations in conventional translation metrics.</p><p><h4>cs.DC, cs.AI, cs.LG, cs.PF, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04489'>Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion</a></h3><h3><a href='https://arxiv.org/pdf/2511.04489' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/30</p><p><b>作者：</b>Oskar Lappi, Huw Leggate, Yannick Marandet, Jan {\AA}str\&quot;om, Keijo Heljanko, Dmitriy V. Borodin</p><p>EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints.</p><p><h4>physics.comp-ph, cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2311.14114'>SySMOL: Co-designing Algorithms and Hardware for Neural Networks with Heterogeneous Precisions</a></h3><h3><a href='https://arxiv.org/pdf/2311.14114' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/30</p><p><b>作者：</b>Cyrus Zhou, Pedro Savarese, Zack Hassman, Vaughn Richard, Michael DiBrino, Michael Maire, Yanjing Li</p><p>Ultra-low-precision inference can sharply reduce memory and latency but often degrades accuracy and relies on specialized hardware. We present SONIQ, a system-optimized, noise-injected quantization framework that learns per-channel mixed precision for both weights and activations while training under the same rules used at inference. By injecting hardware-calibrated quantization noise during training, SONIQ steers models toward the discrete arithmetic used at deployment -- without bespoke runtimes. Across CNNs and Transformers, SONIQ achieves up to 16x and 7x compression, respectively, while matching or exceeding full-precision accuracy. Measured end-to-end, SONIQ delivers up to 7.3x CPU speedup over strong INT8 baselines and up to 6.3x (vector units) / 2.8x (tensor cores) GPU speedup relative to FP16. A practical outcome is that two per-channel precision levels -- one in the 1--4-bit range and one in the 4--8-bit range -- suffice in practice; at inference, each channel selects one of the two, keeping kernels simple and fast. To our knowledge, SONIQ is the first framework to reach or surpass full-precision accuracy under ultra-low (1--4 bits per parameter) regimes while remaining deployable on commodity hardware, narrowing the gap between quantization theory and practical, high-throughput inference.</p><p><h4>cs.AR, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2503.22879'>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</a></h3><h3><a href='https://arxiv.org/pdf/2503.22879' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/30</p><p><b>作者：</b>Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu</p><p>State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.</p><p><h4>cs.LG, cs.AI, cs.CL, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03944'>From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies</a></h3><h3><a href='https://arxiv.org/pdf/2511.03944' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/30</p><p><b>作者：</b>Tong Zhang, Vikram Sharma Mailthody, Fei Sun, Linsen Ma, Chris J. Newburn, Teresa Zhang, Yang Liu, Jiangpeng Li, Hao Zhong, Wen-Mei Hwu</p><p>In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04036'>PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration</a></h3><h3><a href='https://arxiv.org/pdf/2511.04036' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/30</p><p><b>作者：</b>Yue Jiet Chong, Yimin Wang, Zhen Wu, Xuanyao Fong</p><p>This paper presents a 3D-stacked chiplets based large language model (LLM) inference accelerator, consisting of non-volatile in-memory-computing processing elements (PEs) and Inter-PE Computational Network (IPCN), interconnected via silicon photonic to effectively address the communication bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling and workload mapping. Simulation results show it achieves $3.95\times$ speedup and $30\times$ efficiency improvement over the Nvidia A100 before chiplet clustering and power gating scheme (CCPG). Additionally, the system achieves further scalability and efficiency improvement with the implementation of CCPG to accommodate larger models, attaining $57\times$ efficiency improvement over Nvidia H100 at similar throughput.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04677'>Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers</a></h3><h3><a href='https://arxiv.org/pdf/2511.04677' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/30</p><p><b>作者：</b>Joaquin Tarraga-Moreno, Daniel Barley, Francisco J. Andujar Munoz, Jesus Escudero-Sahuquillo, Holger Froning, Pedro Javier Garcia, Francisco J. Quiles, Jose Duato</p><p>The rapid growth of data-intensive applications such as generative AI, scientific simulations, and large-scale analytics is driving modern supercomputers and data centers toward increasingly heterogeneous and tightly integrated architectures. These systems combine powerful CPUs and accelerators with emerging high-bandwidth memory and storage technologies to reduce data movement and improve computational efficiency. However, as the number of accelerators per node increases, communication bottlenecks emerge both within and between nodes, particularly when network resources are shared among heterogeneous components.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04104'>Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs</a></h3><h3><a href='https://arxiv.org/pdf/2511.04104' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/30</p><p><b>作者：</b>Chao Guo, Jiahe Xu, Moshe Zukerman</p><p>Hardware disaggregation seeks to transform Data Center (DC) resources from traditional server fleets into unified resource pools. Despite existing challenges that may hinder its full realization, significant progress has been made in both industry and academia. In this article, we provide an overview of the motivations and recent advancements in hardware disaggregation. We further discuss the research challenges and opportunities associated with disaggregated architectures, focusing on aspects that have received limited attention. We argue that hardware disaggregation has the potential to reshape the entire DC ecosystem, impacting application design, resource scheduling, hardware configuration, cooling, and power system optimization. Additionally, we present a numerical study to illustrate several key aspects of these challenges.</p><p><h4>cs.AR, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04321'>AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM</a></h3><h3><a href='https://arxiv.org/pdf/2511.04321' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/30</p><p><b>作者：</b>Yuanpeng Zhang, Xing Hu, Xi Chen, Zhihang Yuan, Cong Li, Jingchen Zhu, Zhao Wang, Chenguang Zhang, Xin Si, Wei Gao, Qiang Wu, Runsheng Wang, Guangyu Sun</p><p>SRAM Processing-in-Memory (PIM) has emerged as the most promising implementation for high-performance PIM, delivering superior computing density, energy efficiency, and computational precision. However, the pursuit of higher performance necessitates more complex circuit designs and increased operating frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly degrade chip performance and even threaten reliability. Conventional circuit-level IR-drop mitigation methods, such as back-end optimizations, are resource-intensive and often compromise power, performance, and area (PPA). To address these challenges, we propose AIM, comprehensive software and hardware co-design for architecture-level IR-drop mitigation in high-performance PIM. Initially, leveraging the bit-serial and in-situ dataflow processing properties of PIM, we introduce Rtog and HR, which establish a direct correlation between PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS, enabling extensive exploration of architecture-level IR-drop mitigation while maintaining computational accuracy through software optimization. Subsequently, we develop IR-Booster, a dynamic adjustment mechanism that integrates software-level HR information with hardware-based IR-drop monitoring to adapt the V-f pairs of the PIM macro, achieving enhanced energy efficiency and performance. Finally, we propose the HR-aware task mapping method, bridging software and hardware designs to achieve optimal improvement. Post-layout simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement and 1.152x speedup.</p><p><h4>cs.AR, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03765'>LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices</a></h3><h3><a href='https://arxiv.org/pdf/2511.03765' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/30</p><p><b>作者：</b>Hyunseok Kwak, Kyeongwon Lee, Jae-Jin Lee, Woojoo Lee</p><p>On-device fine-tuning of CNNs is essential to withstand domain shift in edge applications such as Human Activity Recognition (HAR), yet full fine-tuning is infeasible under strict memory, compute, and energy budgets. We present LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional layers, (ii) selectively updates only the output-side core with zero-initialization to keep the auxiliary path inactive at the start, and (iii) fuses the update back into dense kernels, leaving inference cost unchanged. This design preserves convolutional structure and reduces the number of trainable parameters by up to two orders of magnitude compared to full fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, consistently outperforming prior parameter-efficient baselines under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and selective-core training yield 1.4-3.8x faster convergence to target F1. LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms.</p><p><h4>cs.CV, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2504.18812'>SynFuzz: Leveraging Fuzzing of Netlist to Detect Synthesis Bugs</a></h3><h3><a href='https://arxiv.org/pdf/2504.18812' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/30</p><p><b>作者：</b>Raghul Saravanan, Sudipta Paria, Aritra Dasgupta, Venkat Nitin Patnala, Swarup Bhunia, Sai Manoj P D</p><p>In the evolving landscape of integrated circuit (IC) design, the increasing complexity of modern processors and intellectual property (IP) cores has introduced new challenges in ensuring design correctness and security. The recent advancements in hardware fuzzing techniques have shown their efficacy in detecting hardware bugs and vulnerabilities at the RTL abstraction level of hardware. However, they suffer from several limitations, including an inability to address vulnerabilities introduced during synthesis and gate-level transformations. These methods often fail to detect issues arising from library adversaries, where compromised or malicious library components can introduce backdoors or unintended behaviors into the design. In this paper, we present a novel hardware fuzzer, SynFuzz, designed to overcome the limitations of existing hardware fuzzing frameworks. SynFuzz focuses on fuzzing hardware at the gate-level netlist to identify synthesis bugs and vulnerabilities that arise during the transition from RTL to the gate-level. We analyze the intrinsic hardware behaviors using coverage metrics specifically tailored for the gate-level. Furthermore, SynFuzz implements differential fuzzing to uncover bugs associated with EDA libraries. We evaluated SynFuzz on popular open-source processors and IP designs, successfully identifying 7 new synthesis bugs. Additionally, by exploiting the optimization settings of EDA tools, we performed a compromised library mapping attack (CLiMA), creating a malicious version of hardware designs that remains undetectable by traditional verification methods. We also demonstrate how SynFuzz overcomes the limitations of the industry-standard formal verification tool, Cadence Conformal, providing a more robust and comprehensive approach to hardware verification.</p><p><h4>cs.CR, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.09198'>Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation</a></h3><h3><a href='https://arxiv.org/pdf/2506.09198' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/30</p><p><b>作者：</b>Ali Rezaei, Luc Jaulmes, Maria Bahna, Oliver Thomson Brown, Antonio Barbalace</p><p>Scalable classical simulation of quantum circuits is crucial for advancing quantum algorithm development and validating emerging hardware. This work focuses on performance enhancements through targeted low-level and NUMA-aware tuning on a single-node system, thereby not only advancing the efficiency of classical quantum simulations but also establishing a foundation for scalable, heterogeneous implementations that bridge toward noiseless quantum computing. Although few prior studies have reported similar hardware-level optimizations, such implementations have not been released as open-source software, limiting independent validation and further development. We introduce an open-source, high-performance extension to the QuEST state vector simulator that integrates state-of-the-art low-level and NUMA-aware optimizations for modern processors. Our approach emphasizes locality-aware computation and incorporates hardware-specific techniques including NUMA-aware memory allocation, thread pinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory prefetching. Experiments demonstrate substantial speedups--5.5-6.5x for single-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum Circuits (RQC), and 1.8x for the Quantum Fourier Transform (QFT). Algorithmic workloads further achieve 4.3-4.6x acceleration for Grover and 2.5x for Shor-like circuits. These results show that systematic, architecture-aware tuning can significantly extend the practical simulation capacity of classical quantum simulators on current hardware.</p><p><h4>quant-ph, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2507.18989'>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</a></h3><h3><a href='https://arxiv.org/pdf/2507.18989' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/30</p><p><b>作者：</b>Maxence Bouvier, Ryan Amaudruz, Felix Arnold, Renzo Andri, Lukas Cavigelli</p><p>As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important for reducing the footprint of digital systems. Conventional design flows, which often rely on manual or heuristic-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, with a focus on multipliers.  At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables deployment of a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two&#x27;s complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL&#x27;s applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.</p><p><h4>cs.LG, cs.AI, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.01213'>JaneEye: A 12-nm 2K-FPS 18.9-$\mu$J/Frame Event-based Eye Tracking Accelerator</a></h3><h3><a href='https://arxiv.org/pdf/2510.01213' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/30</p><p><b>作者：</b>Tao Han, Ang Li, Qinyu Chen, Chang Gao</p><p>Eye tracking has become a key technology for gaze-based interactions in Extended Reality (XR). However, conventional frame-based eye-tracking systems often fall short of XR&#x27;s stringent requirements for high accuracy, low latency, and energy efficiency. Event cameras present a compelling alternative, offering ultra-high temporal resolution and low power consumption. In this paper, we present JaneEye, an energy-efficient event-based eye-tracking hardware accelerator designed specifically for wearable devices, leveraging sparse, high-temporal-resolution event data. We introduce an ultra-lightweight neural network architecture featuring a novel ConvJANET layer, which simplifies the traditional ConvLSTM by retaining only the forget gate, thereby halving computational complexity without sacrificing temporal modeling capability. Our proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+ dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To further enhance hardware efficiency, we employ custom linear approximations of activation functions (hardsigmoid and hardtanh) and fixed-point quantization. Through software-hardware co-design, our 12-nm ASIC implementation operates at 400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a new benchmark in low-power, high-performance eye-tracking solutions suitable for integration into next-generation XR wearables.</p><p><h4>eess.SP, cs.AR, cs.CV, cs.HC, eess.IV</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03941'>Stochastic Modeling for Energy-Efficient Edge Infrastructure</a></h3><h3><a href='https://arxiv.org/pdf/2511.03941' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/30</p><p><b>作者：</b>Fabio Diniz Rossi</p><p>Edge Computing enables low-latency processing for real-time applications but introduces challenges in power management due to the distributed nature of edge devices and their limited energy resources. This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing. By deriving steady-state probabilities and evaluating energy consumption, we demonstrate the benefits of AI-driven predictive power scaling over conventional reactive methods. Monte Carlo simulations validate the model, showing strong alignment between theoretical and empirical results. Sensitivity analysis highlights how varying transition probabilities affect power efficiency, confirming that predictive scaling minimizes unnecessary transitions and improves overall system responsiveness. Our findings suggest that AI-based power management strategies significantly enhance energy efficiency by anticipating workload demands and optimizing state transitions. Experimental results indicate that AI-based power management optimizes workload distribution across heterogeneous edge nodes, reducing energy consumption disparities between devices, improving overall efficiency, and enhancing adaptive power coordination in multi-node environments.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04268'>Parallel Spawning Strategies for Dynamic-Aware MPI Applications</a></h3><h3><a href='https://arxiv.org/pdf/2511.04268' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/30</p><p><b>作者：</b>Iker Mart\&#x27;in-\&#x27;Alvarez, Jos\&#x27;e I. Aliaga, Maribel Castillo, Sergio Iserte</p><p>Dynamic resource management is an increasingly important capability of High Performance Computing systems, as it enables jobs to adjust their resource allocation at runtime. This capability has been shown to reduce workload makespan, substantially decrease job waiting times and improve overall system utilization. In this context, malleability refers to the ability of applications to adapt to new resource allocations during execution. Although beneficial, malleability incurs significant reconfiguration costs, making the reduction of these costs an important research topic.  Some existing methods for MPI applications respawn the entire application, which is an expensive solution that avoids the reuse of original processes. Other MPI methods reuse them, but fail to fully release unneeded processes when shrinking, since some ranks within the same communicator remain active across nodes, preventing the application from returning those nodes to the system. This work overcomes both limitations by proposing a novel parallel spawning strategy, in which all processes cooperate in spawning before redistribution, thereby reducing execution time. Additionally, it removes shrinkage limitations, allowing better adaptation of parallel systems to workload and reducing their makespan. As a result, it preserves competitive expansion times with at most a $1.25\times$ overhead, while enabling fast shrink operations that reduce their cost by at least $20\times$. This strategy has been validated on both homogeneous and heterogeneous systems and can also be applied in shared-resource environments.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04477'>Enabling Dynamic Sparsity in Quantized LLM Inference</a></h3><h3><a href='https://arxiv.org/pdf/2511.04477' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/30</p><p><b>作者：</b>Rongxiang Wang, Kangyuan Shu, Felix Xiaozhu Lin</p><p>Deploying large language models (LLMs) on end-user devices is gaining importance due to benefits in responsiveness, privacy, and operational cost. Yet the limited memory and compute capability of mobile and desktop GPUs make efficient execution difficult. Recent observations suggest that the internal activations of LLMs are often dynamically sparse, meaning that for each input, only part of the network contributes significantly to the output. Such sparsity could reduce computation, but it interacts poorly with group-wise quantization, which remains the dominant approach for fitting LLMs onto resource-constrained hardware. To reconcile these two properties, this study proposes a set of techniques that realize dynamic sparse inference under low-bit quantization. The method features: (1) a zigzag-patterned quantization layout that organizes weights in a way consistent with activation sparsity and improves GPU memory locality; (2) a specialized GEMV kernel designed for this layout to fully utilize parallel compute units; and (3) a compact runtime mechanism that gathers sparse indices with minimal overhead. Across several model scales and hardware configurations, the approach achieves up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference, showing that structured sparsity and quantization can effectively coexist on commodity GPUs.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04523'>A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems</a></h3><h3><a href='https://arxiv.org/pdf/2511.04523' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/30</p><p><b>作者：</b>Silvia Bonomi, Giovanni Farina, Roy Friedman, Eviatar B. Procaccia, Sebastien Tixeuil</p><p>Modern distributed systems face growing security threats, as attackers continuously enhance their skills and vulnerabilities span across the entire system stack, from hardware to the application layer. In the system design phase, fault tolerance techniques can be employed to safeguard systems. From a theoretical perspective, an attacker attempting to compromise a system can be abstracted by considering the presence of Byzantine processes in the system. Although this approach enhances the resilience of the distributed system, it introduces certain limitations regarding the accuracy of the model in reflecting real-world scenarios. In this paper, we consider a self-protecting distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component. Our new model captures the dynamics of evolving attacks and can be used to drive the self-protection and reconfiguration strategy. We analyze mathematically the time that it takes until the number of Byzantine nodes crosses given thresholds, or for the system to self-recover back into a safe state, depending on the rates of Byzantine infection spreading \emph{vs.} the rate of self-recovery. We also provide simulation results that illustrate the behavior of the system under such assumptions.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04631'>Resolving Conflicts with Grace: Dynamically Concurrent Universality</a></h3><h3><a href='https://arxiv.org/pdf/2511.04631' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/30</p><p><b>作者：</b>Petr Kuznetsov, Nathan Josia Schrodt</p><p>Synchronization is the major obstacle to scalability in distributed computing. Concurrent operations on the shared data engage in synchronization when they encounter a \emph{conflict}, i.e., their effects depend on the order in which they are applied. Ideally, one would like to detect conflicts in a \emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it is very common that two concurrent operations conflict only in some rarely occurring states. In this paper, we define the notion of \emph{dynamic concurrency}: an operation employs strong synchronization primitives only if it \emph{has} to arbitrate with concurrent operations, given the current system state. We then present a dynamically concurrent universal construction.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.09334'>ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments</a></h3><h3><a href='https://arxiv.org/pdf/2502.09334' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/30</p><p><b>作者：</b>Youhe Jiang, Fangcheng Fu, Xiaozhe Yao, Taiyi Wang, Bin Cui, Ana Klimovic, Eiko Yoneki</p><p>Recent developments in large language models (LLMs) have demonstrated their remarkable proficiency in a range of tasks. Compared to in-house homogeneous GPU clusters, deploying LLMs in cloud environments with diverse types of GPUs is crucial for addressing the GPU shortage problem and being more cost-effective. However, the diversity of network environments and various GPU types on the cloud bring difficulties to achieving high-performance serving. In this work, we propose ThunderServe, a high-performance and cost-efficient LLM serving system for heterogeneous cloud environments. We introduce a novel scheduling algorithm, which optimizes the deployment plan of LLM serving to accommodate the heterogeneous resource and network bandwidth conditions in cloud environments. Furthermore, we propose a lightweight re-scheduling mechanism, designed to adapt to fluctuating online conditions (e.g., node failures, workload shifts) without the need for costly restarts of ongoing services. Empirical results in both heterogeneous cloud and homogeneous in-house environments reveal that ThunderServe delivers up to a 2.1$\times$ and on average a $1.7\times$ increase in throughput and achieves up to a 2.5$\times$ and on average a $1.5\times$ reduction in latency deadlines compared with state-of-the-art systems given the same price budget, suggesting opting for cloud services provides a more cost-efficient solution.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.11916'>Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture</a></h3><h3><a href='https://arxiv.org/pdf/2505.11916' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/30</p><p><b>作者：</b>Yu Wu, Tongxuan Liu, Yuting Zeng, Siyu Wu, Jun Xiong, Xianzhe Dong, Hailong Yang, Ke Zhang, Jing Li</p><p>Existing large language model (LLM) serving systems typically employ Prefill-Decode disaggregated architecture to prevent computational interference between the prefill and decode phases. However, in real-world LLM serving scenarios, significant fluctuations in request input/output lengths lead to imbalanced computational loads between prefill and decode nodes under traditional static node allocation strategies, consequently preventing efficient utilization of computing resources to improve the system&#x27;s goodput. To address this challenge, we design and implement Arrow, an adaptive scheduler that leverages stateless instances and latency characteristics of prefill and decode tasks to achieve efficient adaptive request and instance scheduling. Arrow dynamically adjusts the number of instances handling prefill and decode tasks based on real-time cluster performance metrics, substantially enhancing the system&#x27;s capability to handle traffic spikes and load variations. Our evaluation under diverse real-world workloads shows that Arrow achieves up to $2.55 \times$ higher request serving rates compared to state-of-the-art Prefill-Decode disaggregated serving systems.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2507.18928'>GPUnion: Autonomous GPU Sharing on Campus</a></h3><h3><a href='https://arxiv.org/pdf/2507.18928' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/30</p><p><b>作者：</b>Yufang Li (The Hong Kong University of Science,Technology), Yuanbo Zhang (Sun Yat-sen University), Hanlong Liao (Sun Yat-sen University), Deke Guo (Sun Yat-sen University), Guoming Tang (The Hong Kong University of Science,Technology)</p><p>A pronounced imbalance in GPU resources exists on campus, where some laboratories own underutilized servers while others lack the compute needed for AI research. GPU sharing can alleviate this disparity, while existing platforms typically rely on centralized oversight and persistent allocation models, conflicting with the voluntary and autonomous nature of academic resource ownership. We present GPUnion, a campus-scale GPU sharing platform enabling voluntary participation while preserving full provider autonomy. GPUnion incorporates three core mechanisms: i) container-based task dispatching and execution, ii) resource provider-first architecture, and iii) resilient execution featuring automatic check-pointing and migration. Case studies across multiple campus scenarios demonstrate 30% more GPU utilization improvement, 40% increase in interactive sessions, and 94% successful workload migration during provider departures.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.03359'>Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture</a></h3><h3><a href='https://arxiv.org/pdf/2511.03359' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/30</p><p><b>作者：</b>Hans De Raedt, Jiri Kraus, Andreas Herten, Vrinda Mehta, Mathis Bode, Markus Hrywniak, Kristel Michielsen, Thomas Lippert</p><p>We have developed a new version of the high-performance J\&quot;ulich universal quantum computer simulator (JUQCS-50) that leverages key features of the GH200 superchips as used in the JUPITER supercomputer, enabling simulations of a 50-qubit universal quantum computer for the first time. JUQCS-50 achieves this through three key innovations: (1) extending usable memory beyond GPU limits via high-bandwidth CPU-GPU interconnects and LPDDR5 memory; (2) adaptive data encoding to reduce memory footprint with acceptable trade-offs in precision and compute effort; and (3) an on-the-fly network traffic optimizer. These advances result in an 11.4-fold speedup over the previous 48-qubit record on the K computer.</p><p><h4>quant-ph, cs.DC, physics.comp-ph</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.04183'>A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing</a></h3><h3><a href='https://arxiv.org/pdf/2511.04183' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/30</p><p><b>作者：</b>Leszek Sliwko</p><p>This paper presents a reinforced genetic approach to a defined d-resource system optimization problem. The classical evolution schema was ineffective due to a very strict feasibility function in the studied problem. Hence, the presented strategy has introduced several modifications and adaptations to standard genetic routines, e.g.: a migration operator which is an analogy to the biological random genetic drift.</p><p><h4>cs.NE, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2408.07587'>FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual Teacher</a></h3><h3><a href='https://arxiv.org/pdf/2408.07587' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/30</p><p><b>作者：</b>Alessio Mora, Lorenzo Valerio, Paolo Bellavista, Andrea Passarella</p><p>Federated Learning (FL) systems enable the collaborative training of machine learning models without requiring centralized collection of individual data. FL participants should have the ability to exercise their right to be forgotten, ensuring their past contributions can be removed from the learned model upon request. In this paper, we propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub the contribution of the data to forget from an FL global model while preserving its generalization ability. FedQUIT directly works on client devices that request to leave the federation, and leverages a teacher-student framework. The FL global model acts as the teacher, and the local model works as the student. To induce forgetting, FedQUIT tailors the teacher&#x27;s output on local data (the data to forget) penalizing the prediction score of the true class. Unlike previous work, our method does not require hardly viable assumptions for cross-device settings, such as storing historical updates of participants or requiring access to proxy datasets. Experimental results on various datasets and model architectures demonstrate that (i) FedQUIT outperforms state-of-the-art competitors in forgetting data, (ii) has the exact computational requirements as a regular FedAvg round, and (iii) reduces the cumulative communication costs by up to 117.6$\times$ compared to retraining from scratch to restore the initial generalization performance after unlearning.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2412.04650'>Revisiting Federated Fine-Tuning: A Single Communication Round is Enough for Foundation Models</a></h3><h3><a href='https://arxiv.org/pdf/2412.04650' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/30</p><p><b>作者：</b>Ziyao Wang, Bowei Tian, Yexiao He, Zheyu Shen, Guoheng Sun, Yuhan Liu, Luyang Liu, Meng Liu, Ang Li</p><p>The recent advancement of foundation models (FMs) has increased the demand for fine-tuning these models on large-scale cross-domain datasets. To address this, federated fine-tuning has emerged, allowing FMs to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size and the multi-round communication in federated learning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we identify and analyze, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of aggregation (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of aggregation. Through rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models. Our extensive experiments show that one-shot federated fine-tuning significantly reduces communication costs. It also has the potential to enable asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning on both text generation and text-to-image generation tasks. Our findings provide insights to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for FMs.</p><p><h4>cs.LG, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.04014'>Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]</a></h3><h3><a href='https://arxiv.org/pdf/2505.04014' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/30</p><p><b>作者：</b>David Chu, Aditya Balasubramanian, Dee Bao, Natacha Crooks, Heidi Howard, Lucky E. Katahanas, Soujanya Ponnapalli</p><p>Today, users can &quot;lift-and-shift&quot; unmodified applications into modern, VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based security guarantees. However, TEEs do not protect applications against disk rollback attacks, where persistent storage can be reverted to an earlier state after a crash; existing rollback resistance solutions either only support a subset of applications or require code modification. Our key insight is that restoring disk consistency after a rollback attack guarantees rollback resistance for any application. We present Rollbaccine, a device mapper that provides automatic rollback resistance for all applications by provably preserving disk consistency. Rollbaccine intercepts and replicates writes to disk, restores lost state from backups during recovery, and minimizes overheads by taking advantage of the weak, multi-threaded semantics of disk operations. Rollbaccine performs on-par with state-of-the-art, non-automatic rollback resistant solutions; in fact, across benchmarks over PostgreSQL, HDFS, and two file systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the fsync-heavy Filebench Varmail.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.16298'>Scalable hybrid quantum Monte Carlo simulation of U(1) gauge field coupled to fermions on GPU</a></h3><h3><a href='https://arxiv.org/pdf/2508.16298' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/30</p><p><b>作者：</b>Kexin Feng, Chuang Chen, Zi Yang Meng</p><p>We develop a GPU-accelerated hybrid quantum Monte Carlo (QMC) algorithm to solve the fundamental yet difficult problem of $U(1)$ gauge field coupled to fermions, which gives rise to a $U(1)$ Dirac spin liquid state under the description of (2+1)d quantum electrodynamics QED$_3$. The algorithm renders a good acceptance rate and, more importantly, nearly linear space-time volume scaling in computational complexity $O(N_{\tau} V_s)$, where $N_\tau$ is the imaginary time dimension and $V_s$ is spatial volume, which is much more efficient than determinant QMC with scaling behavior of $O(N_\tau V_s^3)$. Such acceleration is achieved via a collection of technical improvements, including (i) the design of the efficient problem-specific preconditioner, (ii) customized CUDA kernel for matrix-vector multiplication, and (iii) CUDA Graph implementation on the GPU. These advances allow us to simulate the $U(1)$ Dirac spin liquid state with unprecedentedly large system sizes, which is up to $N_\tau\times L\times L = 660\times66\times66$, and reveal its novel properties. With these technical improvements, we see the asymptotic convergence in the scaling dimensions of various fermion bilinear operators and the conserved current operator when approaching the thermodynamic limit. The scaling dimensions find good agreement with field-theoretical expectation, which provides supporting evidence for the conformal nature of the $U(1)$ Dirac spin liquid state in the \qed. Our technical advancements open an avenue to study the Dirac spin liquid state and its transition towards symmetry-breaking phases at larger system sizes and with less computational burden.</p><p><h4>cond-mat.str-el, cs.DC, hep-th</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.02851'>Action Deviation-Aware Inference for Low-Latency Wireless Robots</a></h3><h3><a href='https://arxiv.org/pdf/2510.02851' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/30</p><p><b>作者：</b>Jeyoung Park, Yeonsub Lim, Seungeun Oh, Jihong Park, Jinho Choi, Seong-Lyun Kim</p><p>To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML with computational resources in mobile, edge, and cloud connected over hyper-reliable low-latency communication (HRLLC). In this setting, speculative decoding can facilitate collaborative inference of models distributively deployed: a lightweight on-device model locally generates drafts while a more capable remote target model on a server verifies and corrects them in parallel with speculative sampling, thus resulting in lower latency without compromising accuracy. However, unlike autoregressive text generation, behavior cloning policies, typically used for embodied AI applications, cannot parallelize verification and correction for multiple drafts as each generated action depends on observation updated by a previous action. To this end, we propose Action Deviation-Aware Hybrid Inference (ADAHI), wherein drafts are selectively transmitted and verified based on action deviation, which has a strong correlation with action&#x27;s rejection probability by the target model. By invoking server operation only when necessary, communication and computational overhead can be reduced while accuracy gain from speculative sampling is preserved. Experiments on our testbed show that ADAHI reduces transmission and server operations by approximately 40%, lowers end-to-end latency by 39.2%, and attains up to 97.2% of the task-success rate of baseline that invokes speculative sampling for every draft embedding vector.</p><p><h4>cs.RO, cs.DC</h4></p></div><hr>
</body>
</html>
