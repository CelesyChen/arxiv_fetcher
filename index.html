<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-11-21</h1>
<div><h3><a href='https://arxiv.org/abs/2511.15015'>Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference</a></h3><h3><a href='https://arxiv.org/pdf/2511.15015' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/27</p><p><b>作者：</b>Kexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Liu, Wei Zhang</p><p>Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.</p><p><h4>cs.PF, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15626'>A Latency-Constrained, Gated Recurrent Unit (GRU) Implementation in the Versal AI Engine</a></h3><h3><a href='https://arxiv.org/pdf/2511.15626' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/27</p><p><b>作者：</b>M. Sapkas, A. Triossi, M. Zanetti</p><p>This work explores the use of the AMD Xilinx Versal Adaptable Intelligent Engine(AIE) to accelerate Gated Recurrent Unit (GRU) inference for latency-Constrained applications. We present a custom workload distribution framework across the AIE&#x27;s vector processors and propose a hybrid AIE - Programmable Logic (PL) design to optimize computational efficiency. Our approach highlights the potential of deploying adaptable neural networks in real-time environments such as online preprocessing in the readout chain of a physics experiment, offering a flexible alternative to traditional fixed-function algorithms.</p><p><h4>cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15503'>A Tensor Compiler for Processing-In-Memory Architectures</a></h3><h3><a href='https://arxiv.org/pdf/2511.15503' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/27</p><p><b>作者：</b>Peiming Yang, Sankeerth Durvasula, Ivan Fernandez, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko, Christina Giannoula</p><p>Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.</p><p><h4>cs.AR, cs.DC, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.21185'>Constructive community race: full-density spiking neural network model drives neuromorphic computing</a></h3><h3><a href='https://arxiv.org/pdf/2505.21185' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/27</p><p><b>作者：</b>Johanna Senk, Anno C. Kurth, Steve Furber, Tobias Gemmeke, Bruno Golosio, Arne Heittmann, James C. Knight, Eric M\&quot;uller, Tobias Noll, Thomas Nowotny, Gorka Peraza Coppola, Luca Peres, Oliver Rhodes, Andrew Rowley, Johannes Schemmel, Tim Stadtmann, Tom Tetzlaff, Gianmarco Tiddia, Sacha J. van Albada, Jos\&#x27;e Villamar, Markus Diesmann</p><p>The local circuitry of the mammalian brain is a focus of the search for generic computational principles because it is largely conserved across species and modalities. In 2014 a model was proposed representing all neurons and synapses of the stereotypical cortical microcircuit below $1\,\text{mm}^2$ of brain surface. The model reproduces fundamental features of brain activity but its impact remained limited because of its computational demands. For theory and simulation, however, the model was a breakthrough because it removes uncertainties of downscaling, and larger models are less densely connected. This sparked a race in the neuromorphic computing community and the model became a de facto standard benchmark. Within a few years real-time performance was reached and surpassed at significantly reduced energy consumption. We review how the computational challenge was tackled by different simulation technologies and derive guidelines for the next generation of benchmarks and other domains of science.</p><p><h4>cs.PF, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.14990'>CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations</a></h3><h3><a href='https://arxiv.org/pdf/2511.14990' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/27</p><p><b>作者：</b>Zhuolun Jiang, Songyue Wang, Xiaokun Pei, Tianyue Lu, Mingyu Chen</p><p>Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15367'>DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution</a></h3><h3><a href='https://arxiv.org/pdf/2511.15367' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/27</p><p><b>作者：</b>Xin Yang, Xin Fan, Zengshi Wang, Jun Han</p><p>Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15397'>Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism</a></h3><h3><a href='https://arxiv.org/pdf/2511.15397' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/27</p><p><b>作者：</b>Cong Wang, Zexin Fu, Jiayi Huang, Shanshi Huang</p><p>Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15505'>Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference</a></h3><h3><a href='https://arxiv.org/pdf/2511.15505' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/27</p><p><b>作者：</b>Anastasios Petropoulos, Theodore Antonakopoulos</p><p>This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15564'>Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond</a></h3><h3><a href='https://arxiv.org/pdf/2511.15564' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/27</p><p><b>作者：</b>Paul Scheffler, Thomas Benz, Tim Fischer, Lorenzo Leone, Sina Arjmandpour, Luca Benini</p><p>We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.03866'>Flexible In-NAND Cryptographic Processing for Secure Flash Storage</a></h3><h3><a href='https://arxiv.org/pdf/2508.03866' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/27</p><p><b>作者：</b>Seock-Hwan Noh, Hoyeon Lee, Junkyum Kim, Junsu Im, Jay H. Park, Sungjin Lee, Sam H. Noh, Yeseong Kim, Jaeha Kung</p><p>We present FlashVault, an in-NAND self-encryption architecture that embeds a reconfigurable cryptographic engine into the unused silicon area of a state-of-the-art 4D V-NAND structure. FlashVault supports not only block ciphers for data encryption but also public-key and post-quantum algorithms for digital signatures, all within the NAND flash chip. This design enables each NAND chip to operate as a self-contained enclave without incurring area overhead, while eliminating the need for off-chip encryption. We implement FlashVault at the register-transfer level (RTL) and perform place-and-route (P&amp;amp;R) for accurate power/area evaluation. Our analysis shows that the power budget determines the number of cryptographic engines per NAND chip. We integrate this architectural choice into a full-system simulation and evaluate its performance on a wide range of cryptographic algorithms. Our results show that FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x) and near-core processing architecture (1.02~2.01x), demonstrating its effectiveness as a secure SSD architecture that meets diverse cryptographic requirements imposed by regulatory standards and enterprise policies.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.07252'>Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference</a></h3><h3><a href='https://arxiv.org/pdf/2508.07252' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/27</p><p><b>作者：</b>Siyuan He, Peiran Yan, Yandong He, Youwei Zhuo, Tianyu Jia</p><p>The autoregressive decoding in LLMs is the major inference bottleneck due to the memory-intensive operations and limited hardware bandwidth. 3D-stacked architecture is a promising solution with significantly improved memory bandwidth, which vertically stacked multi DRAM dies on top of logic die. However, our experiments also show the 3D-stacked architecture faces severer thermal issues compared to 2D architecture, in terms of thermal temperature, gradient and scalability. To better exploit the potential of 3D-stacked architecture, we present Tasa, a heterogeneous architecture with cross-stack thermal optimizations to balance the temperature distribution and maximize the performance under the thermal constraints. High-performance core is designed for compute-intensive operations, while high-efficiency core is used for memory-intensive operators, e.g. attention layers. Furthermore, we propose a bandwidth sharing scheduling to improve the bandwidth utilization in such heterogeneous architecture. Extensive thermal experiments show that our Tasa architecture demonstrates greater scalability compared with the homogeneous 3D-stacked architecture, i.e. up to 5.55 $\tccentigrade$, 9.37 $\tccentigrade$, and 7.91 $\tccentigrade$ peak temperature reduction for 48, 60, and 72 core configurations. Our experimental for Llama-65B and GPT-3 66B inferences also demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and state-of-the-art heterogeneous PIM-based LLM accelerator</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15076'>GPU-Initiated Networking for NCCL</a></h3><h3><a href='https://arxiv.org/pdf/2511.15076' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/27</p><p><b>作者：</b>Khaled Hamidouche (NVIDIA Corporation), John Bachan (NVIDIA Corporation), Pak Markthub (NVIDIA Corporation), Peter-Jan Gootzen (NVIDIA Corporation), Elena Agostini (NVIDIA Corporation), Sylvain Jeaugey (NVIDIA Corporation), Aamir Shafi (NVIDIA Corporation), Georgios Theodorakis (NVIDIA Corporation), Manjunath Gorentla Venkata (NVIDIA Corporation)</p><p>Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN&#x27;s practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL&#x27;s unified runtime, combining low-latency operations with NCCL&#x27;s collective algorithms and production infrastructure.</p><p><h4>cs.DC, cs.AI, cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2410.05686'>Deep Learning and Machine Learning with GPGPU and CUDA: Unlocking the Power of Parallel Computing</a></h3><h3><a href='https://arxiv.org/pdf/2410.05686' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/27</p><p><b>作者：</b>Ming Li, Ziqian Bi, Tianyang Wang, Yizhu Wen, Qian Niu, Xinyuan Song, Zekun Jiang, Junyu Liu, Benji Peng, Sen Zhang, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Keyu Chen, Caitlyn Heqi Yin, Pohsun Feng, Ming Liu</p><p>General Purpose Graphics Processing Unit (GPGPU) computing plays a transformative role in deep learning and machine learning by leveraging the computational advantages of parallel processing. Through the power of Compute Unified Device Architecture (CUDA), GPUs enable the efficient execution of complex tasks via massive parallelism. This work explores CPU and GPU architectures, data flow in deep learning, and advanced GPU features, including streams, concurrency, and dynamic parallelism. The applications of GPGPU span scientific computing, machine learning acceleration, real-time rendering, and cryptocurrency mining. This study emphasizes the importance of selecting appropriate parallel architectures, such as GPUs, FPGAs, TPUs, and ASICs, tailored to specific computational tasks and optimizing algorithms for these platforms. Practical examples using popular frameworks such as PyTorch, TensorFlow, and XGBoost demonstrate how to maximize GPU efficiency for training and inference tasks. This resource serves as a comprehensive guide for both beginners and experienced practitioners, offering insights into GPU-based parallel computing and its critical role in advancing machine learning and artificial intelligence.</p><p><h4>cs.DC, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.01497'>SPICEMixer - Netlist-Level Circuit Evolution</a></h3><h3><a href='https://arxiv.org/pdf/2506.01497' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/27</p><p><b>作者：</b>Stefan Uhlich, Andrea Bonetti, Arun Venkitaraman, Chia-Yu Hsieh, Ya\u{g}{\i}z Gen\c{c}er, Mustafa Emre G\&quot;ursoy, Ryoga Matsuo, Lorenzo Servadei</p><p>We present SPICEMixer, a genetic algorithm that synthesizes circuits by directly evolving SPICE netlists. SPICEMixer operates on individual netlist lines, making it compatible with arbitrary components and subcircuits and enabling general-purpose genetic operators: crossover, mutation, and pruning, all applied directly at the netlist level. To support these operators, we normalize each netlist by enforcing consistent net naming (inputs, outputs, supplies, and internal nets) and by sorting components and nets into a fixed order, so that similar circuit structures appear at similar line positions. This normalized netlist format improves the effectiveness of crossover, mutation, and pruning. We demonstrate SPICEMixer by synthesizing standard cells (e.g., NAND2 and latch) and by designing OpAmps that meet specified targets. Across tasks, SPICEMixer matches or exceeds recent synthesis methods while requiring substantially fewer simulations.</p><p><h4>cs.NE, cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.14852'>PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants</a></h3><h3><a href='https://arxiv.org/pdf/2511.14852' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/27</p><p><b>作者：</b>Mingkun Yu, Heming Zhong, Dan Huang, Yutong Lu, Jiazhi Jiang</p><p>Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.14966'>A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2511.14966' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/27</p><p><b>作者：</b>David L. Cole, Jordan Jalving, Jonah Langlieb, Jesse D. Jenkins</p><p>We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.</p><p><h4>cs.DC, cs.MS, math.OC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15361'>BlueBottle: Fast and Robust Blockchains through Subsystem Specialization</a></h3><h3><a href='https://arxiv.org/pdf/2511.15361' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/27</p><p><b>作者：</b>Preston Vander Vos, Alberto Sonnino, Giorgos Tsimos, Philipp Jovanovic, Lefteris Kokoris-Kogias</p><p>Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f&lt;3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.</p><p><h4>cs.DC, cs.CR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15421'>When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit</a></h3><h3><a href='https://arxiv.org/pdf/2511.15421' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/27</p><p><b>作者：</b>Ethan Hicks, Joseph Oglio, Mikhail Nesterenko, Gokarna Sharma</p><p>We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.</p><p><h4>cs.DC, cs.CR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15388'>Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies</a></h3><h3><a href='https://arxiv.org/pdf/2511.15388' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/27</p><p><b>作者：</b>Lucianna Kiffer, Lioba Heimbach, Dennis Trautwein, Yann Vonlanthen, Oliver Gasser</p><p>Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum&#x27;s discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol&#x27;s default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15491'>Proving there is a leader without naming it</a></h3><h3><a href='https://arxiv.org/pdf/2511.15491' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/27</p><p><b>作者：</b>Laurent Feuilloley, Josef Erik Sedl\&#x27;a\v{c}ek, Martin Sl\&#x27;avik</p><p>Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $\Theta(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.10862'>Minimmit: Fast Finality with Even Faster Blocks</a></h3><h3><a href='https://arxiv.org/pdf/2508.10862' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/27</p><p><b>作者：</b>Brendan Kobayashi Chou, Andrew Lewis-Pye, Patrick O&#x27;Grady</p><p>Achieving low-latency consensus in geographically distributed systems remains a key challenge for blockchain and distributed database applications. To this end, there has been significant recent interest in State-Machine-Replication (SMR) protocols that achieve 2-round finality under the assumption that $5f+1\leq n$, where $n$ is the number of processors and $f$ bounds the number of processors that may exhibit Byzantine faults. In these protocols, instructions are organised into views, each led by a different designated leader, and 2-round finality means that a leader&#x27;s proposal can be finalised after just a single round of voting, meaning two rounds overall (one round for the proposal and one for voting).  We introduce Minimmit, a Byzantine-fault-tolerant SMR protocol with lower latency than previous 2-round finality approaches. Our key insight is that view progression and transaction finality can operate on different quorum thresholds without compromising safety or liveness. Experiments simulating a globally distributed network of 50 processors, uniformly assigned across ten virtual regions, show that the approach leads to a 23.1% reduction in view latency and a 10.7% reduction in transaction latency compared to the state-of-the-art.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15031'>GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems</a></h3><h3><a href='https://arxiv.org/pdf/2511.15031' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/27</p><p><b>作者：</b>Yifan Cai, Linh Thi Xuan Phan</p><p>Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.  We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.</p><p><h4>cs.CR, cs.DC, cs.SY, eess.SY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15278'>Privacy-Preserving IoT in Connected Aircraft Cabin</a></h3><h3><a href='https://arxiv.org/pdf/2511.15278' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/27</p><p><b>作者：</b>Nilesh Vyas, Benjamin Zhao, Ayg\&quot;un Baltaci, Gustavo de Carvalho Bertoli, Hassan Asghar, Markus Kl\&quot;ugel, Gerrit Schramm, Martin Kubisch, Dali Kaafar</p><p>The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.</p><p><h4>cs.CR, cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15479'>Towards a Formal Verification of Secure Vehicle Software Updates</a></h3><h3><a href='https://arxiv.org/pdf/2511.15479' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/27</p><p><b>作者：</b>Martin Slind Hagen, Emil Lundqvist, Alex Phu, Yenan Wang, Kim Strandberg, Elad Michael Schiller</p><p>With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF&#x27;s architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF&#x27;s compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.</p><p><h4>cs.CR, cs.DC, cs.LO</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.15517'>Beluga: Block Synchronization for BFT Consensus Protocols</a></h3><h3><a href='https://arxiv.org/pdf/2511.15517' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/27</p><p><b>作者：</b>Tasos Kichidis, Lefteris Kokoris-Kogias, Arun Koshy, Ilya Sergey, Alberto Sonnino, Mingwei Tian, Jianting Zhang</p><p>Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.</p><p><h4>cs.CR, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.11729'>Harli: SLO-Aware Co-location of LLM Inference and PEFT-based Finetuning on Model-as-a-Service Platforms</a></h3><h3><a href='https://arxiv.org/pdf/2511.11729' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/27</p><p><b>作者：</b>Ao Xu, Han Zhao, Weihao Cui, Quan Chen, Yukang Chen, Shulai Zhang, Shuang Chen, Jiemin Jiang, Zhibin Yu, Minyi Guo</p><p>Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2211.12640'>Resource-Constrained Decentralized Federated Learning via Personalized Event-Triggering</a></h3><h3><a href='https://arxiv.org/pdf/2211.12640' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/27</p><p><b>作者：</b>Shahryar Zehtabi, Seyyedali Hosseinalipour, Christopher G. Brinton</p><p>Federated learning (FL) is a popular technique for distributing machine learning (ML) across a set of edge devices. In this paper, we study fully decentralized FL, where in addition to devices conducting training locally, they carry out model aggregations via cooperative consensus formation over device-to-device (D2D) networks. We introduce asynchronous, event-triggered communications among the devices to handle settings where access to a central server is not feasible. To account for the inherent resource heterogeneity and statistical diversity challenges in FL, we define personalized communication triggering conditions at each device that weigh the change in local model parameters against the available local network resources. We theoretically recover the $O(\ln{k} / \sqrt{k})$ convergence rate to the globally optimal model of decentralized gradient descent (DGD) methods in the setup of our methodology. We provide our convergence guarantees for the last iterates of models, under relaxed graph connectivity and data heterogeneity assumptions compared with the existing literature. To do so, we demonstrate a $B$-connected information flow guarantee in the presence of sporadic communications over the time-varying D2D graph. Our subsequent numerical evaluations demonstrate that our methodology obtains substantial improvements in convergence speed and/or communication savings compared to existing decentralized FL baselines.</p><p><h4>cs.LG, cs.DC, math.OC</h4></p></div><hr>
</body>
</html>
