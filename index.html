<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-11-06</h1>
<div><h3><a href='https://arxiv.org/abs/2511.01893'>mLR: Scalable Laminography Reconstruction based on Memoization</a></h3><h3><a href='https://arxiv.org/pdf/2511.01893' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/33</p><p><b>作者：</b>Bin Ma, Viktor Nikitin, Xi Wang, Tekin Bicer, Dong Li</p><p>ADMM-FFT is an iterative method with high reconstruction accuracy for laminography but suffers from excessive computation time and large memory consumption. We introduce mLR, which employs memoization to replace the time-consuming Fast Fourier Transform (FFT) operations based on an unique observation that similar FFT operations appear in iterations of ADMM-FFT. We introduce a series of techniques to make the application of memoization to ADMM-FFT performance-beneficial and scalable. We also introduce variable offloading to save CPU memory and scale ADMM-FFT across GPUs within and across nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of 2Kx2Kx2K, which is the largest input problem laminography reconstruction has ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8% performance improvement on average (up to 65.4%), compared to the original ADMM-FFT.</p><p><h4>cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02043'>Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants</a></h3><h3><a href='https://arxiv.org/pdf/2511.02043' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/33</p><p><b>作者：</b>Bozhi You, Irene Wang, Zelal Su Mustafaoglu, Abhinav Jangda, Ang\&#x27;elica Moreira, Roshan Dathathri, Divya Mahajan, Keshav Pingali</p><p>Bad charactors when submitting to arXiv: Attention is a fundamental building block of large language models (LLMs), so there have been many efforts to implement it efficiently. For example, FlashAttention leverages tiling and kernel fusion to optimize attention. Recently, a number of variants of attention have been introduced to enhance model quality or efficiency. Supporting them efficiently remains difficult since they usually require specialized kernels or hand-tuned implementations. FlexAttention recently addressed part of this gap by using static programming templates to support FlashAttention-like kernels for a subset of attention variants.  In this paper, we introduce Flashlight, a compiler-native framework within the PyTorch ecosystem that automatically generates fused, FlashAttention-style kernels for arbitrary attention-based programs, without relying on static templates or predefined kernel specializations. Flashlight leverages PyTorch&#x27;s compilation workflow to fuse and tile attention computations transparently, enabling efficient execution for diverse attention patterns. Not only does it support all variants expressible in the FlexAttention model but it also handles more general, data-dependent attention formulations that are beyond the capabilities of FlexAttention.  Our results show that Flashlight produces kernels with competitive or superior performance to FlexAttention, while offering the flexibility of native PyTorch code, enabling developers to rapidly explore new attention models without sacrificing performance.</p><p><h4>cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02132'>Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects</a></h3><h3><a href='https://arxiv.org/pdf/2511.02132' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/33</p><p><b>作者：</b>Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika</p><p>The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD&#x27;s MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.</p><p><h4>cs.AR, cs.DC, cs.LG, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02196'>BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction</a></h3><h3><a href='https://arxiv.org/pdf/2511.02196' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/33</p><p><b>作者：</b>Liwei Ni, Jiaxi Zhang, Shenggen Zheng, Junfeng Liu, Xingyu Meng, Biwei Xie, Xingquan Li, Huawei Li</p><p>Boolean equivalence allows Boolean networks with identical functionality to exhibit diverse graph structures. This gives more room for exploration in logic optimization, while also posing a challenge for tasks involving consistency between Boolean networks. To tackle this challenge, we introduce BoolSkeleton, a novel Boolean network skeletonization method that improves the consistency and reliability of design-specific evaluations. BoolSkeleton comprises two key steps: preprocessing and reduction. In preprocessing, the Boolean network is transformed into a defined Boolean dependency graph, where nodes are assigned the functionality-related status. Next, the homogeneous and heterogeneous patterns are defined for the node-level pattern reduction step. Heterogeneous patterns are preserved to maintain critical functionality-related dependencies, while homogeneous patterns can be reduced. Parameter K of the pattern further constrains the fanin size of these patterns, enabling fine-tuned control over the granularity of graph reduction. To validate BoolSkeleton&#x27;s effectiveness, we conducted four analysis/downstream tasks around the Boolean network: compression analysis, classification, critical path analysis, and timing prediction, demonstrating its robustness across diverse scenarios. Furthermore, it improves above 55% in the average accuracy compared to the original Boolean network for the timing prediction task. These experiments underscore the potential of BoolSkeleton to enhance design consistency in logic synthesis.</p><p><h4>cs.AR, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02269'>Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA</a></h3><h3><a href='https://arxiv.org/pdf/2511.02269' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/33</p><p><b>作者：</b>Takuto Ando, Yu Eto, Ayumu Takeuchi, Yasuhiko Nakashima</p><p>The rise of generative AI for tasks like Automatic Speech Recognition (ASR) has created a critical energy consumption challenge. While ASICs offer high efficiency, they lack the programmability to adapt to evolving algorithms. To address this trade-off, we implement and evaluate Whisper&#x27;s core computational kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs) accelerator. To our knowledge, this is the first work to execute a Whisper kernel on a CGRA and compare its performance against CPUs and GPUs. Using hardware/software co-design, we evaluate our system via an FPGA prototype and project performance for a 28 nm ASIC. Our results demonstrate superior energy efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This work positions CGLA as a promising platform for sustainable ASR on power-constrained edge devices.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02408'>Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA</a></h3><h3><a href='https://arxiv.org/pdf/2511.02408' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/33</p><p><b>作者：</b>Takuto Ando, Yusuke Inoue</p><p>In this paper, we implement a stand-alone facial expression recognition system on an SoC FPGA with multi-threading using a Deep learning Processor Unit (DPU). The system consists of two steps: one for face detection step and one for facial expression recognition. In the previous work, the Haar Cascade detector was run on a CPU in the face detection step due to FPGA resource limitations, but this detector is less accurate for profile and variable illumination condition images. Moreover, the previous work used a dedicated circuit accelerator, so running a second DNN inference for face detection on the FPGA would require the addition of a new accelerator. As an alternative to this approach, we run the two inferences by DNN on a DPU, which is a general-purpose CNN accelerator of the systolic array type. Our method for face detection using DenseBox and facial expression recognition using CNN on the same DPU enables the efficient use of FPGA resources while maintaining a small circuit size. We also developed a multi-threading technique that improves the overall throughput while increasing the DPU utilization efficiency. With this approach, we achieved an overall system throughput of 25 FPS and a throughput per power consumption of 2.4 times.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02494'>Digit-Recurrence Posit Division</a></h3><h3><a href='https://arxiv.org/pdf/2511.02494' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/33</p><p><b>作者：</b>Raul Murillo, Julio Villalba-Moreno, Alberto A. Del Barrio, Guillermo Botella</p><p>Posit arithmetic has emerged as a promising alternative to IEEE 754 floating-point representation, offering enhanced accuracy and dynamic range. However, division operations in posit systems remain challenging due to their inherent hardware complexity. In this work, we present posit division units based on the digit-recurrence algorithm, marking the first implementation of radix-4 digit-recurrence techniques within this context. Our approach incorporates hardware-centric optimizations including redundant arithmetic, on-the-fly quotient conversion, and operand scaling to streamline the division process while mitigating latency, area, and power overheads. Comprehensive synthesis evaluations across multiple posit configurations demonstrate significant performance improvements, including more than 80% energy reduction with small area overhead compared to existing methods, and a substantial decrease in the number of iterations. These results underscore the potential of our adapted algorithm to enhance the efficiency of posit-based arithmetic units.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02530'>Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator</a></h3><h3><a href='https://arxiv.org/pdf/2511.02530' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/33</p><p><b>作者：</b>Takuto Ando, Yu Eto, Yasuhiko Nakashima</p><p>This paper presents the first implementation and in-depth evaluation of the primary computational kernels from the stable-diffusion.cpp image generation framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array (CGRA) accelerator. We designed IMAX3 as a versatile computational platform, and this work assesses its capabilities by executing a demanding image generation workload. We evaluate its performance on a current Field-Programmable Gate Array (FPGA) prototype to establish a baseline and project its potential for a future Application-Specific Integrated Circuit (ASIC) implementation. Our results demonstrate that, despite its general-purpose architecture, IMAX3 achieves promising performance and power efficiency, particularly in its projected ASIC form. This work provides concrete guidelines for future IMAX architectural designs and establishes a foundation for developing next-generation, AI-specialized Coarse-Grained Linear Array (CGLA) accelerators by refining this versatile platform. Ultimately, this achievement contributes to the realization of energy-efficient, on-device, multi-modal AI platforms.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02285'>VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning</a></h3><h3><a href='https://arxiv.org/pdf/2511.02285' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/33</p><p><b>作者：</b>Zhuorui Zhao, Bing Li, Grace Li Zhang, Ulf Schlichtmann</p><p>Large Language Models (LLMs) have shown impressive potential in generating Verilog codes, but ensuring functional correctness remains a challenge. Existing approaches often rely on self-consistency or simulation feedback to select the best candidate, but they miss opportunities to focus LLM reasoning on the most informative parts of the design. We propose VFocus, a three-stage framework that enhances Verilog generation by sharpening the focus of LLM reasoning onto critical decision points in the code generation process. In the \textbf{pre-ranking stage}, VFocus generates multiple code candidates through LLM prompting, retries for syntactically valid outputs, and introduces a \textit{Density-guided Filtering} to retain candidates that fall within the &quot;reasoning sweet spot&quot; for functional correctness. In the \textbf{ranking stage}, we simulate each code candidate using an automatically generated testbench and apply self-consistency-based clustering to identify the most consistent outputs. Finally, in the \textbf{post-ranking refinement stage}, VFocus performs inconsistency mining on top-ranked candidates and invokes reasoning-augmented LLM prompts for candidate refinement. Experiments on the VerilogEval-Human benchmark show that VFocus significantly improves the pass@1 correctness across multiple reasoning LLMs, demonstrating its effectiveness in enhancing Verilog generation for complex hardware design tasks.</p><p><h4>cs.AR, cs.PL, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01866'>EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs</a></h3><h3><a href='https://arxiv.org/pdf/2511.01866' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/33</p><p><b>作者：</b>Benjamin Kubwimana, Qijing Huang</p><p>Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.</p><p><h4>cs.DC, cs.AI, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2503.04426'>FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN Inference</a></h3><h3><a href='https://arxiv.org/pdf/2503.04426' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/33</p><p><b>作者：</b>Natalia Cherezova, Artur Jutman, Maksim Jenihhin</p><p>The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical applications brings their reliability to the front. High performance demands of DNNs require the use of specialized hardware accelerators. Systolic array architecture is widely used in DNN accelerators due to its parallelism and regular structure. This work presents a run-time reconfigurable systolic array architecture with three execution modes and four implementation options. All four implementations are evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous mapping of different network layers to different execution modes. The approach is supported by a novel reliability assessment method based on fault propagation analysis. It is used for the exploration of the appropriate execution mode--layer mapping for DNN inference. The proposed architecture efficiently protects registers and MAC units of systolic array PEs from transient and permanent faults. The reconfigurability feature enables a speedup of up to $3\times$, depending on layer vulnerability. Furthermore, it requires $6\times$ fewer resources compared to static redundancy and $2.5\times$ fewer resources compared to the previously proposed solution for transient faults.</p><p><h4>cs.AR, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.04266'>CPU-Based Layout Design for Picker-to-Parts Pallet Warehouses</a></h3><h3><a href='https://arxiv.org/pdf/2506.04266' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/33</p><p><b>作者：</b>Timo Looms, Lin Xie</p><p>Picker-to-parts pallet warehouses often face inefficiencies due to conventional layouts causing excessive travel distances and high labor requirements. This study introduces a novel layout design inspired by CPU architecture, partitioning warehouse space into specialized zones, namely Performance (P), Efficiency (E), and Shared (S). Discrete-event simulation is used to evaluate this design against traditional rectangular (random and ABC storage) and Flying-V layouts. Results demonstrate significant improvements in throughput time and reduced labor requirements, highlighting the potential for CPU-based layouts in optimizing warehouse operations.</p><p><h4>cs.MA, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2506.11225'>Controlling quantum chaos via Parrondo strategies on noisy intermediate-scale quantum hardware</a></h3><h3><a href='https://arxiv.org/pdf/2506.11225' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/33</p><p><b>作者：</b>Aditi Rath, Dinesh Kumar Panda, Colin Benjamin</p><p>Advancements in Noisy Intermediate-Scale Quantum (NISQ) computing are steadily pushing these systems toward outperforming classical supercomputers on specific, well-defined computational tasks. In this work, we explore and control quantum chaos in NISQ systems using discrete-time quantum walks (DTQW) on cyclic graphs. To efficiently implement quantum walks on NISQ hardware, we employ the quantum Fourier transform (QFT) to diagonalize the conditional shift operator, optimizing circuit depth and fidelity. We experimentally realize the transition from quantum chaos to order via DTQW dynamics on both odd and even cyclic graphs, specifically 3- and 4-cycle graphs, using the counterintuitive Parrondo&#x27;s paradox strategy across three different NISQ devices. While the 4-cycle graphs exhibit high-fidelity quantum evolution, the 3-cycle implementation shows significant fidelity improvement when augmented with dynamical decoupling pulses. Our results demonstrate a practical approach to probing and harnessing controlled chaotic dynamics on real quantum hardware, laying the groundwork for future quantum algorithms and cryptographic protocols based on quantum walks.</p><p><h4>quant-ph, cond-mat.dis-nn, cs.AR, nlin.CD</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01860'>A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks</a></h3><h3><a href='https://arxiv.org/pdf/2511.01860' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/33</p><p><b>作者：</b>Leszek Sliwko</p><p>This review analyzes deployed and actively used workload schedulers&#x27; solutions and presents a taxonomy in which those systems are divided into several hierarchical groups based on their architecture and design. While other taxonomies do exist, this review has focused on the key design factors that affect the throughput and scalability of a given solution, as well as the incremental improvements which bettered such an architecture. This review gives special attention to Google&#x27;s Borg, which is one of the most advanced and published systems of this kind.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01861'>Conceptual Design Report for FAIR Computing</a></h3><h3><a href='https://arxiv.org/pdf/2511.01861' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/33</p><p><b>作者：</b>Johan Messchendorp, Mohammad Al-Turany, Volker Friese, Thorsten Kollegger, Bastian Loeher, Jochen Markert, Andrew Mistry, Thomas Neff, Adrian Oeftiger, Michael Papenbrock, Stephane Pietri, Shahab Sanjari, Tobias Stockmanns</p><p>This Conceptual Design Report (CDR) presents the plans of the computing infrastructure for research at FAIR, Darmstadt, Germany. It presents the computing requirements of the various research groups, the policies for the computing and storage infrastructure, the foreseen FAIR computing model including the open data, software and services policies and architecture for the periods starting in 2028 with the &quot;first science (plus)&quot; phase to the modularized start version of FAIR. The overall ambition is to create a federated and centrally-orchestrated infrastructure serving the large diversity of the research lines present with sufficient scalability and flexibility to cope with future data challenges that will be present at FAIR.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01871'>Structural Analysis of Multi-Core Processor and Reliability Evaluation Model</a></h3><h3><a href='https://arxiv.org/pdf/2511.01871' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/33</p><p><b>作者：</b>S. Tsiramua, H. Meladze, T. Davitashvili, J. M. Sanchez, F. Criado-Aldeanueva</p><p>In the present paper, the models of structural analysis and evaluation of efficiency indicators (reliability, fault tolerance, viability, and flexibility) of a multi core processor with variable structure, equipped with multi functional cores, are considered. Using logical probabilistic methods, the following has been developed: models for evaluating the reliability and fault tolerance of processor cores as multi functional elements; logical probabilistic models of the shortest paths, flexibility, and performance conditions for successful operation of multi core processors based on multi functional cores; and models for estimating the reliability, fault tolerance, and lifetime of multi core processors considering all possible states of performance. The results of the structural analysis of two core and four core processors and the trends of increasing the efficiency indicators of multi core processors are presented.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01881'>HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing</a></h3><h3><a href='https://arxiv.org/pdf/2511.01881' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/33</p><p><b>作者：</b>Zhengxin Fang, Hui Ma, Gang Chen, Rajkumar Buyya</p><p>Microservice architecture has become a dominant paradigm in application development due to its advantages of being lightweight, flexible, and resilient. Deploying microservice applications in the container-based cloud enables fine-grained elastic resource allocation. Autoscaling is an effective approach to dynamically adjust the resource provisioned to containers. However, the intricate microservice dependencies and the deployment scheme of the container-based cloud bring extra challenges of resource scaling. This article proposes a novel autoscaling approach named HGraphScale. In particular, HGraphScale captures microservice dependencies and the deployment scheme by a newly designed hierarchical graph neural network, and makes effective scaling actions for rapidly changing user requests workloads. Extensive experiments based on real-world traces of user requests are conducted to evaluate the effectiveness of HGraphScale. The experiment results show that the HGraphScale outperforms existing state-of-the-art autoscaling approaches by reducing at most 80.16\% of the average response time under a certain VM rental budget of application providers.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01888'>Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions</a></h3><h3><a href='https://arxiv.org/pdf/2511.01888' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/33</p><p><b>作者：</b>Cynthia Marcelino, Thomas Pusztai, Stefan Nastic</p><p>Serverless computing provides infrastructure management and elastic auto-scaling, therefore reducing operational overhead. By design serverless functions are stateless, which means they typically leverage external remote services to store and exchange data. Transferring data over a network typically involves serialization and deserialization. These operations usually require multiple data copies and transitions between user and kernel space, resulting in overhead from context switching and memory allocation, contributing significantly to increased latency and resource consumption. To address these issues, we present Roadrunner, a sidecar shim that enables near-zero copy and serialization-free data transfer between WebAssembly-based serverless functions. Roadrunner reduces the multiple copies between user space and kernel space by mapping the function memory and moving the data along a dedicated virtual data hose, bypassing the costly processes of serialization and deserialization. This approach reduces data movement overhead and context switching, achieving near-native latency performance for WebAssembly-based serverless functions. Our experimental results demonstrate that Roadrunner significantly improves the inter-function communication latency from 44% up to 89%, reducing the serialization overhead in 97% of data transfer, and increasing throughput by 69 times compared to state-of-the-art WebAssembly-based serverless functions.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02257'>Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators</a></h3><h3><a href='https://arxiv.org/pdf/2511.02257' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/33</p><p><b>作者：</b>Oguz Selvitopi, Emin Ozturk, Jie Chen, Ponnuswamy Sadayappan, Robert G. Edwards, Ayd{\i}n Bulu\c{c}</p><p>Computation of correlation functions is a key operation in Lattice quantum chromodynamics (LQCD) simulations to extract nuclear physics observables. These functions involve many binary batch tensor contractions, each tensor possibly occupying hundreds of MBs of memory. Performing these contractions on GPU accelerators poses the challenge of scheduling them as to optimize tensor reuse and reduce data traffic. In this work we propose two fast novel scheduling algorithms that reorder contractions to increase temporal locality via input/intermediate tensor reuse. Our schedulers take advantage of application-specific features, such as contractions being binary and locality within contraction trees, to optimize the objective of minimizing peak memory. We integrate them into the LQCD analysis software suite Redstar and improve time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory, which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data traffic, resulting in upto 1.9x faster correlation function computation time.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02743'>Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)</a></h3><h3><a href='https://arxiv.org/pdf/2511.02743' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/33</p><p><b>作者：</b>Fedor Ryabinin, Alexey Gotsman, Pierre Sutra</p><p>Classical state-machine replication protocols, such as Paxos, rely on a distinguished leader process to order commands. Unfortunately, this approach makes the leader a single point of failure and increases the latency for clients that are not co-located with it. As a response to these drawbacks, Egalitarian Paxos introduced an alternative, leaderless approach, that allows replicas to order commands collaboratively. Not relying on a single leader allows the protocol to maintain non-zero throughput with up to $f$ crashes of any processes out of a total of $n = 2f+1$. The protocol furthermore allows any process to execute a command $c$ fast, in $2$ message delays, provided no more than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently submitted commands commute with $c$; the latter condition is often satisfied in practical systems.  Egalitarian Paxos has served as a foundation for many other replication protocols. But unfortunately, the protocol is very complex, ambiguously specified and suffers from nontrivial bugs. In this paper, we present EPaxos* -- a simpler and correct variant of Egalitarian Paxos. Our key technical contribution is a simpler failure-recovery algorithm, which we have rigorously proved correct. Our protocol also generalizes Egalitarian Paxos to cover the whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1, 2f+1\}$ -- the number of processes that we show to be optimal.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01862'>Possible Futures for Cloud Cost Models</a></h3><h3><a href='https://arxiv.org/pdf/2511.01862' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/33</p><p><b>作者：</b>Vanessa Sochat, Daniel Milroy</p><p>Cloud is now the leading software and computing hardware innovator, and is changing the landscape of compute to one that is optimized for artificial intelligence and machine learning (AI/ML). Computing innovation was initially driven to meet the needs of scientific computing. As industry and consumer usage of computing proliferated, there was a shift to satisfy a multipolar customer base. Demand for AI/ML now dominates modern computing and innovation has centralized on cloud. As a result, cost and resource models designed to serve AI/ML use cases are not currently well suited for science. If resource contention resulting from a unipole consumer makes access to contended resources harder for scientific users, a likely future is running scientific workloads where they were not intended. In this article, we discuss the past, current, and possible futures of cloud cost models for the continued support of discovery and science.</p><p><h4>cs.DC, cs.CY</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01863'>SPHERE: Spherical partitioning for large-scale routing optimization</a></h3><h3><a href='https://arxiv.org/pdf/2511.01863' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/33</p><p><b>作者：</b>Robert Fabian Lindermann, Paul-Niklas Ken Kandora, Simon Caspar Zeller, Adrian Asmund Fessler, Steffen Rebennack</p><p>We study shortest-path routing in large weighted, undirected graphs, where expanding search frontiers raise time and memory costs for exact solvers. We propose \emph{SPHERE}, a source-target-aware heuristic that identifies an $s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count. Selecting an anchor $a$ in this overlap partitions the task into two subproblems with unchanged problem-topology, $s\to a$ and $a\to t$; if either remains large, the procedure recurses on its induced subgraph. Because the cut lies inside the overlap, concatenating the resulting subpaths yields a valid $s\to t$ route without boundary repair. SPHERE is independent of the downstream solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large networks, it achieves faster runtimes and smaller optimality gaps than Louvain-based routing and a METIS-based pipeline, even on graphs with more than a million nodes and edges, while also outperforming Dijkstra in runtime.</p><p><h4>cs.DC, cs.DM</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01872'>Learned Cost Model for Placement on Reconfigurable Dataflow Hardware</a></h3><h3><a href='https://arxiv.org/pdf/2511.01872' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/33</p><p><b>作者：</b>Etash Guha, Tianxiao Jiang, Andrew Deng, Jian Zhang, Muthu Annamalai</p><p>Mapping a dataflow-graph of an ML model onto a reconfigurable system is difficult, as different mappings have different throughputs and consume resource constraints differently. To solve this, a model to evaluate the throughput of mappings is necessary as measuring throughput completely is expensive. Many use a hand-designed analytical model, relying on proxy features or intuition, introducing error. We provide a Learned Approach that predicts throughput 31%-52% more accurately over a variety of graphs. In addition, our approach shows no accuracy degradation after removing performance annotations. We show that using this approach results in 5.6% faster compiled graphs.</p><p><h4>cs.DC, cs.LG, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02034'>GPoS: Geospatially-aware Proof of Stake</a></h3><h3><a href='https://arxiv.org/pdf/2511.02034' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/33</p><p><b>作者：</b>Shashank Motepalli, Naman Garg, Gengrui Zhang, Hans-Arno Jacobsen</p><p>Geospatial decentralization is essential for blockchains, ensuring regulatory resilience, robustness, and fairness. We empirically analyze five major Proof of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui, revealing that a few geographic regions dominate consensus voting power, resulting in limited geospatial decentralization. To address this, we propose Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity with stake-based voting power. Experimental evaluation demonstrates an average 45% improvement in geospatial decentralization, as measured by the Gini coefficient of Eigenvector centrality, while incurring minimal performance overhead in BFT protocols, including HotStuff and CometBFT. These results demonstrate that GPoS can improve geospatial decentralization {while, in our experiments, incurring minimal overhead} to consensus performance.</p><p><h4>cs.DC, cs.ET, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02168'>Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs</a></h3><h3><a href='https://arxiv.org/pdf/2511.02168' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/33</p><p><b>作者：</b>Octavian Alexandru Trifan, Karthik Sangaiah, Muhammad Awad, Muhammad Osama, Sumanth Gudaparthi, Alexandru Nicolau, Alexander Veidenbaum, Ganesh Dasika</p><p>As large language models (LLMs) continue to scale, their workloads increasingly rely on distributed execution across multiple GPUs. However, the conventional bulk synchronous parallel~(BSP) model used in such settings introduces significant performance inefficiencies. To characterize these bottlenecks, we introduce the &#x27;&#x27;Three Taxes&#x27;&#x27; (Bulk Synchronous, Inter-Kernel Data Locality, and Kernel Launch Overhead) as an analytical framework. We propose moving beyond the rigid BSP model to address key inefficiencies in distributed GPU execution. By exploiting libraries like Iris for Triton, we gain access to in-kernel communication primitives that enable the design of novel fine-grained programming patterns, offering greater flexibility and performance than traditional BSP-based approaches. These patterns systematically eliminate the three taxes by creating direct, tile-level producer-consumer pipelines and replacing global barriers with fine-grained dataflow synchronization. Applying this methodology to critical kernels, from the foundational All-Gather + general matrix multiplication operation to the complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end latency over BSP-based approaches, establishing a more programmable and efficient paradigm for distributed LLM workloads.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02248'>From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models</a></h3><h3><a href='https://arxiv.org/pdf/2511.02248' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/33</p><p><b>作者：</b>Xingqi Cui, Chieh-Jan Mike Liang, Jiarong Xing, Haoran Qiu</p><p>Serving large generative models such as LLMs and multi- modal transformers requires balancing user-facing SLOs (e.g., time-to-first-token, time-between-tokens) with provider goals of efficiency and cost reduction. Existing solutions rely on static provisioning or model-level autoscaling, both of which treat the model as a monolith. This coarse-grained resource management leads to degraded performance or significant resource underutilization due to poor adaptability to dynamic inference traffic that is common online.  The root cause of this inefficiency lies in the internal structure of generative models: they are executed as graphs of interconnected operators. Through detailed characterization and systematic analysis, we find that operators are heterogeneous in their compute and memory footprints and exhibit diverse sensitivity to workload and resource factors such as batch size, sequence length, and traffic rate. This heterogeneity suggests that the operator, rather than the entire model, is the right granularity for scaling decisions.  We propose an operator-level autoscaling framework, which allocates resources at finer (operator)-granularity, optimizing the scaling, batching, and placement based on individual operator profiles. Evaluated on production-scale traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less energy, or under fixed resources achieves 1.6x higher throughput with 5% less energy. These results show that the operator, rather than the model, is fundamentally a more effective unit for scaling large generative workloads.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02293'>3D Point Cloud Object Detection on Edge Devices for Split Computing</a></h3><h3><a href='https://arxiv.org/pdf/2511.02293' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/33</p><p><b>作者：</b>Taisuke Noguchi, Takuya Azumi</p><p>The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</p><p><h4>cs.DC, cs.CV</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02647'>Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks</a></h3><h3><a href='https://arxiv.org/pdf/2511.02647' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/33</p><p><b>作者：</b>Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor</p><p>Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn&#x27;s potential to deliver scalability and efficiency in real-world edge deployments.</p><p><h4>cs.DC, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02655'>Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks</a></h3><h3><a href='https://arxiv.org/pdf/2511.02655' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/33</p><p><b>作者：</b>Johansell Villalobos, Josef Ruzicka, Silvio Rizzi</p><p>Scientific computing in the exascale era demands increased computational power to solve complex problems across various domains. With the rise of heterogeneous computing architectures the need for vendor-agnostic, performance portability frameworks has been highlighted. Libraries like Kokkos have become essential for enabling high-performance computing applications to execute efficiently across different hardware platforms with minimal code changes. In this direction, this paper presents preliminary time-to-solution results for two representative scientific computing applications: an N-body simulation and a structured grid simulation. Both applications used a distributed memory approach and hardware acceleration through four performance portability frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed significant performance variability among frameworks. OCCA demonstrated faster execution times for small-scale validation problems, likely due to JIT compilation, however its lack of optimized reduction algorithms may limit scalability for larger simulations while using its out of the box API. OpenMP performed poorly in the structured grid simulation most likely due to inefficiencies in inter-node data synchronization and communication. These findings highlight the need for further optimization to maximize each framework&#x27;s capabilities. Future work will focus on enhancing reduction algorithms, data communication, memory management, as wells as performing scalability studies, and a comprehensive statistical analysis to evaluate and compare framework performance.</p><p><h4>cs.DC, cs.MS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.01884'>CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2511.01884' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>30/33</p><p><b>作者：</b>Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding</p><p>Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\% correctness of generated kernels and an average 1.68$\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge</p><p><h4>cs.LG, cs.AI, cs.CL, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02029'>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</a></h3><h3><a href='https://arxiv.org/pdf/2511.02029' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>31/33</p><p><b>作者：</b>Duc A. Tran, Dung Truong, Duy Le</p><p>Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%</p><p><h4>cs.LG, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2511.02230'>Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live</a></h3><h3><a href='https://arxiv.org/pdf/2511.02230' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>32/33</p><p><b>作者：</b>Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen, Alvin Cheung, Joseph Gonzalez, Ion Stoica</p><p>Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum</p><p><h4>cs.OS, cs.AI, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2508.13652'>Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles</a></h3><h3><a href='https://arxiv.org/pdf/2508.13652' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>33/33</p><p><b>作者：</b>L\&#x27;or\&#x27;ant Meszl\&#x27;enyi, Julius Kahle, Dominik P\&quot;ullen, Stefan Kowalewski, Stefan Katzenbeisser, Alexandru Kampmann</p><p>As the automotive industry transitions toward centralized Linux-based architectures, ensuring the predictable execution of mixed-criticality applications becomes essential. However, concurrent use of the Linux network stack introduces interference, resulting in unpredictable latency and jitter. To address this challenge, we present a layered software architecture that enforces timing isolation for Ethernet-based data exchange between mixed-criticality applications on Linux-based automotive control units. Our approach integrates traffic prioritization strategies at the middleware layer, the network stack layer, and the hardware layer to achieve isolation across the full software stack. At the middleware layer, we implement a fixed-priority, non-preemptive scheduler to manage publishers of varying criticality. At the network layer, we leverage the express data path (XDP) to route high-priority data directly from the network interface driver into critical application memory, bypassing the standard Linux network stack. At the hardware layer, we dedicate a network interface card (NIC) queue exclusively to real-time traffic. We demonstrate how our architecture performs in a Data Distribution Service (DDS)-based system. Our evaluation shows that the approach leads to consistent and predictable latencies for real-time traffic, even under heavy interference from best-effort applications.</p><p><h4>cs.NI, cs.OS</h4></p></div><hr>
</body>
</html>
