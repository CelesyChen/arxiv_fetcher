<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>ArXiv 最新论文列表</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; }
    h1, h2, h3 { color: #333; }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>2025-10-23</h1>
<div><h3><a href='https://arxiv.org/abs/2510.19765'>Tidying Up the Address Space</a></h3><h3><a href='https://arxiv.org/pdf/2510.19765' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>1/29</p><p><b>作者：</b>Vinay Banakar, Suli Yang, Kan Wu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Kimberly Keeton</p><p>Memory tiering in datacenters does not achieve its full potential due to hotness fragmentation -- the intermingling of hot and cold objects within memory pages. This fragmentation prevents page-based reclamation systems from distinguishing truly hot pages from pages containing mostly cold objects, fundamentally limiting memory efficiency despite highly skewed accesses. We introduce address-space engineering: dynamically reorganizing application virtual address spaces to create uniformly hot and cold regions that any page-level tiering backend can manage effectively. HADES demonstrates this frontend/backend approach through a compiler-runtime system that tracks and migrates objects based on access patterns, requiring minimal developer intervention. Evaluations across ten data structures achieve up to 70% memory reduction with 3% performance overhead, showing that address space engineering enables existing reclamation systems to reclaim memory aggressively without performance degradation.</p><p><h4>cs.OS, cs.PF, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19783'>On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers</a></h3><h3><a href='https://arxiv.org/pdf/2510.19783' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>2/29</p><p><b>作者：</b>Miguel S\&#x27;anchez de la Rosa, Francisco J. and\&#x27;ujar, Jesus Escudero-Sahuquillo, Jos\&#x27;e L. S\&#x27;anchez, Francisco J. Alfaro-Cort\&#x27;es</p><p>The increase in computation and storage has led to a significant growth in the scale of systems powering applications and services, raising concerns about sustainability and operational costs. In this paper, we explore power-saving techniques in high-performance computing (HPC) and datacenter networks, and their relation with performance degradation. From this premise, we propose leveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to conventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI and Omnipath.  We analyze the PerfBound proposal, identifying possible improvements and modeling it into a simulation framework. Through different experiments, we examine its impact on performance and determine the most appropriate interconnect. We also study traffic patterns generated by selected HPC and machine learning applications to evaluate the behavior of power-saving techniques.  From these experiments, we provide an analysis of how applications affect system and network energy consumption. Based on this, we disclose the weakness of dynamic power-down mechanisms and propose an approach that improves energy reduction with minimal or no performance penalty. To our knowledge, this is the first power management proposal tailored to future Ethernet-based HPC architectures, with promising results.</p><p><h4>cs.NI, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2408.08129'>Efficient and scalable atmospheric dynamics simulations using non-conforming meshes</a></h3><h3><a href='https://arxiv.org/pdf/2408.08129' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>3/29</p><p><b>作者：</b>Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura</p><p>We present the massively parallel performance of a $h$-adaptive solver for atmosphere dynamics that allows for non-conforming mesh refinement. The numerical method is based on a Discontinuous Galerkin (DG) spatial discretization, highly scalable thanks to its data locality properties, and on a second order Implicit-Explicit Runge-Kutta (IMEX-RK) method for time discretization, particularly well suited for low Mach number flows. Simulations with non-conforming meshes for flows over orography can increase the accuracy of the local flow description without affecting the larger scales, which can be solved on coarser meshes. We show that the local refining procedure has no significant impact on the parallel performance and, therefore, both efficiency and scalability can be achieved in this framework.</p><p><h4>math.NA, cs.DC, cs.NA, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.08206'>Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency</a></h3><h3><a href='https://arxiv.org/pdf/2502.08206' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>4/29</p><p><b>作者：</b>Abdelkrim Alahyane (LAAS-SARA, LAAS), C\&#x27;eline Comte (CNRS, LAAS-SARA, LAAS), Matthieu Jonckheere (CNRS, LAAS-SARA, LAAS), \&#x27;Eric Moulines (X)</p><p>Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling and analysis to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. Compared to most existing studies, we account for the joint impact of heterogeneous and variable service speeds and heterogeneous datasets at the clients. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little&#x27;s law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date, using the upper-bound of Leconte et al. as a proxy. Second, we observe that naively optimizing this metric drastically slows down the system by overemphasizing staleness at the expense of throughput. This motivates us to introduce an alternative metric that also accounts for speed, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show these optimizations enhance accuracy by 10% to 30%.</p><p><h4>cs.LG, cs.PF, math.OC, math.PR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.23819'>Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using $\mathbb{F}_2$</a></h3><h3><a href='https://arxiv.org/pdf/2505.23819' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>5/29</p><p><b>作者：</b>Keren Zhou, Mario Lezcano, Adam Goucher, Akhmed Rakhmati, Jeff Niu, Justin Lebar, Pawel Szczerbuk, Peter Bell, Phil Tillet, Thomas Raoux, Zahi Moudallal</p><p>Efficient tensor computation is a cornerstone of modern deep learning (DL) workloads, yet existing approaches struggle to achieve flexible and performant design and implementation of tensor layouts -- mappings between logical tensors and hardware resources. The increasing complexity of DL algorithms and hardware demands a generic and systematic approach to handling tensor layouts. In this work, we introduce Linear Layouts, a novel approach that models tensor layouts using linear algebra over $\mathbb{F}_2$. By representing tensor layouts as binary matrices acting on the bits of the hardware representation, our approach enables a generic layout definition -- as opposed to the classical case-by-case approach -- and allows for generic layout-to-layout conversions, eliminating the quadratic explosion that plagues existing solutions. We integrate linear layouts with Triton and demonstrate their effectiveness in optimizing individual Triton operators as well as kernels written in Triton. We also show that linear layouts reduce engineering effort in the compiler backend while fixing several bugs in Triton&#x27;s legacy layout system.</p><p><h4>cs.PL, cs.AR, cs.DC, cs.PF</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19260'>Res-DPU: Resource-shared Digital Processing-in-memory Unit for Edge-AI Workloads</a></h3><h3><a href='https://arxiv.org/pdf/2510.19260' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>6/29</p><p><b>作者：</b>Mukul Lokhande, Narendra Singh Dhakad, Seema Chouhan, Akash Sankhe, Santosh Kumar Vishvakarma</p><p>Processing-in-memory (PIM) has emerged as the go to solution for addressing the von Neumann bottleneck in edge AI accelerators. However, state-of-the-art (SoTA) digital PIM approaches suffer from low compute density, primarily due to the use of bulky bit cells and transistor-heavy adder trees, which impose limitations on macro scalability and energy efficiency. This work introduces Res-DPU, a resource-shared digital PIM unit, with a dual-port 5T SRAM latch and shared 2T AND compute logic. This reflects the per-bit multiplication cost to just 5.25T and reduced the transistor count of the PIM array by up to 56% over the SoTA works. Furthermore, a Transistor-Reduced 2D Interspersed Adder Tree (TRAIT) with FA-7T and PG-FA-26T helps reduce the power consumption of the adder tree by up to 21.35% and leads to improved energy efficiency by 59% compared to conventional 28T RCA designs. We propose a Cycle-controlled Iterative Approximate-Accurate Multiplication (CIA2M) approach, enabling run-time accuracy-latency trade-offs without requiring error-correction circuitry. The 16 KB REP-DPIM macro achieves 0.43 TOPS throughput and 87.22 TOPS/W energy efficiency in TSMC 65nm CMOS, with 96.85% QoR for ResNet-18 or VGG-16 on CIFAR-10, including 30% pruning. The proposed results establish a Res-DPU module for highly scalable and energy-efficient real-time edge AI accelerators.</p><p><h4>cs.AR, cs.ET, eess.IV</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19577'>gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration</a></h3><h3><a href='https://arxiv.org/pdf/2510.19577' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>7/29</p><p><b>作者：</b>Zuoming Fu, Alex Manley, Mohammad Alian</p><p>Generative AI is increasing the productivity of software and hardware development across many application domains. In this work, we utilize the power of Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5 users with automating design space exploration. Computer architecture design space exploration is complex and time-consuming, given that numerous parameter settings and simulation statistics must be analyzed before improving the current design. The emergence of LLMs has significantly accelerated the analysis of long-text data as well as smart decision making, two key functions in a successful design space exploration task. In this project, we first build gem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI for smooth user interaction, agent automation, and result summarization. We also implemented a language for design space exploration, as well as a Design Space Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a Retrieval Augmented Generation system for gem5 design space exploration. We experiment on cost-constraint optimization with four cost ranges and compare our results with two baseline models. Results show that gem5 Co-Pilot can quickly identify optimal parameters for specific design constraints based on performance and cost, with limited user interaction.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.20965'>On the Impact of Intra-node Communication in the Performance of Supercomputer and Data Center Interconnection Networks</a></h3><h3><a href='https://arxiv.org/pdf/2502.20965' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>8/29</p><p><b>作者：</b>Joaquin Tarraga-Moreno, Jesus Escudero-Sahuquillo, Pedro Javier Garcia, Francisco J. Quiles</p><p>In the last decade, specific-purpose computing and storage devices, such as GPUs, TPUs, or high-speed storage, have been incorporated into server nodes of Supercomputers and Data centers. The development of high-bandwidth memory (HBM) enabled a much more compact form factor for these devices, thus allowing the interconnection of several of them within a server node, typically using an intra-node interconnection network (e.g., PCIe, NVLink, or Infinity Fabric). These networks allow scaling up the number of specific computing and storage devices per node. Furthermore, the inter-node networks communicate thousands of these devices placed in different server nodes in a Supercomputer or Data Center. Unfortunately, the intra- and inter-node networks may become the system&#x27;s bottleneck due to the increasing communication demand among accelerators of applications such as generative AI. Although current intra-node network designs alleviate this bottleneck by increasing the bandwidth of the intra-node network, we show in this paper that such a high bandwidth for intra-node communication may hinder the inter-node communication performance when traffic from outside the node arrives at the intra-node devices, resulting in interference with intra-node traffic. To analyze the impact of this interference, we have studied the communication operations of realistic traffic patterns exploiting intra-node communication. We have developed a generic intra- and inter-node simulation model based on OMNeT++ and modeled the mentioned communication operations. We have also performed extensive simulation experiments that confirm that increasing the intra-node network bandwidth and the number of computing devices per node (i.e., accelerators) is counterproductive to the inter-node communication performance.</p><p><h4>cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19296'>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</a></h3><h3><a href='https://arxiv.org/pdf/2510.19296' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>9/29</p><p><b>作者：</b>Yang Zhang, Rui Zhang, Jiaming Guo, Lei Huang, Di Huang, Yunpu Zhao, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</p><p>The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/zy1xxx/SALV.</p><p><h4>cs.LG, cs.AR, cs.PL</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2505.01386'>CATransformers: Carbon Aware Transformers Through Joint Model-Hardware Optimization</a></h3><h3><a href='https://arxiv.org/pdf/2505.01386' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>10/29</p><p><b>作者：</b>Irene Wang, Newsha Ardalani, Mostafa Elhoushi, Daniel Jiang, Samuel Hsia, Ekin Sumbul, Divya Mahajan, Carole-Jean Wu, Bilge Acun</p><p>Machine learning solutions are rapidly adopted to enable a variety of key use cases, from conversational AI assistants to scientific discovery. This growing adoption is expected to increase the associated lifecycle carbon footprint, including both \emph{operational carbon} from training and inference and \emph{embodied carbon} from AI hardware manufacturing. We introduce \ourframework -- the first carbon-aware co-optimization framework for Transformer-based models and hardware accelerators. By integrating both operational and embodied carbon into early-stage design space exploration, \ourframework enables sustainability-driven model architecture and hardware accelerator co-design that reveals fundamentally different trade-offs than latency- or energy-centric approaches. Evaluated across a range of Transformer models, \ourframework consistently demonstrates the potential to reduce total carbon emissions -- by up to 30\% -- while maintaining accuracy and latency. We further highlight its extensibility through a focused case study on multi-modal models. Our results emphasize the need for holistic optimization methods that prioritize carbon efficiency without compromising model capability and execution time performance. The source code of \ourframework is available at {\small{\href{https://github.com/facebookresearch/CATransformers}{\texttt{https://github.com/facebookresearch/CATransformers}}}}.</p><p><h4>cs.LG, cs.AR</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.18893'>CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation</a></h3><h3><a href='https://arxiv.org/pdf/2510.18893' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>11/29</p><p><b>作者：</b>Sergey Pugachev</p><p>Multi-agent LLM systems fail to realize parallel speedups due to costly coordination. We present CodeCRDT, an observation-driven coordination pattern where agents coordinate by monitoring a shared state with observable updates and deterministic convergence, rather than explicit message passing. Using Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free, conflict-free concurrent code generation with strong eventual consistency. Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, and 100% convergence with zero merge failures. The study formalizes observation-driven coordination for stochastic LLM agents, revealing semantic conflict rates (5-10%) and quality-performance tradeoffs, and provides empirical characterization of when parallel coordination succeeds versus fails based on task structure.</p><p><h4>cs.DC, cs.AI, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.18897'>AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators</a></h3><h3><a href='https://arxiv.org/pdf/2510.18897' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>12/29</p><p><b>作者：</b>Jacopo Tagliabue</p><p>We explore AI-driven distributed-systems policy design by combining stochastic code generation from large language models (LLMs) with deterministic verification in a domain-specific simulator. Using a Function-as-a-Service runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we frame scheduler design as an iterative generate-and-verify loop: an LLM proposes a Python policy, the simulator evaluates it on standardized traces, and structured feedback steers subsequent generations. This setup preserves interpretability while enabling targeted search over a large design space. We detail the system architecture and report preliminary results on throughput improvements across multiple models. Beyond early gains, we discuss the limits of the current setup and outline next steps; in particular, we conjecture that AI will be crucial for scaling this methodology by helping to bootstrap new simulators.</p><p><h4>cs.DC, cs.AI, cs.DB, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19012'>Comparative analysis of large data processing in Apache Spark using Java, Python and Scala</a></h3><h3><a href='https://arxiv.org/pdf/2510.19012' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>13/29</p><p><b>作者：</b>Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii</p><p>During the study, the results of a comparative analysis of the process of handling large datasets using the Apache Spark platform in Java, Python, and Scala programming languages were obtained. Although prior works have focused on individual stages, comprehensive comparisons of full ETL workflows across programming languages using Apache Iceberg remain limited. The analysis was performed by executing several operations, including downloading data from CSV files, transforming and loading it into an Apache Iceberg analytical table. It was found that the performance of the Spark algorithm varies significantly depending on the amount of data and the programming language used. When processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71 seconds, which is superior to Scala&#x27;s score of 9.13 seconds and Java&#x27;s time of 9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming languages demonstrated similar results: the fastest performance was showed in Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56 seconds, respectively. When performing a more complex operation that involved combining two CSV files into a single dataset for further loading into an Apache Iceberg table, Scala demonstrated the highest performance, at 374.42 seconds. Java processing was completed in 379.8 seconds, while Python was the least efficient, with a runtime of 398.32 seconds. It follows that the programming language significantly affects the efficiency of data processing by the Apache Spark algorithm, with Scala and Java being more productive for processing large amounts of data and complex operations, while Python demonstrates an advantage in working with small amounts of data. The results obtained can be useful for optimizing data handling processes depending on specific performance requirements and the amount of information being processed.</p><p><h4>cs.DC, cs.DB, cs.PL, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19151'>On the Randomized Locality of Matching Problems in Regular Graphs</a></h3><h3><a href='https://arxiv.org/pdf/2510.19151' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>14/29</p><p><b>作者：</b>Seri Khoury, Manish Purohit, Aaron Schild, Joshua Wang</p><p>The main goal in distributed symmetry-breaking is to understand the locality of problems; i.e., the radius of the neighborhood that a node needs to explore in order to arrive at its part of a global solution. In this work, we study the locality of matching problems in the family of regular graphs, which is one of the main benchmarks for establishing lower bounds on the locality of symmetry-breaking problems, as well as for obtaining classification results. For approximate matching, we develop randomized algorithms to show that $(1 + \epsilon)$-approximate matching in regular graphs is truly local; i.e., the locality depends only on $\epsilon$ and is independent of all other graph parameters. Furthermore, as long as the degree $\Delta$ is not very small (namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal matching in regular graphs which requires some dependence on the number of nodes $n$ or the degree $\Delta$. We show matching lower bounds for both results. For maximal matching, our techniques further allow us to establish a strong separation between the node-averaged complexity and worst-case complexity of maximal matching in regular graphs, by showing that the former is only $O(1)$. Central to our main technical contribution is a novel martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In particular, our analysis shows that applying one round of Luby&#x27;s algorithm on the line graph of a $\Delta$-regular graph results in an almost $\Delta/2$-regular graph.</p><p><h4>cs.DC, cs.DS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2411.11718'>Distributed Maximum Flow in Planar Graphs</a></h3><h3><a href='https://arxiv.org/pdf/2411.11718' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>15/29</p><p><b>作者：</b>Yaseen Abd-Elhaleem (University of Haifa), Michal Dory (University of Haifa), Merav Parter (Weizmann Institute of Science), Oren Weimann (University of Haifa)</p><p>The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019].  We extend the distributed algorithmic toolkit to work on the dual graph $G^*$. These tools can then facilitate various algorithms on $G$ by solving a suitable dual problem on $G^*$.  Given a directed planar graph $G$ with positive and negative edge-lengths and hop-diameter $D$, our key result is an $\tilde{O}(D^2)$-round algorithm for Single Source Shortest Paths on $G^*$, which then implies $\tilde{O}(D^2)$-round algorithms for Maximum $st$-Flow and Directed Global Min-Cut on $G$. Prior to our work, no $\tilde{O}(\text{poly}(D))$-round algorithm was known for those problems. We further obtain a $D\cdot n^{o(1)}$-rounds $(1-\epsilon)$-approximation algorithm for Maximum $st$-Flow on $G$ when $G$ is undirected and $st$-planar. Finally, we give a near optimal $\tilde O(D)$-round algorithm for computing the weighted girth of $G$. The main challenges in our work are that $G^*$ is not the communication graph (e.g., a vertex of $G$ is mapped to multiple vertices of $G^*$), and that the diameter of $G^*$ can be much larger than $D$ (i.e., possibly by a linear factor). We overcome these challenges by carefully defining and maintaining subgraphs of the dual graph $G^*$ while applying the recursive decomposition on the primal graph $G$. The main technical difficulty, is that along the recursive decomposition, a face of $G$ gets shattered into (disconnected) components yet we still need to treat it as a dual node.</p><p><h4>cs.DC, cs.DS</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19225'>RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs</a></h3><h3><a href='https://arxiv.org/pdf/2510.19225' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>16/29</p><p><b>作者：</b>Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica</p><p>Reinforcement learning (RL) has become essential for unlocking advanced reasoning capabilities in large language models (LLMs). RL workflows involve interleaving rollout and training stages with fundamentally different resource requirements. Rollout typically dominates overall execution time, yet scales efficiently through multiple independent instances. In contrast, training requires tightly-coupled GPUs with full-mesh communication. Existing RL frameworks fall into two categories: co-located and disaggregated architectures. Co-located ones fail to address this resource tension by forcing both stages to share the same GPUs. Disaggregated architectures, without modifications of well-established RL algorithms, suffer from resource under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances on public clouds and spare capacity in production clusters, present significant cost-saving opportunities for accelerating RL workflows, if efficiently harvested for rollout.  In this paper, we present RLBoost, a systematic solution for cost-efficient RL training that harvests preemptible GPU resources. Our key insight is that rollout&#x27;s stateless and embarrassingly parallel nature aligns perfectly with preemptible and often fragmented resources. To efficiently utilize these resources despite frequent and unpredictable availability changes, RLBoost adopts a hybrid architecture with three key techniques: (1) adaptive rollout offload to dynamically adjust workloads on the reserved (on-demand) cluster, (2) pull-based weight transfer that quickly provisions newly available instances, and (3) token-level response collection and migration for efficient preemption handling and continuous load balancing. Extensive experiments show RLBoost increases training throughput by 1.51x-1.97x while improving cost efficiency by 28%-49% compared to using only on-demand GPU resources.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2501.12407'>The Streaming Batch Model for Efficient and Fault-Tolerant Heterogeneous Execution</a></h3><h3><a href='https://arxiv.org/pdf/2501.12407' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>17/29</p><p><b>作者：</b>Frank Sifei Luan, Ron Yifeng Wang, Yile Gu, Ziming Mao, Charlotte Lin, Amog Kamsetty, Hao Chen, Cheng Su, Balaji Veeramani, Scott Lee, SangBin Cho, Clark Zinzow, Eric Liang, Ion Stoica, Stephanie Wang</p><p>While ML model training and inference are both GPU-intensive, CPU-based data processing is often the bottleneck. Distributed data processing systems based on the batch or stream processing models assume homogeneous resource requirements. They excel at CPU-based computation but either under-utilize heterogeneous resources or impose high overheads on failure and reconfiguration.  We introduce the streaming batch model, a hybrid of batch and streaming that enables efficient and fault-tolerant heterogeneous execution. The key idea is to use partitions as the unit of execution to achieve elasticity, but to allow partitions to be dynamically created and streamed between heterogeneous operators for memory-efficient pipelining. We present Ray Data, a streaming batch system that improves throughput on heterogeneous batch inference pipelines by 2.5-12$\times$ compared to traditional batch and stream processing systems. By leveraging heterogeneous clusters, Ray Data improves training throughput for multimodal models such as Stable Diffusion by 31% compared to single-node ML data loaders.</p><p><h4>cs.DC, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19262'>RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training</a></h3><h3><a href='https://arxiv.org/pdf/2510.19262' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>18/29</p><p><b>作者：</b>Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li</p><p>Training Mixture-of-Experts (MoE) models introduces sparse and highly imbalanced all-to-all communication that dominates iteration time. Conventional load-balancing methods fail to exploit the deterministic topology of Rail architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a distributed load-balancing framework that minimizes all-to-all completion time in MoE training. RailS leverages the Rail topology&#x27;s symmetry to prove that uniform sending ensures uniform receiving, transforming global coordination into local scheduling. Each node independently executes a Longest Processing Time First (LPT) spraying scheduler to proactively balance traffic using local information. RailS activates N parallel rails for fine-grained, topology-aware multipath transmission. Across synthetic and real-world MoE workloads, RailS improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For Mixtral workloads, it shortens iteration time by 18%--40% and achieves near-optimal load balance, fully exploiting architectural parallelism in distributed training.</p><p><h4>cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19725'>CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing</a></h3><h3><a href='https://arxiv.org/pdf/2510.19725' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>19/29</p><p><b>作者：</b>Jingfan Meng, Tianji Yang, Jun Xu</p><p>In the set reconciliation (\textsf{SetR}) problem, two parties Alice and Bob, holding sets $\mathsf{A}$ and $\mathsf{B}$, communicate to learn the symmetric difference $\mathsf{A} \Delta \mathsf{B}$. In this work, we study a related but under-explored problem: set intersection (\textsf{SetX})~\cite{Ozisik2019}, where both parties learn $\mathsf{A} \cap \mathsf{B}$ instead. However, existing solutions typically reuse \textsf{SetR} protocols due to the absence of dedicated \textsf{SetX} protocols and the misconception that \textsf{SetR} and \textsf{SetX} have comparable costs. Observing that \textsf{SetX} is fundamentally cheaper than \textsf{SetR}, we developed a multi-round \textsf{SetX} protocol that outperforms the information-theoretic lower bound of \textsf{SetR} problem. In our \textsf{SetX} protocol, Alice sends Bob a compressed sensing (CS) sketch of $\mathsf{A}$ to help Bob identify his unique elements (those in $\mathsf{B \setminus A}$). This solves the \textsf{SetX} problem, if $\mathsf{A} \subseteq \mathsf{B}$. Otherwise, Bob sends a CS sketch of the residue (a set of elements he cannot decode) back to Alice for her to decode her unique elements (those in $\mathsf{A \setminus B}$). As such, Alice and Bob communicate back and forth %with a set membership filter (SMF) of estimated $\mathsf{B \setminus A}$. Alice updates $\mathsf{A}$ and communication repeats until both parties agrees on $\mathsf{A} \cap \mathsf{B}$. On real world datasets, experiments show that our $\mathsf{SetX}$ protocol reduces the communication cost by 8 to 10 times compared to the IBLT-based $\mathsf{SetR}$ protocol.</p><p><h4>cs.DC, cs.NI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19301'>FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems</a></h3><h3><a href='https://arxiv.org/pdf/2510.19301' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>20/29</p><p><b>作者：</b>Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang</p><p>The Viterbi algorithm is a key operator for structured sequence inference in modern data systems, with applications in trajectory analysis, online recommendation, and speech recognition. As these workloads increasingly migrate to resource-constrained edge platforms, standard Viterbi decoding remains memory-intensive and computationally inflexible. Existing methods typically trade decoding time for space efficiency, but often incur significant runtime overhead and lack adaptability to various system constraints. This paper presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly Viterbi decoding operator that enhances adaptability and resource efficiency. FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning and parallelization techniques to enhance both time and memory efficiency, making it well-suited for resource-constrained data systems.To further decouple space complexity from the hidden state space size, we present FLASH-BS Viterbi, a dynamic beam search variant built on a memory-efficient data structure. Both proposed algorithms exhibit strong adaptivity to diverse deployment scenarios by dynamically tuning internal parameters.To ensure practical deployment on edge devices, we also develop FPGA-based hardware accelerators for both algorithms, demonstrating high throughput and low resource usage. Extensive experiments show that our algorithms consistently outperform existing baselines in both decoding time and memory efficiency, while preserving adaptability and hardware-friendly characteristics essential for modern data systems. All codes are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19617'>Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud</a></h3><h3><a href='https://arxiv.org/pdf/2510.19617' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>21/29</p><p><b>作者：</b>Eric Ding</p><p>Collaborative Machine Learning is a paradigm in the field of distributed machine learning, designed to address the challenges of data privacy, communication overhead, and model heterogeneity. There have been significant advancements in optimization and communication algorithm design and ML hardware that enables fair, efficient and secure collaborative ML training. However, less emphasis is put on collaborative ML infrastructure development. Developers and researchers often build server-client systems for a specific collaborative ML use case, which is not scalable and reusable. As the scale of collaborative ML grows, the need for a scalable, efficient, and ideally multi-tenant resource management system becomes more pressing. We propose a novel system, Propius, that can adapt to the heterogeneity of client machines, and efficiently manage and control the computation flow between ML jobs and edge resources in a scalable fashion. Propius is comprised of a control plane and a data plane. The control plane enables efficient resource sharing among multiple collaborative ML jobs and supports various resource sharing policies, while the data plane improves the scalability of collaborative ML model sharing and result collection. Evaluations show that Propius outperforms existing resource management techniques and frameworks in terms of resource utilization (up to $1.88\times$), throughput (up to $2.76$), and job completion time (up to $1.26\times$).</p><p><h4>cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19470'>HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission</a></h3><h3><a href='https://arxiv.org/pdf/2510.19470' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>22/29</p><p><b>作者：</b>Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang</p><p>Mixture-of-Experts (MoE) has become a popular architecture for scaling large models. However, the rapidly growing scale outpaces model training on a single DC, driving a shift toward a more flexible, cross-DC training paradigm. Under this, Expert Parallelism (EP) of MoE faces significant scalability issues due to the limited cross-DC bandwidth. Specifically, existing EP optimizations attempt to overlap data communication and computation, which has little benefit in low-bandwidth scenarios due to a much longer data communication time. Therefore, the trends of cross-DC EP scaling is fast becoming a critical roadblock to the continued growth of MoE models.  To address this, we propose HybridEP, a modeling-guided framework to optimize EP under constrained bandwidth. Our key idea is to dynamically transform the spatial placement of experts to reduce data communication traffic and frequency, thereby minimizing EP&#x27;s communication overheads. However, it is non-trivial to find the optimal solution because it complicates the original communication pattern by mixing data and expert communication. We therefore build a stream-based model to determine the optimal transmission ratio. Guided by this, we incorporate two techniques: (1) domain-based partition to construct the mapping between hybrid patterns and specific communication topology at GPU level, and (2) parameter-efficient migration to further refine this topology by reducing expert transmission overhead and enlarging the domain size. Combining all these designs, HybridEP can be considered as a more general EP with better scalability. Experimental results show that HybridEP outperforms existing state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth. We further compare HybridEP and EP on large-scale simulations. HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.</p><p><h4>cs.DC, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19689'>Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation</a></h3><h3><a href='https://arxiv.org/pdf/2510.19689' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>23/29</p><p><b>作者：</b>Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</p><p>Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale batch or streaming analytics but introduce coordination complexity and auditing overheads that misalign with moderate-scale, latency-sensitive inference. Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet enable interpretable tabular ML, motivating new deployment blueprints for regulated environments. In this paper, we present a production-oriented Big Data as a Service (BDaaS) blueprint that integrates a single-node serverless GPU runtime with TabNet. The design leverages GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets, comparing our approach against Spark and CPU baselines. Our results show that GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines, while compliance mechanisms add only ~5.7 ms latency with p99 &lt; 22 ms. Interpretability remains stable under peak load, ensuring reliable auditability. Taken together, these findings provide a compliance-aware benchmark, a reproducible Helm-packaged blueprint, and a decision framework that demonstrate the practicality of secure, interpretable, and cost-efficient serverless GPU analytics for regulated enterprise and government settings.</p><p><h4>cs.DC, cs.AI, cs.LG</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19805'>Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond</a></h3><h3><a href='https://arxiv.org/pdf/2510.19805' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>24/29</p><p><b>作者：</b>Carl-Johan Fauvelle Munck af Rosensch&quot;old, Feras M. Awaysheh, Ahmad Awad</p><p>In-memory key-value datastores have become indispensable building blocks of modern cloud-native infrastructures, yet their evolution faces scalability, compatibility, and sustainability constraints. The current literature lacks an experimental evaluation of state-of-the-art tools in the domain. This study addressed this timely gap by benchmarking Redis alternatives and systematically evaluating Valkey, KeyDB, and Garnet under realistic workloads within Kubernetes deployments. The results demonstrate clear trade-offs among the benchmarked data systems. Our study presents a comprehensive performance and viability assessment of the emerging in-memory key-value stores. Metrics include throughput, tail latency, CPU and memory efficiency, and migration complexity. We highlight trade-offs between performance, compatibility, and long-term viability, including project maturity, community support, and sustained development.</p><p><h4>cs.DC, cs.DB</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2509.23130'>SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems</a></h3><h3><a href='https://arxiv.org/pdf/2509.23130' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>25/29</p><p><b>作者：</b>Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu</p><p>Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI&#x27;s ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today&#x27;s critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes nine diverse system artifacts: the Raft implementation of Etcd and Redis, the Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively added. SysMoBench enables us to understand the capabilities and limitations of today&#x27;s LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.</p><p><h4>cs.AI, cs.DC, cs.SE</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2510.19322'>Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks</a></h3><h3><a href='https://arxiv.org/pdf/2510.19322' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>26/29</p><p><b>作者：</b>Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu</p><p>Collective communication (CC) is widely adopted for large-scale distributed machine learning (DML) training workloads. DML&#x27;s predictable traffic pattern provides a great oppotunity for applying optical network technology. Existing optical interconnects-based CC schemes adopt ``one-shot network reconfiguration&#x27;&#x27;, which provisions static high-capacity topologies for an entire collective operation -- sometimes for a full training iteration. However, this approach faces significant scalability limitations when supporting more complex and efficient CC algorithms required for modern workloads: the ``one-shot&#x27;&#x27; strategies either demand excessive resource overprovisioning or suffer performance degradation due to rigid resource allocation.  To address these challenges, we propose SWOT, a demand-aware optical network framework. SWOT employs ``intra-collective reconfiguration&#x27;&#x27; and can dynamically align network resources with CC traffic patterns. SWOT incorporates a novel scheduling technique that overlaps optical switch reconfigurations with ongoing transmissions, and improves communication efficiency. SWOT introduce a lightweight collective communication shim that enables coordinated optical network configuration and transmission scheduling while supporting seamless integration with existing CC libraries. Our simulation results demonstrate SWOT&#x27;s significant performance improvements.</p><p><h4>cs.NI, cs.AI, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2502.00937'>ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving</a></h3><h3><a href='https://arxiv.org/pdf/2502.00937' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>27/29</p><p><b>作者：</b>Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, \&#x27;I\~nigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, Rodrigo Fonseca</p><p>Large multimodal models (LMMs) demonstrate impressive capabilities in understanding images, videos, and audio beyond text. However, efficiently serving LMMs in production environments poses significant challenges due to their complex architectures and heterogeneous characteristics across their multi-stage inference pipelines. We present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, across six representative open-source models, revealing key systems design implications. We also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions and bursty traffic patterns. Based on these insights, we propose ModServe, a modular LMM serving system that decouples stages for independent optimization and adaptive scaling. ModServe dynamically reconfigures stages and handles bursty traffic with modality-aware scheduling and autoscaling to meet tail latency SLOs while minimizing costs. ModServe achieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while meeting SLOs on a 128-GPU cluster with production traces.</p><p><h4>cs.DC, cs.AI</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2406.14441'>Vahana.jl -- A framework (not only) for large-scale agent-based models</a></h3><h3><a href='https://arxiv.org/pdf/2406.14441' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>28/29</p><p><b>作者：</b>Steffen F\&quot;urst, Tim Conrad, Carlo Jaeger, Sarah Wolf</p><p>Agent-based models (ABMs) offer a powerful framework for understanding complex systems. However, their computational demands often become a significant barrier as the number of agents and complexity of the simulation increase. Traditional ABM platforms often struggle to fully exploit modern computing resources, hindering the development of large-scale simulations. This paper presents Vahana.jl, a high performance computing open source framework that aims to address these limitations. Building on the formalism of synchronous graph dynamical systems, Vahana.jl is especially well suited for models with a focus on (social) networks. The framework seamlessly supports distribution across multiple compute nodes, enabling simulations that would otherwise be beyond the capabilities of a single machine. Implemented in Julia, Vahana.jl leverages the interactive Read-Eval-Print Loop (REPL) environment, facilitating rapid model development and experimentation.</p><p><h4>cs.MA, cs.DC</h4></p></div><hr>
<div><h3><a href='https://arxiv.org/abs/2503.18191'>DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed Userspace File Systems</a></h3><h3><a href='https://arxiv.org/pdf/2503.18191' target='_blank' rel='noopener noreferrer'> <b>[pdf]</b> </a></h3><p>29/29</p><p><b>作者：</b>Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon</p><p>Cloud platforms host thousands of tenants that demand POSIX semantics, high throughput, and rapid evolution from their storage layer. Kernel-native distributed file systems supply raw speed, but their privileged code base couples every release to the kernel, widens the blast radius of crashes, and slows innovation. FUSE-based distributed file systems flip those trade-offs: they run in user space for fast deployment and strong fault isolation, yet the FUSE interface disables the kernel&#x27;s write-back page cache whenever strong consistency is required. Practitioners must therefore choose between (i) weak consistency with fast write-back caching or (ii) strong consistency with slow write-through I/O, a limitation that has kept FUSE distributed file systems out of write-intensive cloud workloads.  To this end, we present DFUSE, the first distributed FUSE file system that delivers write-back kernel caching and strong consistency. DFUSE achieves this by offloading userspace consistency control to the kernel driver, allowing coordinated access to the kernel&#x27;s page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide strong consistency without compromising performance. In our evaluation, DFUSE achieves up to 68.0% higher throughput and 40.4% lower latency than the existing write-through design of FUSE-based distributed file systems.</p><p><h4>cs.OS</h4></p></div><hr>
</body>
</html>
